<!DOCTYPE html>
<html lang="en-us">
  <head>
    
<meta charset="UTF-8">
<title>Crawl web content | Elastic App Search Documentation [7.16] | Elastic</title>
<link rel="home" href="index.html" title="Elastic App Search Documentation [7.16]"/>
<link rel="up" href="guides.html" title="Guides"/>
<link rel="prev" href="analytics-tags-guide.html" title="Analytics Tags Guide"/>
<link rel="next" href="curations-guide.html" title="Curations"/>
<meta name="DC.type" content="Learn/Docs/App Search/Guide/7.16"/>
<meta name="DC.subject" content="App Search"/>
<meta name="DC.identifier" content="7.16"/>
<meta name="robots" content="noindex,nofollow"/>

    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script src="https://cdn.optimizely.com/js/18132920325.js"></script>
    <link rel="apple-touch-icon" sizes="57x57" href="/apple-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="60x60" href="/apple-icon-60x60.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/apple-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="76x76" href="/apple-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/apple-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="120x120" href="/apple-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/apple-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="152x152" href="/apple-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-icon-180x180.png">
    <link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/android-chrome-192x192.png" sizes="192x192">
    <link rel="icon" type="image/png" href="/favicon-96x96.png" sizes="96x96">
    <link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
    <link rel="manifest" href="/manifest.json">
    <meta name="apple-mobile-web-app-title" content="Elastic">
    <meta name="application-name" content="Elastic">
    <meta name="msapplication-TileColor" content="#ffffff">
    <meta name="msapplication-TileImage" content="/mstile-144x144.png">
    <meta name="theme-color" content="#ffffff">
    <meta name="naver-site-verification" content="936882c1853b701b3cef3721758d80535413dbfd" />
    <meta name="yandex-verification" content="d8a47e95d0972434" />
    <meta name="localized" content="true" />
    <meta name="st:robots" content="follow,index" />
    <meta property="og:image" content="https://www.elastic.co/static/images/elastic-logo-200.png" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <link rel="apple-touch-icon-precomposed" sizes="64x64" href="/favicon_64x64_16bit.png">
    <link rel="apple-touch-icon-precomposed" sizes="32x32" href="/favicon_32x32.png">
    <link rel="apple-touch-icon-precomposed" sizes="16x16" href="/favicon_16x16.png">
    <!-- Give IE8 a fighting chance -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <link rel="stylesheet" type="text/css" href="/guide/static/styles.css" />
  </head>

  <!--© 2015-2022 Elasticsearch B.V. -->
  <!-- All Elastic documentation is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License. -->
  <!-- http://creativecommons.org/licenses/by-nc-nd/4.0/ -->

  <body>
    <!-- Google Tag Manager -->
    <script>dataLayer = [];</script><noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-58RLH5" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start': new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0], j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src= '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f); })(window,document,'script','dataLayer','GTM-58RLH5');</script>
    <!-- End Google Tag Manager -->

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-12395217-16"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-12395217-16');
    </script>

    <!--BEGIN QUALTRICS WEBSITE FEEDBACK SNIPPET-->
    <script type='text/javascript'>
      (function(){var g=function(e,h,f,g){
      this.get=function(a){for(var a=a+"=",c=document.cookie.split(";"),b=0,e=c.length;b<e;b++){for(var d=c[b];" "==d.charAt(0);)d=d.substring(1,d.length);if(0==d.indexOf(a))return d.substring(a.length,d.length)}return null};
      this.set=function(a,c){var b="",b=new Date;b.setTime(b.getTime()+6048E5);b="; expires="+b.toGMTString();document.cookie=a+"="+c+b+"; path=/; "};
      this.check=function(){var a=this.get(f);if(a)a=a.split(":");else if(100!=e)"v"==h&&(e=Math.random()>=e/100?0:100),a=[h,e,0],this.set(f,a.join(":"));else return!0;var c=a[1];if(100==c)return!0;switch(a[0]){case "v":return!1;case "r":return c=a[2]%Math.floor(100/c),a[2]++,this.set(f,a.join(":")),!c}return!0};
      this.go=function(){if(this.check()){var a=document.createElement("script");a.type="text/javascript";a.src=g;document.body&&document.body.appendChild(a)}};
      this.start=function(){var a=this;window.addEventListener?window.addEventListener("load",function(){a.go()},!1):window.attachEvent&&window.attachEvent("onload",function(){a.go()})}};
      try{(new g(100,"r","QSI_S_ZN_emkP0oSe9Qrn7kF","https://znemkp0ose9qrn7kf-elastic.siteintercept.qualtrics.com/WRSiteInterceptEngine/?Q_ZID=ZN_emkP0oSe9Qrn7kF")).start()}catch(i){}})();
    </script><div id='ZN_emkP0oSe9Qrn7kF'><!--DO NOT REMOVE-CONTENTS PLACED HERE--></div>
    <!--END WEBSITE FEEDBACK SNIPPET-->

    <div id='elastic-nav' style="display:none;"></div>
    <script src='https://www.elastic.co/elastic-nav.js'></script>

    <div class="main-container">
      <section id="content" >
        <div class="content-wrapper">

          <section id="guide" lang="en">
            <div class="container-fluid">
              <div class="row pb-3">
                <div class="col-12 order-2 col-md-4 order-md-1 col-lg-3 h-almost-full-md sticky-top-md" id="left_col">
                  <!-- The TOC is appended here -->
                </div>

                <div class="col-12 order-1 col-md-8 order-md-2 col-lg-7 order-lg-2 guide-section" id="middle_col">
                  <!-- start body -->
                  <div class="page_header">
<strong>IMPORTANT</strong>: No additional bug fixes or documentation updates
will be released for this version. For the latest information, see the
<a href="../current/index.html">current release documentation</a>.
</div>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span><span class="chevron-right">›</span>
<span class="breadcrumb-link"><a href="index.html">Elastic App Search Documentation [7.16]</a></span><span class="chevron-right">›</span>
<span class="breadcrumb-link"><a href="guides.html">Guides</a></span>
</div>
<div class="navheader">
<span class="prev">
<a href="analytics-tags-guide.html">« Analytics Tags Guide</a>
</span>
<span class="next">
<a href="curations-guide.html">Curations »</a>
</span>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="crawl-web-content"></a>Crawl web content<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h2>
</div></div></div>
<p>Complete the following steps to crawl your web content using the Enterprise Search web crawler.</p>
<p><span class="strong strong"><strong>1. Identify your web content and create engines:</strong></span></p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-identify-web-content" title="Identify web content">Identify web content</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-create-engine" title="Create engine">Create engine</a>
</li>
</ul>
</div>
<p><span class="strong strong"><strong>2. For each engine, complete the first crawl cycle:</strong></span></p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<p><a class="xref" href="crawl-web-content.html#crawl-web-content-manage-crawl" title="Manage crawl">Manage crawl</a></p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-manage-domains" title="Manage domains">Manage domains</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-manage-entry-points" title="Manage entry points">Manage entry points</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-manage-crawl-rules" title="Manage crawl rules">Manage crawl rules</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-manage-robots-txt-files" title="Manage robots.txt files">Manage robots.txt files</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-manage-sitemaps" title="Manage sitemaps">Manage sitemaps</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-manage-duplicate-documents" title="Manage duplicate document handling">Manage duplicate document handling</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-embed-web-crawler-instructions" title="Embed web crawler instructions within content">Embed web crawler instructions within content</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-start-crawl" title="Start crawl">Start crawl</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-cancel-crawl" title="Cancel crawl">Cancel crawl</a>
</li>
</ul>
</div>
</li>
<li class="listitem">
<p><a class="xref" href="crawl-web-content.html#crawl-web-content-monitor-crawl" title="Monitor crawl">Monitor crawl</a></p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-view-crawl-status" title="View crawl status">View crawl status</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-view-crawl-request-id" title="View crawl request ID">View crawl request ID</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-view-web-crawler-events-logs" title="View web crawler events logs">View web crawler events logs</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-view-web-crawler-events-by-crawl-id-and-url" title="View web crawler events by crawl ID and URL">View web crawler events by crawl ID and URL</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-view-web-crawler-system-logs" title="View web crawler system logs">View web crawler system logs</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-view-indexed-documents" title="View indexed documents">View indexed documents</a>
</li>
</ul>
</div>
</li>
<li class="listitem">
<p><a class="xref" href="crawl-web-content.html#crawl-web-content-troubleshoot-crawl" title="Troubleshoot crawl">Troubleshoot crawl</a></p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-troubleshoot-specific-errors" title="Troubleshoot specific errors">Troubleshoot specific errors</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-troubleshoot-crawl-stability" title="Troubleshoot crawl stability">Troubleshoot crawl stability</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-troubleshoot-content-discovery" title="Troubleshoot content discovery">Troubleshoot content discovery</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-troubleshoot-content-extraction-and-indexing" title="Troubleshoot content extraction and indexing">Troubleshoot content extraction and indexing</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-provide-feedback" title="Provide feedback">Provide feedback</a>
</li>
</ul>
</div>
</li>
</ul>
</div>
<p><span class="strong strong"><strong>3. Re-crawl your web content and optionally schedule crawls:</strong></span></p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-re-crawl" title="Re-crawl web content">Re-crawl web content</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-re-apply-crawl-rules" title="Re-apply crawl rules">Re-apply crawl rules</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-automatic-crawling" title="Automatic crawling">Automatic crawling</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-schedule-crawls" title="Schedule crawls">Schedule crawls</a>
</li>
</ul>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="crawl-web-content-identify-web-content"></a>Identify web content<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h3>
</div></div></div>
<p>Before crawling your web content, you must inventory your domains and decide which you&#8217;d like to crawl and where you&#8217;d like to store the crawled documents.
Consider an organization managing the following web content:</p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Content type</th>
<th align="left" valign="top">URL</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p>Website</p></td>
<td align="left" valign="top"><p><code class="literal">https://example.com</code></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Blog</p></td>
<td align="left" valign="top"><p><code class="literal">https://example.com/blog</code></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Ecommerce application</p></td>
<td align="left" valign="top"><p><code class="literal">https://shop.example.com</code></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Ecommerce administrative dashboard</p></td>
<td align="left" valign="top"><p><code class="literal">https://shop.example.com/admin</code></p></td>
</tr>
</tbody>
</table>
</div>
<p>This organization may decide to index their website and blog using the web crawler, while using the <a class="xref" href="documents.html" title="Documents API">Documents API</a> to index their ecommerce data.</p>
<p>Complete this exercise with your own content to determine which domains you&#8217;d like to crawl.
If you haven&#8217;t already, read the <a class="xref" href="web-crawler-faq.html" title="Web crawler FAQ">Web crawler FAQ</a> to evaluate the crawler&#8217;s capabilities and limitations.</p>
<p>After choosing domains to crawl, decide where you will store the resulting search documents.
Consider another organization with the following web content.</p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Content type</th>
<th align="left" valign="top">URL</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p>Website</p></td>
<td align="left" valign="top"><p><code class="literal">https://example.com</code></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Blog</p></td>
<td align="left" valign="top"><p><code class="literal">https://blog.example.com</code></p></td>
</tr>
</tbody>
</table>
</div>
<p>Although their website and blog are separate domains, they may choose to index them into a single engine.</p>
<p>Again, complete this exercise with your own content.
Choose one engine per search experience.
Each engine has its own crawl configuration, and is limited to a single active crawl.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="crawl-web-content-create-engine"></a>Create engine<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h3>
</div></div></div>
<p>After reviewing <a class="xref" href="crawl-web-content.html#crawl-web-content-identify-web-content" title="Identify web content">Identify web content</a>, create one or more new engines for your content.
See <a class="xref" href="getting-started.html#getting-started-with-app-search-engine" title="1 - Create an Engine">Create an engine</a> for an explanation of the process.</p>
<p>The following sections of this document describe a crawl cycle composed of the following steps: <em>manage</em>, <em>monitor</em>, <em>troubleshoot</em>.
Repeat this cycle for each new engine.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="crawl-web-content-manage-crawl"></a>Manage crawl<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h3>
</div></div></div>
<p>A crawl is the process by which the web crawler discovers, extracts, and indexes web content into an engine.
See <a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl" title="Crawl">Crawl</a> in the web crawler reference for a detailed explanation of a crawl.</p>
<p>Primarily, you manage each crawl in the App Search dashboard.
There, you manage domains, entry points, and crawl rules; and start and cancel the active crawl.
However, you can also manage a crawl using files, such as robots.txt files and sitemaps.
And you can embed crawler instructions within your content, such as canonical URL link tags, robots meta tags, and nofollow links.
You can also start and cancel a crawl using the App Search API.</p>
<p>The following sections cover these topics.</p>
<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-manage-domains"></a>Manage domains<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>A domain is a website or property you&#8217;d like to crawl.
You must associate one or more domains to a crawl.
See <a class="xref" href="web-crawler-reference.html#web-crawler-reference-domain" title="Domain">Domain</a> in the web crawler reference for a detailed explanation of a domain.</p>
<p>Manage the domains for a crawl through the web crawler dashboard.
From the engine menu, choose <span class="strong strong"><strong>Web Crawler</strong></span>.</p>
<div class="imageblock">
<div class="content">
<img src="images/app-search/web-crawler-navigation.png" alt="web crawler navigation">
</div>
</div>
<p>Add your first domain on the getting started screen.</p>
<div class="imageblock">
<div class="content">
<img src="images/app-search/add-first-domain.png" alt="add first domain">
</div>
</div>
<p>From there, you can view, add, manage, and delete domains using the web crawler dashboard.</p>
<div class="imageblock">
<div class="content">
<img src="images/app-search/web-crawler-dashboard-domains.png" alt="web crawler dashboard domains">
</div>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-manage-entry-points"></a>Manage entry points<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>Each domain must have one or more entry points.
These are paths from which the crawler will start each crawl.
See <a class="xref" href="web-crawler-reference.html#web-crawler-reference-entry-point" title="Entry point">Entry point</a> in the web crawler reference for a detailed explanation of an entry point.</p>
<p>Manage the entry points for a domain through the domain dashboard.
From the engine menu, choose <span class="strong strong"><strong>Web Crawler</strong></span>.
Choose <span class="strong strong"><strong>Manage</strong></span> next to the domain you&#8217;d like to manage.
Then locate the <em>Entry Points</em> section of the dashboard.</p>
<div class="imageblock">
<div class="content">
<img src="images/app-search/domains-dashboard-entry-points.png" alt="domains dashboard entry points">
</div>
</div>
<p>From here, you can view, add, edit, and delete entry points.</p>
<p>The dashboard adds a default entry point of <code class="literal">/</code> to each domain.
You can delete this entry point, but each domain must have at least one entry point.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-manage-crawl-rules"></a>Manage crawl rules<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>Each domain must also have one or more crawl rules.
These rules instruct the crawler which pages to crawl within the domain.
See <a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl-rule" title="Crawl rule">Crawl rule</a> in the web crawler reference for a detailed explanation of a crawl rule.</p>
<p>Manage the crawl rules for a domain through the domain dashboard.
From the engine menu, choose <span class="strong strong"><strong>Web Crawler</strong></span>.
Choose <span class="strong strong"><strong>Manage</strong></span> next to the domain you&#8217;d like to manage.
Then locate the <em>Crawl Rules</em> section of the dashboard.</p>
<div class="imageblock">
<div class="content">
<img src="images/app-search/domains-dashboard-crawl-rules.png" alt="domains dashboard crawl rules">
</div>
</div>
<p>From here, you can view, add, edit, delete, and re-order crawl rules.</p>
<p>The dashboard adds a default crawl rule to allow all paths.
You cannot delete this crawl rule, but you can insert more restrictive rules in front of this rule.
See <a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl-rule" title="Crawl rule">Crawl rule</a> for explanations of crawl rule logic and the effects of crawl rule order.</p>
<p>After updating your crawl rules, you can immediately <a class="xref" href="crawl-web-content.html#crawl-web-content-re-apply-crawl-rules" title="Re-apply crawl rules">re-apply the updated rules</a> to your existing documents.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-manage-robots-txt-files"></a>Manage robots.txt files<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>Each domain may have a robots.txt file.
This is a plain text file that provides instructions to web crawlers.
The instructions within the file, also called directives, communicate which paths within that domain are disallowed (and allowed) for crawling.
See <a class="xref" href="web-crawler-reference.html#web-crawler-reference-robots-txt-file" title="Robots.txt file">Robots.txt file</a> in the web crawler reference for a detailed explanation of a robots.txt file.</p>
<p>You can also use a robots.txt file to specify sitemaps for a domain.
See <a class="xref" href="crawl-web-content.html#crawl-web-content-manage-sitemaps" title="Manage sitemaps">Manage sitemaps</a>.</p>
<p>Most web crawlers automatically fetch and parse the robots.txt file for each domain they crawl.
If you already publish a robots.txt file for other web crawlers, be aware the Enterprise Search web crawler will fetch this file and honor the directives within it.
You may want to add, remove, or update the robots.txt file for each of your domains.</p>
<p><span class="strong strong"><strong>Example: add a robots.txt file to a domain</strong></span></p>
<p>To add a robots.txt file to the domain <code class="literal">https://shop.example.com</code>:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Determine which paths within the domain you&#8217;d like to exclude.
</li>
<li class="listitem">
<p>Create a robots.txt file with the appropriate directives from the <a href="https://en.wikipedia.org/wiki/Robots_exclusion_standard" class="ulink" target="_blank" rel="noopener">Robots exclusion standard</a>.
For instance:</p>
<div class="pre_wrapper lang-yaml">
<pre class="programlisting prettyprint lang-yaml">User-agent: *
Disallow: /cart
Disallow: /login
Disallow: /account</pre>
</div>
</li>
<li class="listitem">
Publish the file, with filename <code class="literal">robots.txt</code>, at the root of the domain: <code class="literal">https://shop.example.com/robots.txt</code>.
</li>
</ol>
</div>
<p>The next time the web crawler visits the domain, it will fetch and parse the robots.txt file.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-manage-sitemaps"></a>Manage sitemaps<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>Each domain may have one or more sitemaps.
These are XML files that inform web crawlers about pages within that domain.
XML elements within these files identify specific URLs that are available for crawling.
See <a class="xref" href="web-crawler-reference.html#web-crawler-reference-sitemap" title="Sitemap">Sitemap</a> in the web crawler reference for a detailed explanation of a sitemap.</p>
<p>If you already publish sitemaps for other web crawlers, the Enterprise Search web crawler can use the same sitemaps.
For the Enterprise Search web crawler to discover your sitemaps, you can specify them within <a class="xref" href="crawl-web-content.html#crawl-web-content-manage-robots-txt-files" title="Manage robots.txt files">robots.txt files</a>.</p>
<p><span class="strong strong"><strong>Example: add a sitemap via robots.txt</strong></span></p>
<p>To add a sitemap to the domain <code class="literal">https://shop.example.com</code>:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Determine which pages within the domain you&#8217;d like to include.
Ensure these paths are allowed by the domain&#8217;s <a class="xref" href="crawl-web-content.html#crawl-web-content-manage-crawl-rules" title="Manage crawl rules">crawl rules</a> and the directives within the domain&#8217;s <a class="xref" href="crawl-web-content.html#crawl-web-content-manage-robots-txt-files" title="Manage robots.txt files">robots.txt file</a>.
</li>
<li class="listitem">
<p>Create a sitemap file with the appropriate elements from the <a href="https://www.sitemaps.org/index.html" class="ulink" target="_blank" rel="noopener">sitemap standard</a>.
For instance:</p>
<div class="pre_wrapper lang-xml">
<pre class="programlisting prettyprint lang-xml">&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"&gt;
  &lt;url&gt;
    &lt;loc&gt;https://shop.example.com/products/1/&lt;/loc&gt;
  &lt;/url&gt;
  &lt;url&gt;
    &lt;loc&gt;https://shop.example.com/products/2/&lt;/loc&gt;
  &lt;/url&gt;
  &lt;url&gt;
    &lt;loc&gt;https://shop.example.com/products/3/&lt;/loc&gt;
  &lt;/url&gt;
&lt;/urlset&gt;</pre>
</div>
</li>
<li class="listitem">
Publish the file on your site, for example, at the root of the domain: <code class="literal">https://shop.example.com/sitemap.xml</code>.
</li>
<li class="listitem">
<p>Create or modify the robots.txt file for the domain, located at <code class="literal">https://shop.example.com/robots.txt</code>.
Anywhere within the file, add a <code class="literal">Sitemap</code> directive that provides the location of the sitemap.
For instance:</p>
<div class="pre_wrapper lang-yaml">
<pre class="programlisting prettyprint lang-yaml">Sitemap: https://shop.example.com/sitemap.xml</pre>
</div>
</li>
<li class="listitem">
Publish the new or updated robots.txt file.
</li>
</ol>
</div>
<p>The next time the web crawler visits the domain, it will fetch and parse the robots.txt file and the sitemap.</p>
<p>Alternatively, you can also manage the sitemaps for a domain through the domain dashboard.
From the engine menu, choose <span class="strong strong"><strong>Web Crawler</strong></span>.
Choose <span class="strong strong"><strong>Manage</strong></span> next to the domain you&#8217;d like to manage.
Then locate the <em>Sitemaps</em> section of the dashboard.</p>
<p>From here, you can view, add, edit, and delete sitemaps.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-manage-duplicate-documents"></a>Manage duplicate document handling<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>After extracting the content of a web document, the web crawler compares that content to your existing documents, to check for duplication.
See <a class="xref" href="web-crawler-reference.html#web-crawler-reference-content-deduplication" title="Duplicate document handling">Duplicate document handling</a>.</p>
<p>To compare documents, the web crawler examines specific fields.
Manage these fields for each domain within the web crawler UI:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Navigate to <span class="strong strong"><strong>Enterprise Search</strong></span> &#8594; <span class="strong strong"><strong>App Search</strong></span> &#8594; <span class="strong strong"><strong>Engines</strong></span> &#8594; <em>engine name</em> &#8594; <span class="strong strong"><strong>Web crawler</strong></span> &#8594; <em>domain name</em>.
</li>
<li class="listitem">
Locate the the section named <span class="strong strong"><strong>Duplicate document handling</strong></span>.
</li>
<li class="listitem">
Select or deselect the fields you&#8217;d like the crawler to use.
</li>
</ol>
</div>
<p>Alternatively, <em>allow</em> duplicate documents for a domain by deselecting <span class="strong strong"><strong>Prevent duplicate documents</strong></span>.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-embed-web-crawler-instructions"></a>Embed web crawler instructions within content<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>You can also embed instructions for the web crawler within your HTML content.
These instructions are specific HTML tags, attributes, and values that affect the web crawler&#8217;s behavior.</p>
<p>The Enterprise Search web crawler recognizes the following embedded instructions, each of which is described further in the web crawler reference:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-canonical-url-link-tag" title="Canonical URL link tag">Canonical URL link tag</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-robots-meta-tags" title="Robots meta tags">Robots meta tags</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-meta-tags-content-extraction" title="Meta tags and data attributes to extract custom fields">Meta tags and data attributes to extract custom fields</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-nofollow-link" title="Nofollow link">Nofollow link</a>
</li>
</ul>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-start-crawl"></a>Start crawl<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>Start a crawl from the web crawler or domain dashboard, or using the App Search API.</p>
<p>To use a dashboard, navigate to <span class="strong strong"><strong>Web Crawler</strong></span>, then optionally choose a domain to manage.
Choose the <span class="strong strong"><strong>Start a Crawl</strong></span> button at the top of the dashboard.</p>
<div class="imageblock">
<div class="content">
<img src="images/app-search/start-crawl-button-default.png" alt="start crawl button default">
</div>
</div>
<p>Each engine may have only one active crawl.
The start button changes state to reflect a crawl is in progress.</p>
<div class="imageblock">
<div class="content">
<img src="images/app-search/start-crawl-button-crawling.png" alt="start crawl button crawling">
</div>
</div>
<p>To start a crawl programmatically, refer to the following API reference:</p>
<p><a class="xref" href="web-crawler-api-reference.html#web-crawler-apis-post-crawler-crawl-requests" title="Create a new crawl request">Create a new crawl request</a></p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-cancel-crawl"></a>Cancel crawl<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>Cancel an active crawl from the web crawler or domain dashboard, or using the App Search API.</p>
<p>To use a dashboard, navigate to <span class="strong strong"><strong>Web Crawler</strong></span>, then optionally choose a domain to manage.
Expand the <span class="strong strong"><strong>Crawling&#8230;&#8203;</strong></span> button at the top of the dashboard.
Choose <span class="strong strong"><strong>Cancel Crawl</strong></span>.</p>
<div class="imageblock">
<div class="content">
<img src="images/app-search/cancel-crawl-button.png" alt="cancel crawl button">
</div>
</div>
<p>To cancel a crawl programmatically, refer to the following API reference:</p>
<p><a class="xref" href="web-crawler-api-reference.html#web-crawler-apis-post-crawler-crawl-requests-active-cancel" title="Cancel an active crawl">Cancel an active crawl</a></p>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="crawl-web-content-monitor-crawl"></a>Monitor crawl<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h3>
</div></div></div>
<p>You can monitor a crawl while it is running or audit the crawl after it has completed.</p>
<p>Monitoring includes viewing the crawl status, crawl request ID, web crawler event logs (optionally filtered by the crawl ID and a specific URL), web crawler system logs, and documents indexed by the crawl.</p>
<p>The following sections cover these topics.</p>
<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-view-crawl-status"></a>View crawl status<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>Each crawl has a status, which quickly communicates its state.
See <a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl-status" title="Crawl status">Crawl status</a> in the web crawler reference for a description of each crawl status.</p>
<p>View the status of a crawl within the web crawler dashboard or using the App Search API.</p>
<p>To use the dashboard, navigate to <span class="strong strong"><strong>Web Crawler</strong></span> and locate the <em>Recent crawl requests</em> section.</p>
<div class="imageblock">
<div class="content">
<img src="images/app-search/domains-dashboard-recent-crawl-requests.png" alt="domains dashboard recent crawl requests">
</div>
</div>
<p>Refer to the <em>Status</em> column for the status of each recent crawl.</p>
<p>To get a crawl status programmatically, refer to the following API references:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="web-crawler-api-reference.html#web-crawler-apis-get-crawler-crawl-requests-active" title="Get current active crawl request">Get current active crawl request</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-api-reference.html#web-crawler-apis-get-crawler-crawl-requests" title="List crawl requests">List crawl requests</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-api-reference.html#web-crawler-apis-get-crawler-crawl-requests-id" title="View details for a crawl request">View details for a crawl request</a>
</li>
</ul>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-view-crawl-request-id"></a>View crawl request ID<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>Each crawl has an associated <em>crawl request</em>, which is identified by a unique ID in the following format: <code class="literal">60106315beae67d49a8e787d</code>.
Use a crawl request ID to filter the <a class="xref" href="crawl-web-content.html#crawl-web-content-view-web-crawler-events-logs" title="View web crawler events logs">web crawler events logs</a> to a specific crawl.</p>
<p>View the request ID of a crawl within the web crawler dashboard or using the App Search API.</p>
<p>To use the dashboard, navigate to <span class="strong strong"><strong>Web Crawler</strong></span> and locate the <em>Recent crawl requests</em> section.</p>
<div class="imageblock">
<div class="content">
<img src="images/app-search/domains-dashboard-recent-crawl-requests.png" alt="domains dashboard recent crawl requests">
</div>
</div>
<p>Refer to the <em>Request ID</em> column for the request ID of each recent crawl.</p>
<p>To get a crawl request ID programmatically, refer to the following API references:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="web-crawler-api-reference.html#web-crawler-apis-get-crawler-crawl-requests-active" title="Get current active crawl request">Get current active crawl request</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-api-reference.html#web-crawler-apis-get-crawler-crawl-requests" title="List crawl requests">List crawl requests</a>
</li>
</ul>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-view-web-crawler-events-logs"></a>View web crawler events logs<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>The Enterprise Search web crawler records detailed structured events logs for each crawl.
The crawler indexes these logs into Elasticsearch, and you can view the logs using <a href="/guide/en/kibana/current/index.html" class="ulink" target="_blank" rel="noopener">Kibana</a>.</p>
<p>See <a class="xref" href="view-web-crawler-events-logs.html" title="View web crawler events logs">View web crawler events logs</a> for a step by step process to view the web crawler events logs in Kibana.</p>
<p>For a complete reference of all events, see <a class="xref" href="web-crawler-events-logs-reference.html" title="Web crawler events logs reference">Web crawler events logs reference</a>.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-view-web-crawler-events-by-crawl-id-and-url"></a>View web crawler events by crawl ID and URL<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>To monitor a specific crawl or a specific domain, you must filter the <a class="xref" href="crawl-web-content.html#crawl-web-content-view-web-crawler-events-logs" title="View web crawler events logs">web crawler events logs</a> within Kibana.</p>
<p>To view the events for a specific crawl, first <a class="xref" href="crawl-web-content.html#crawl-web-content-view-crawl-request-id" title="View crawl request ID">get the crawl&#8217;s request ID</a>.
Then filter within Kibana on the <code class="literal">crawler.crawl.id</code> field.</p>
<p>You can filter further to narrow your results to a specific URL.
Use the following fields:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
The full URL: <code class="literal">url.full</code>
</li>
<li class="listitem">
Required components of the URL: <code class="literal">url.scheme</code>, <code class="literal">url.domain</code>, <code class="literal">url.port</code>, <code class="literal">url.path</code>
</li>
<li class="listitem">
Optional components of the URL: <code class="literal">url.query</code>, <code class="literal">url.fragment</code>, <code class="literal">url.username</code>, <code class="literal">url.password</code>
</li>
</ul>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-view-web-crawler-system-logs"></a>View web crawler system logs<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>If you are managing your own Enterprise Search deployment, you can also view the web crawler system logs.</p>
<p>View these logs on disk in the <code class="literal">crawler.log</code> file.</p>
<p>The events in these logs are less verbose then the web crawler events logs, but they can help solve web crawler issues.
Each event has a crawl request ID, which allows you to analyze the logs for a specific crawl.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-view-indexed-documents"></a>View indexed documents<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>The web crawler extracts the content from each web page, transforming it into a search document.
It indexes these documents within the engine associated with the crawl.
See <a class="xref" href="web-crawler-reference.html#web-crawler-reference-content-extraction-and-indexing" title="Content extraction and indexing">Content extraction and indexing</a> for more details on this process, and see <a class="xref" href="web-crawler-reference.html#web-crawler-reference-web-crawler-schema" title="Web crawler schema">Web crawler schema</a> for more details on the structure of each search document.</p>
<p>View the indexed documents using the Documents or Query Tester views within the App Search dashboard, or use the search API.</p>
<p><span class="strong strong"><strong>To find a specific document</strong></span>, wrap the document&#8217;s URL in quotes, and use that as your search query.
For example: <code class="literal">"https://example.com/some/page.html"</code>.
If the document is present in the engine, it should be a top result (or only result).</p>
<p>To access the documents dashboard, choose <span class="strong strong"><strong>Documents</strong></span> from the engine menu.</p>
<div class="imageblock">
<div class="content">
<img src="images/app-search/documents-dashboard.png" alt="documents dashboard">
</div>
</div>
<p>To access the query tester, choose <span class="strong strong"><strong>Query Tester</strong></span> from the engine menu.</p>
<div class="imageblock">
<div class="content">
<img src="images/app-search/query-tester-dashboard.png" alt="query tester dashboard">
</div>
</div>
<p>To use the search API, refer to the following API reference:</p>
<p><a class="xref" href="search.html" title="Search API">Search API</a></p>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="crawl-web-content-troubleshoot-crawl"></a>Troubleshoot crawl<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h3>
</div></div></div>
<p>A crawl may not behave as expected or discover and index the documents you expected.
The web crawler faces many challenges while it crawls, including:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Network issues: lost packets, timeouts, DNS issues
</li>
<li class="listitem">
Resource contention: memory usage, CPU cycles
</li>
<li class="listitem">
Parsing problems: broken HTML
</li>
<li class="listitem">
HTTP protocol issues: broken HTTP servers, incorrect HTTP status codes
</li>
</ul>
</div>
<p>For a detailed look at crawl issues, see <a href="/elasticon/archive/2020/global/sprinting-to-a-crawl-building-an-effective-web-crawler" class="ulink" target="_blank" rel="noopener">Sprinting to a crawl: Building an effective web crawler</a>.</p>
<p>However, these issues generally fall into three categories: <em>crawl stability</em>, <em>content discovery</em>, and <em>content extraction and indexing</em>.
We also provide solutions for some specific errors.</p>
<p><span class="strong strong"><strong>Use the following sections to guide your troubleshooting:</strong></span></p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<p>See <a class="xref" href="crawl-web-content.html#crawl-web-content-troubleshoot-specific-errors" title="Troubleshoot specific errors">Troubleshoot specific errors</a> if:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
You&#8217;re troubleshooting a specific error message
</li>
</ul>
</div>
</li>
<li class="listitem">
<p>See <a class="xref" href="crawl-web-content.html#crawl-web-content-troubleshoot-crawl-stability" title="Troubleshoot crawl stability">Troubleshoot crawl stability</a> if:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
You&#8217;re not sure where to start (resolve stability issues first)
</li>
<li class="listitem">
No documents in the engine
</li>
<li class="listitem">
Many documents missing or outdated
</li>
<li class="listitem">
Crawl fails
</li>
<li class="listitem">
Crawl runs for the maximum duration (defaults to 24 hours)
</li>
</ul>
</div>
</li>
<li class="listitem">
<p>See <a class="xref" href="crawl-web-content.html#crawl-web-content-troubleshoot-content-discovery" title="Troubleshoot content discovery">Troubleshoot content discovery</a> if:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Specific documents missing or outdated
</li>
</ul>
</div>
</li>
<li class="listitem">
<p>See <a class="xref" href="crawl-web-content.html#crawl-web-content-troubleshoot-content-extraction-and-indexing" title="Troubleshoot content extraction and indexing">Troubleshoot content extraction and indexing</a> if:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Specific documents missing or outdated
</li>
<li class="listitem">
Incorrect content within documents
</li>
<li class="listitem">
Content missing from documents
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-troubleshoot-specific-errors"></a>Troubleshoot specific errors<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>If you are troubleshooting a specific error, you may find the error message and possible solutions within the following list:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<p><span class="strong strong"><strong>Failed HTTP request: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target</strong></span></p>
<p>The domain&#8217;s SSL certificate is not included in the Java root certificate store.
The domain owner may need to work with the SSL certificate provider to get their root certificate added to the Java root certificate store.</p>
<p>For a self-signed certificate, add your certificate to the web crawler configuration, or change the SSL verification mode.</p>
<p>Add your certificate using the <a class="xref" href="web-crawler-reference.html#web-crawler-reference-web-crawler-configuration-settings" title="Web crawler configuration settings">web crawler configuration setting</a> <code class="literal">crawler.security.ssl.certificate_authorities</code>.</p>
<p>If you have file access to the server, specify the location of the file:</p>
<div class="pre_wrapper lang-yaml">
<pre class="programlisting prettyprint lang-yaml">crawler.security.ssl.certificate_authorities:
  - /path/to/QuoVadis-PKIoverheid-Organisatie-Server-CA-G3-PEM.pem</pre>
</div>
<p>For Elastic Cloud or other environments without file access, provide the contents of the file inline:</p>
<div class="pre_wrapper lang-yaml">
<pre class="programlisting prettyprint lang-yaml">crawler.security.ssl.certificate_authorities:
  - |
    -----BEGIN CERTIFICATE-----
    MIIHWzCCBUOgAwIBAgIINBHa3VUceEkwDQYJKoZIhvcNAQELBQAwajELMAkGA1UE
    BhMCTkwxHjAcBgNVBAoMFVN0YWF0IGRlciBOZWRlcmxhbmRlbjE7MDkGA1UEAwwy
    U3RhYXQgZGVyIE5lZGVybGFuZGVuIE9yZ2FuaXNhdGllIFNlcnZpY2VzIENBIC0g
    RzMwHhcNMTYxMTAzMTQxMjExWhcNMjgxMTEyMDAwMDAwWjCBgjELMAkGA1UEBhMC
    ...
    TkwxIDAeBgNVBAoMF1F1b1ZhZGlzIFRydXN0bGluayBCLlYuMRcwFQYDVQRhDA5O
    VFJOTC0zMDIzNzQ1OTE4MDYGA1UEAwwvUXVvVmFkaXMgUEtJb3ZlcmhlaWQgT3Jn
    YW5pc2F0aWUgU2VydmVyIENBIC0gRzMwggIiMA0GCSqGSIb3DQEBAQUAA4ICDwAw
    SFfzGre9T6yBL4I+6nxG
    -----END CERTIFICATE-----</pre>
</div>
<p>Alternatively, for development and test environments, change the SSL verification mode using <code class="literal">crawler.security.ssl.verification_mode</code>:</p>
<div class="pre_wrapper lang-yaml">
<pre class="programlisting prettyprint lang-yaml"># INSECURE - DO NOT USE IN PRODUCTION ENVIRONMENTS
crawler.security.ssl.verification_mode: none</pre>
</div>
</li>
</ul>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-troubleshoot-crawl-stability"></a>Troubleshoot crawl stability<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>Crawl stability issues prevent the crawler from discovering, extracting, and indexing your content.
It is therefore critical you address these issues first.</p>
<p>Use the following techniques to troubleshoot crawl stability issues.</p>
<p><span class="strong strong"><strong>Validate one or more domains:</strong></span></p>
<p>When adding a domain through the web crawler UI, the crawler validates the domain:</p>
<div class="imageblock">
<div class="content">
<img src="images/app-search/validate-domain.png" alt="validate domain">
</div>
</div>
<p>However, when adding domains through the API, you should validate each domain through the API as well.
See <a class="xref" href="web-crawler-api-reference.html#web-crawler-apis-post-crawler-validate-domain" title="Validate a domain">Validate a domain</a>.</p>
<p>An invalid domain is a common cause for a crawl with no results.
Fix the domain issues and re-crawl.</p>
<p><span class="strong strong"><strong>Analyze web crawler events logs for the most recent crawl:</strong></span></p>
<p>First:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-view-crawl-request-id" title="View crawl request ID">Find the crawl request ID</a> for the most recent crawl.
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-view-web-crawler-events-by-crawl-id-and-url" title="View web crawler events by crawl ID and URL">Filter the web crawler events logs</a> by that ID.
</li>
</ol>
</div>
<p>Then:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Order the events by timestamp, oldest first.
</li>
<li class="listitem">
Locate the <code class="literal">crawl-end</code> event and preceding events.
These events communicate what happened before the crawl failed.
</li>
</ul>
</div>
<p><span class="strong strong"><strong>Analyze web crawler system logs:</strong></span></p>
<p>These logs may contain additional information about your crawl.</p>
<p>See <a class="xref" href="crawl-web-content.html#crawl-web-content-view-web-crawler-system-logs" title="View web crawler system logs">View web crawler system logs</a>.</p>
<p><span class="strong strong"><strong>Modify the web crawler configuration:</strong></span></p>
<p>As a last resort, operators can modify the web crawler configuration, including resource limits.</p>
<p>See <a class="xref" href="web-crawler-reference.html#web-crawler-reference-web-crawler-configuration-settings" title="Web crawler configuration settings">Web crawler configuration settings</a> in the web crawler reference.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-troubleshoot-content-discovery"></a>Troubleshoot content discovery<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>After your crawls are stable, you may find the crawler is not discovering your content as expected.
It&#8217;s helpful to understand how the web crawler discovers content.
See <a class="xref" href="web-crawler-reference.html#web-crawler-reference-content-discovery" title="Content discovery">Content discovery</a> in the web crawler reference.</p>
<p>Use the following techniques to troubleshoot content discovery issues.</p>
<p><span class="strong strong"><strong>Confirm the most recent crawl completed successfully:</strong></span></p>
<p>View the status of the most recent crawl to confirm it completed successfully.
See <a class="xref" href="crawl-web-content.html#crawl-web-content-view-crawl-status" title="View crawl status">View crawl status</a>.</p>
<p>If the crawl failed, look for signs of crawl stability issues.
See <a class="xref" href="crawl-web-content.html#crawl-web-content-troubleshoot-crawl-stability" title="Troubleshoot crawl stability">Troubleshoot crawl stability</a>.</p>
<p><span class="strong strong"><strong>View indexed documents to confirm missing pages:</strong></span></p>
<p>Identify which pages are missing from your engine, or focus on specific pages.
See <a class="xref" href="crawl-web-content.html#crawl-web-content-view-indexed-documents" title="View indexed documents">View indexed documents</a> for instructions to view all documents and specific documents.</p>
<p><span class="strong strong"><strong>Validate or trace specific URLs:</strong></span></p>
<p>Use the following API operations to get detailed information about how the web crawler sees a specific URL now, and whether it saw the URL in recent crawls.</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="web-crawler-api-reference.html#web-crawler-apis-post-crawler-validate-url" title="Validate a URL">Validate a URL</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-api-reference.html#web-crawler-apis-post-crawler-trace-url" title="Trace a URL">Trace a URL</a>
</li>
</ul>
</div>
<p><span class="strong strong"><strong>Analyze web crawler events logs for the most recent crawl:</strong></span></p>
<p>First:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-view-crawl-request-id" title="View crawl request ID">Find the crawl request ID</a> for the most recent crawl.
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-view-web-crawler-events-by-crawl-id-and-url" title="View web crawler events by crawl ID and URL">Filter the web crawler events logs</a>by that ID.
</li>
<li class="listitem">
Find the URL of a specific document missing from the engine.
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-view-web-crawler-events-by-crawl-id-and-url" title="View web crawler events by crawl ID and URL">Filter the web crawler events logs</a> by that URL.
</li>
</ol>
</div>
<p>Then:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Locate <code class="literal">url-discover</code> events to confirm the crawler has seen links to your page.
The <code class="literal">outcome</code> and <code class="literal">message</code> fields may explain why the web crawler did not crawl the page.
</li>
<li class="listitem">
If <code class="literal">url-discover</code> events indicate discovery was successful, locate <code class="literal">url-fetch</code> events to analyze the fetching phase of the crawl.
</li>
</ul>
</div>
<p><span class="strong strong"><strong>Analyze web crawler system logs:</strong></span></p>
<p>These may contain additional information about specific pages.</p>
<p>See <a class="xref" href="crawl-web-content.html#crawl-web-content-view-web-crawler-system-logs" title="View web crawler system logs">View web crawler system logs</a>.</p>
<p><span class="strong strong"><strong>Address specific content discovery problems:</strong></span></p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Problem</th>
<th align="left" valign="top">Description</th>
<th align="left" valign="top">Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p>External domain</p></td>
<td align="left" valign="top"><p>The web crawler does not follow links that go outside the <a class="xref" href="web-crawler-reference.html#web-crawler-reference-domain" title="Domain">domains</a> configured for each crawl.</p></td>
<td align="left" valign="top"><p><a class="xref" href="crawl-web-content.html#crawl-web-content-manage-domains" title="Manage domains">Manage domains</a> for your crawl to add any missing domains.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Disallowed path</p></td>
<td align="left" valign="top"><p>The web crawler does not follow links whose paths are disallowed by a domain&#8217;s <a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl-rule" title="Crawl rule">crawl rules</a> or <a class="xref" href="web-crawler-reference.html#web-crawler-reference-robots-txt-file" title="Robots.txt file">robots.txt</a> directives.</p></td>
<td align="left" valign="top"><p>Manage <a class="xref" href="crawl-web-content.html#crawl-web-content-manage-crawl-rules" title="Manage crawl rules">crawl rules</a> and <a class="xref" href="crawl-web-content.html#crawl-web-content-manage-robots-txt-files" title="Manage robots.txt files">robots.txt files</a> for each domain to ensure paths are allowed.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>No incoming links</p></td>
<td align="left" valign="top"><p>The web crawler cannot find pages that have no incoming links, unless you provide the path as an entry point. See <a class="xref" href="web-crawler-reference.html#web-crawler-reference-content-discovery" title="Content discovery">Content discovery</a> for an explanation of how the web crawler discovers content.</p></td>
<td align="left" valign="top"><p>Add links to the content from other content that the web crawler has already discovered, or explicitly add the URL <a class="xref" href="crawl-web-content.html#crawl-web-content-manage-entry-points" title="Manage entry points">as an entry point</a> or <a class="xref" href="crawl-web-content.html#crawl-web-content-manage-sitemaps" title="Manage sitemaps">within a sitemap</a>.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Nofollow links</p></td>
<td align="left" valign="top"><p>The web crawler does not follow <a class="xref" href="web-crawler-reference.html#web-crawler-reference-nofollow-link" title="Nofollow link">nofollow links</a>.</p></td>
<td align="left" valign="top"><p>Remove the nofollow link to allow content discovery.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">nofollow</code> robots meta tag</p></td>
<td align="left" valign="top"><p>If a page contains a <code class="literal">nofollow</code> <a class="xref" href="web-crawler-reference.html#web-crawler-reference-robots-meta-tags" title="Robots meta tags">robots meta tag</a>, the web crawler will not follows links from that page.</p></td>
<td align="left" valign="top"><p>Remove the meta tag from your page.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Page too large</p></td>
<td align="left" valign="top"><p>The web crawler does not parse HTTP responses larger than <code class="literal">crawler.http.response_size.limit</code>.</p></td>
<td align="left" valign="top"><p>Reduce the size of your page.
Or, increase the limit for your deployment.
Increasing the limit may increase crawl durations and resource consumption, and could reduce crawl stability.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Too many redirects</p></td>
<td align="left" valign="top"><p>The web crawler does not follow redirect chains longer than <code class="literal">crawler.http.redirects.limit</code>.</p></td>
<td align="left" valign="top"><p>Reduce the number of redirects for the page.
Or, increase the limit for your deployment.
Increasing the limit may increase crawl durations and resource consumption, and could reduce crawl stability.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Network latency</p></td>
<td align="left" valign="top"><p>The web crawler fails requests that exceed the following network timeouts:
<code class="literal">crawler.http.connection_timeout</code>,
<code class="literal">crawler.http.read_timeout</code>,
<code class="literal">crawler.http.request_timeout</code>.</p></td>
<td align="left" valign="top"><p>Reduce network latency.
Or, increase these timeouts for your deployment.
Increasing the timeouts may increase crawl durations and resource consumption, and could reduce crawl stability.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>HTTP errors</p></td>
<td align="left" valign="top"><p>The web crawler cannot discover and index content if it cannot fetch HTML pages from a domain.
The web crawler will not index pages that respond with a <code class="literal">4xx</code> or <code class="literal">5xx</code> response code.</p></td>
<td align="left" valign="top"><p>Fix HTTP server errors.
Ensure correct HTTP response codes.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>HTML errors</p></td>
<td align="left" valign="top"><p>The web crawler cannot parse extremely broken HTML pages.
In that case, the web crawler cannot index the page, and cannot discover links coming from that page.</p></td>
<td align="left" valign="top"><p>Use the <a href="https://validator.w3.org/" class="ulink" target="_blank" rel="noopener">W3C markup validation service</a> to identify and resolve HTML errors in your content.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Security</p></td>
<td align="left" valign="top"><p>The web crawler cannot access content requiring authentication or authorization.</p></td>
<td align="left" valign="top"><p>Remove the security to allow access to the web crawler.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Non-HTML content</p></td>
<td align="left" valign="top"><p>The web crawler does not extract and index non-HTML content (e.g. JavaScript, PDF).</p></td>
<td align="left" valign="top"><p>Publish your content in HTML format.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Non-HTTP protocol</p></td>
<td align="left" valign="top"><p>The web crawler recognizes only the HTTP and HTTPS protocols.</p></td>
<td align="left" valign="top"><p>Publish your content at URLs using HTTP or HTTPS protocols.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Invalid SSL certificate</p></td>
<td align="left" valign="top"><p>The web crawler will not crawl HTTPS pages with invalid certificates.</p></td>
<td align="left" valign="top"><p>Replace invalid certificates with valid certificates.</p></td>
</tr>
</tbody>
</table>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-troubleshoot-content-extraction-and-indexing"></a>Troubleshoot content extraction and indexing<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>The web crawler may be discovering your content but not extracting and indexing it as expected.
It&#8217;s helpful to understand how the web crawler extracts and indexes content.
See <a class="xref" href="web-crawler-reference.html#web-crawler-reference-content-extraction-and-indexing" title="Content extraction and indexing">Content extraction and indexing</a> in the web crawler reference.</p>
<p>Use the following techniques to troubleshoot content discovery issues.</p>
<p><span class="strong strong"><strong>Confirm the most recent crawl completed successfully:</strong></span></p>
<p>View the status of the most recent crawl to confirm it completed successfully.
See <a class="xref" href="crawl-web-content.html#crawl-web-content-view-crawl-status" title="View crawl status">View crawl status</a>.</p>
<p>If the crawl failed, look for signs of crawl stability issues.
See <a class="xref" href="crawl-web-content.html#crawl-web-content-troubleshoot-crawl-stability" title="Troubleshoot crawl stability">Troubleshoot crawl stability</a>.</p>
<p><span class="strong strong"><strong>View indexed documents to confirm missing pages:</strong></span></p>
<p>Identify which pages are missing from your engine, or focus on specific pages.
See <a class="xref" href="crawl-web-content.html#crawl-web-content-view-indexed-documents" title="View indexed documents">View indexed documents</a> for instructions to view all documents and specific documents.</p>
<p>If documents are missing from the engine, look for signs of content discovery issues.
See <a class="xref" href="crawl-web-content.html#crawl-web-content-troubleshoot-content-discovery" title="Troubleshoot content discovery">Troubleshoot content discovery</a>.</p>
<p><span class="strong strong"><strong>Use the <em>extract URL</em> API operation to extract content:</strong></span></p>
<p>Use the <em>extract URL</em> API operation to extract the content of a specific URL without requesting a full crawl.
Use this API to dynamically update content (as the content owner) and extract the content (as the crawler operator).</p>
<p>See <a class="xref" href="web-crawler-api-reference.html#web-crawler-apis-post-crawler-extract-url" title="Extract content from a URL">Extract content from a URL</a>.</p>
<p><span class="strong strong"><strong>Analyze web crawler events logs for the most recent crawl:</strong></span></p>
<p>First:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-view-crawl-request-id" title="View crawl request ID">Find the crawl request ID</a> for the most recent crawl.
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-view-web-crawler-events-by-crawl-id-and-url" title="View web crawler events by crawl ID and URL">Filter the web crawler events logs</a>by that ID.
</li>
<li class="listitem">
Find the URL of a specific document missing from the engine.
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-view-web-crawler-events-by-crawl-id-and-url" title="View web crawler events by crawl ID and URL">Filter the web crawler events logs</a> by that URL.
</li>
</ol>
</div>
<p>Then:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Locate <code class="literal">url-extracted</code> events to confirm the crawler was able to extract content from your page.
The <code class="literal">outcome</code> and <code class="literal">message</code> fields may explain why the web crawler could not extract and index the content.
</li>
<li class="listitem">
If <code class="literal">url-extracted</code> events indicate extraction was successful, locate <code class="literal">url-output</code> events to confirm the web crawler attempted ingestion of the page&#8217;s content.
</li>
</ul>
</div>
<p><span class="strong strong"><strong>Analyze web crawler system logs:</strong></span></p>
<p>These may contain additional information about specific pages.</p>
<p>See <a class="xref" href="crawl-web-content.html#crawl-web-content-view-web-crawler-system-logs" title="View web crawler system logs">View web crawler system logs</a>.</p>
<p><span class="strong strong"><strong>Address specific content extraction and indexing problems:</strong></span></p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Problem</th>
<th align="left" valign="top">Description</th>
<th align="left" valign="top">Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p>Duplicate content</p></td>
<td align="left" valign="top"><p>If your website contains pages with duplicate content, those pages are stored as a single document within your engine.
The document&#8217;s <code class="literal">additional_urls</code> field indicates the URLs that contain the same content.</p></td>
<td align="left" valign="top"><p>Use a <a class="xref" href="web-crawler-reference.html#web-crawler-reference-canonical-url-link-tag" title="Canonical URL link tag">canonical URL link tag</a> within any document containing duplicate content.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Non-HTML content</p></td>
<td align="left" valign="top"><p>The web crawler does not extract and index non-HTML content (e.g. JavaScript, PDF).</p></td>
<td align="left" valign="top"><p>Publish your content in HTML format.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">noindex</code> robots meta tag</p></td>
<td align="left" valign="top"><p>The web crawler will not index pages that include a <code class="literal">noindex</code> <a class="xref" href="web-crawler-reference.html#web-crawler-reference-robots-meta-tags" title="Robots meta tags">robots meta tag</a>.</p></td>
<td align="left" valign="top"><p>Remove the meta tag from your page.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Page too large</p></td>
<td align="left" valign="top"><p>The web crawler does not parse HTTP responses larger than <code class="literal">crawler.http.response_size.limit</code>.</p></td>
<td align="left" valign="top"><p>Reduce the size of your page.
Or, increase the limit for your deployment.
Increasing the limit may increase crawl durations and resource consumption, and could reduce crawl stability.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Truncated fields</p></td>
<td align="left" valign="top"><p>The web crawler truncates some fields before indexing the document, according to the following limits:
<code class="literal">crawler.extraction.body_size.limit</code>
<code class="literal">crawler.extraction.description_size.limit</code>,
<code class="literal">crawler.extraction.headings_count.limit</code>,
<code class="literal">crawler.extraction.indexed_links_count.limit</code>,
<code class="literal">crawler.extraction.keywords_size.limit</code>,
<code class="literal">crawler.extraction.title_size.limit</code>.</p></td>
<td align="left" valign="top"><p>Reduce the length of these fields within your content.
Or, increase these limits for your deployment.
Increasing the limits may increase crawl durations and resource consumption, and could reduce crawl stability.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Broken HTML</p></td>
<td align="left" valign="top"><p>The web crawler cannot parse extremely broken HTML pages.</p></td>
<td align="left" valign="top"><p>Use the <a href="https://validator.w3.org/" class="ulink" target="_blank" rel="noopener">W3C markup validation service</a> to identify and resolve HTML errors in your content.</p></td>
</tr>
</tbody>
</table>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-provide-feedback"></a>Provide feedback<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>After troubleshooting your crawl, we&#8217;d love to know what worked, what didn&#8217;t, and what we can improve.</p>
<p>Please <a class="xref" href="web-crawler.html#web-crawler-feedback" title="Provide web crawler feedback">send us your feedback</a>.</p>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="crawl-web-content-re-crawl"></a>Re-crawl web content<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h3>
</div></div></div>
<p>For each engine, repeat the <em>manage-monitor-troubleshoot</em> cycle until the web crawler is discovering and indexing your documents as expected.</p>
<p>From there, you move into the next cycle: update your web content, re-crawl your web content, (repeat).</p>
<p>At this point, you may want to move beyond manual crawls and <a class="xref" href="crawl-web-content.html#crawl-web-content-schedule-crawls" title="Schedule crawls">schedule crawls</a> or set up <a class="xref" href="crawl-web-content.html#crawl-web-content-automatic-crawling" title="Automatic crawling">automatic crawling</a> instead.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="crawl-web-content-re-apply-crawl-rules"></a>Re-apply crawl rules<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h3>
</div></div></div>
<p>After <a class="xref" href="crawl-web-content.html#crawl-web-content-manage-crawl-rules" title="Manage crawl rules">modifying crawl rules</a>, you can re-apply the updated rules to your existing documents without running a full crawl.
The web crawler will remove all existing documents that are no longer allowed by your current crawl rules.
This operation is called a <a class="xref" href="web-crawler-reference.html#web-crawler-reference-process-crawl" title="Process crawl">process crawl</a>.</p>
<p>Using the web crawler UI, you can re-apply crawl rules for <em>all</em> domains or a <em>specific</em> domain.
The only difference in the process is where you initiate the request.</p>
<p><span class="strong strong"><strong>Re-apply crawl rules to <em>all</em> domains:</strong></span></p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Navigate to <span class="strong strong"><strong>Enterprise Search</strong></span> &#8594; <span class="strong strong"><strong>App Search</strong></span> &#8594; <span class="strong strong"><strong>Engines</strong></span> &#8594; <em>engine name</em> &#8594; <span class="strong strong"><strong>Web crawler</strong></span>
</li>
<li class="listitem">
<p>Choose <span class="strong strong"><strong>Manage crawls</strong></span>, then <span class="strong strong"><strong>Re-apply crawl rules</strong></span></p>
<div class="imageblock">
<div class="content">
<img src="images/app-search/re-apply-crawl-rules.png" alt="re apply crawl rules">
</div>
</div>
</li>
</ol>
</div>
<p><span class="strong strong"><strong>Re-apply crawl rules to a <em>specific</em> domain:</strong></span></p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Navigate to <span class="strong strong"><strong>Enterprise Search</strong></span> &#8594; <span class="strong strong"><strong>App Search</strong></span> &#8594; <span class="strong strong"><strong>Engines</strong></span> &#8594; <em>engine name</em> &#8594; <span class="strong strong"><strong>Web crawler</strong></span> &#8594; <em>domain name</em>
</li>
<li class="listitem">
<p>Choose <span class="strong strong"><strong>Manage crawls</strong></span>, then <span class="strong strong"><strong>Re-apply crawl rules</strong></span>:</p>
<div class="imageblock">
<div class="content">
<img src="images/app-search/re-apply-crawl-rules.png" alt="re apply crawl rules">
</div>
</div>
</li>
</ol>
</div>
<p>Alternatively, you can re-apply crawl rules using the web crawler API.
See <a class="xref" href="web-crawler-api-reference.html#web-crawler-apis-process-crawls" title="Process crawls">Process crawls</a>.</p>
<p>It is recommended to cancel any active web crawls before opting to re-apply crawl rules.
A web crawl that runs concurrently with a process crawl may continue to index fresh documents with out of date configuration; the changes in crawl rule configuration will only apply to documents indexed at the time of the request.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="crawl-web-content-automatic-crawling"></a>Automatic crawling<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h3>
</div></div></div>
<p>Configure the cadence for new crawls to start automatically.
New crawls will be skipped if there is an active crawl.</p>
<p>Manage automatic crawling within the web crawler or domain dashboard, or using the App Search API.</p>
<p>To use a dashboard, navigate to <span class="strong strong"><strong>Web Crawler</strong></span>, then optionally choose a domain to manage.
Choose the <span class="strong strong"><strong>Manage Crawls</strong></span> button at the top of the dashboard.</p>
<div class="imageblock">
<div class="content">
<img src="images/app-search/configure-automatic-crawling.png" alt="configure automatic crawling">
</div>
</div>
<p>To manage automatic crawling programmatically, refer to the following API references:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="web-crawler-api-reference.html#web-crawler-apis-get-crawler-crawl-schedule" title="Get current crawl schedule">Get current crawl schedule</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-api-reference.html#web-crawler-apis-put-crawler-crawl-schedule" title="Create or update a crawl schedule">Create or update a crawl schedule</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-api-reference.html#web-crawler-apis-delete-crawler-crawl-schedule" title="Delete a crawl schedule">Delete a crawl schedule</a>
</li>
</ul>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="crawl-web-content-schedule-crawls"></a>Schedule crawls<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h3>
</div></div></div>
<p>You may require more control over your crawl schedule&#8212;&#8203;perhaps to crawl at a specific time each day.</p>
<p>Use the following API operation to request a new crawl programmatically: <a class="xref" href="web-crawler-api-reference.html#web-crawler-apis-post-crawler-crawl-requests" title="Create a new crawl request">Create a new crawl request</a>.</p>
<p>Trigger this operation when needed.
Use a job scheduler, like <a href="https://en.wikipedia.org/wiki/Cron" class="ulink" target="_blank" rel="noopener">cron</a>, or write your own application code.</p>
</div>

</div>
<div class="navfooter">
<span class="prev">
<a href="analytics-tags-guide.html">« Analytics Tags Guide</a>
</span>
<span class="next">
<a href="curations-guide.html">Curations »</a>
</span>
</div>
</div>

                  <!-- end body -->
                </div>

                <div class="col-12 order-3 col-lg-2 order-lg-3 h-almost-full-lg sticky-top-lg" id="right_col">
                  <div id="sticky_content">
                    <!-- The OTP is appended here -->
                    <div class="row">
                      <div class="col-0 col-md-4 col-lg-0" id="bottom_left_col"></div>
                      <div class="col-12 col-md-8 col-lg-12">
                        <div id="rtpcontainer">
                          <div class="mktg-promo" id="most-popular">
                            <p class="aside-heading">Most Popular</p>
                            <div class="pb-2">
                              <p class="media-type">Video</p>
                              <a href="https://www.elastic.co/webinars/getting-started-elasticsearch?baymax=default&elektra=docs&storm=top-video">
                                <p class="mb-0">Get Started with Elasticsearch</p>
                              </a>
                            </div>
                            <div class="pb-2">
                              <p class="media-type">Video</p>
                              <a href="https://www.elastic.co/webinars/getting-started-kibana?baymax=default&elektra=docs&storm=top-video">
                                <p class="mb-0">Intro to Kibana</p>
                              </a>
                            </div>
                            <div class="pb-2">
                              <p class="media-type">Video</p>
                              <a href="https://www.elastic.co/webinars/introduction-elk-stack?baymax=default&elektra=docs&storm=top-video">
                                <p class="mb-0">ELK for Logs & Metrics</p>
                              </a>
                            </div>
                          </div>
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </section>

        </div>


<div id='elastic-footer'></div>
<script src='https://www.elastic.co/elastic-footer.js'></script>
<!-- Footer Section end-->

      </section>
    </div>

<script src="/guide/static/jquery.js"></script>
<script type="text/javascript" src="/guide/static/docs.js"></script>
<script type="text/javascript">
  window.initial_state = {}</script>
  </body>
</html>
