<!DOCTYPE html>
<html lang="en-us">
  <head>
    
<meta charset="UTF-8">
<meta name="keywords" content="hot-spotting, hotspot, hot-spot, hot spot, hotspots, hotspotting">
<title>Troubleshooting shards capacity health issues | Elasticsearch Guide [8.9] | Elastic</title>
<meta class="elastic" name="content" content="Troubleshooting shards capacity health issues | Elasticsearch Guide [8.9]">

<link rel="home" href="index.html" title="Elasticsearch Guide [8.9]"/>
<link rel="up" href="troubleshooting.html" title="Troubleshooting"/>
<link rel="prev" href="troubleshooting-searches.html" title="Troubleshooting searches"/>
<link rel="next" href="rest-apis.html" title="REST APIs"/>
<meta class="elastic" name="product_version" content="8.9"/>
<meta class="elastic" name="product_name" content="Elasticsearch"/>
<meta class="elastic" name="website_area" content="documentation"/>
<meta name="DC.type" content="Learn/Docs/Elasticsearch/Reference/8.9"/>
<meta name="DC.subject" content="Elasticsearch"/>
<meta name="DC.identifier" content="8.9"/>
<meta name="robots" content="noindex,nofollow"/>

    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script src="https://cdn.optimizely.com/js/18132920325.js"></script>
    <link rel="apple-touch-icon" sizes="57x57" href="/apple-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="60x60" href="/apple-icon-60x60.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/apple-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="76x76" href="/apple-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/apple-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="120x120" href="/apple-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/apple-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="152x152" href="/apple-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-icon-180x180.png">
    <link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/android-chrome-192x192.png" sizes="192x192">
    <link rel="icon" type="image/png" href="/favicon-96x96.png" sizes="96x96">
    <link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
    <link rel="manifest" href="/manifest.json">
    <meta name="apple-mobile-web-app-title" content="Elastic">
    <meta name="application-name" content="Elastic">
    <meta name="msapplication-TileColor" content="#ffffff">
    <meta name="msapplication-TileImage" content="/mstile-144x144.png">
    <meta name="theme-color" content="#ffffff">
    <meta name="naver-site-verification" content="936882c1853b701b3cef3721758d80535413dbfd" />
    <meta name="yandex-verification" content="d8a47e95d0972434" />
    <meta name="localized" content="true" />
    <meta name="st:robots" content="follow,index" />
    <meta property="og:image" content="https://static-www.elastic.co/v3/assets/bltefdd0b53724fa2ce/blt280217a63b82a734/6202d3378b1f312528798412/elastic-logo.svg" />
    <meta property="og:image:width" content="500" />
    <meta property="og:image:height" content="172" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <link rel="apple-touch-icon-precomposed" sizes="64x64" href="/favicon_64x64_16bit.png">
    <link rel="apple-touch-icon-precomposed" sizes="32x32" href="/favicon_32x32.png">
    <link rel="apple-touch-icon-precomposed" sizes="16x16" href="/favicon_16x16.png">
    <!-- Give IE8 a fighting chance -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <link rel="stylesheet" type="text/css" href="/guide/static/styles.css" />
  </head>

  <!--© 2015-2022 Elasticsearch B.V. -->
  <!-- All Elastic documentation is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License. -->
  <!-- http://creativecommons.org/licenses/by-nc-nd/4.0/ -->

  <body>
    <!-- Google Tag Manager -->
    <script>dataLayer = [];</script><noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-58RLH5" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start': new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0], j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src= '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f); })(window,document,'script','dataLayer','GTM-58RLH5');</script>
    <!-- End Google Tag Manager -->

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-12395217-16"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-12395217-16');
    </script>

    <!-- Google Tag Manager for GA4 -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start': new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0], j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-KNJMG2M');</script>
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-KNJMG2M" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager for GA4-->

    <div id='elastic-nav' style="display:none;"></div>
    <script src='https://www.elastic.co/elastic-nav.js'></script>

    <div class="main-container">
      <section id="content" >
        <div class="content-wrapper">

          <section id="guide" lang="en">
            <div class="container-fluid">
              <div class="row pb-3">
                <div class="col-12 order-2 col-md-4 order-md-1 col-lg-3 h-almost-full-md sticky-top-md" id="left_col">
                  <!-- The TOC is appended here -->
                </div>

                <div class="col-12 order-1 col-md-8 order-md-2 col-lg-7 order-lg-2 guide-section" id="middle_col">
                  <!-- start body -->
                  <div class="page_header">
You are looking at preliminary documentation for a future release.
Not what you want? See the
<a href="../current/index.html">current release documentation</a>.
</div>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="index.html">Elasticsearch Guide [8.9]</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="troubleshooting.html">Troubleshooting</a></span>
</div>
<div class="navheader">
<span class="prev">
<a href="troubleshooting-searches.html">« Troubleshooting searches</a>
</span>
<span class="next">
<a href="rest-apis.html">REST APIs »</a>
</span>
</div>
<div class="chapter">
<div class="titlepage"><div><div>
<h2 class="title"><a id="troubleshooting-shards-capacity-issues"></a>Troubleshooting shards capacity health issues<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/troubleshooting/troubleshooting-shards-capacity.asciidoc">edit</a></h2>
</div></div></div>
<p>Elasticsearch limits the maximum number of shards to be held per node using the
<a class="xref" href="misc-cluster-settings.html#cluster-max-shards-per-node"><code class="literal">cluster.max_shards_per_node</code></a> and
<a class="xref" href="misc-cluster-settings.html#cluster-max-shards-per-node-frozen"><code class="literal">cluster.max_shards_per_node.frozen</code></a> settings.
The current shards capacity of the cluster is available in the
<a class="xref" href="health-api.html#health-api-response-details-shards-capacity" title="shards_capacity">health API shards capacity section</a>.</p>
<h3><a id="_cluster_is_close_to_reaching_the_configured_maximum_number_of_shards_for_data_nodes"></a>Cluster is close to reaching the configured maximum number of shards for data nodes.<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/tab-widgets/troubleshooting/troubleshooting-shards-capacity-widget.asciidoc">edit</a></h3>
<p>The <a class="xref" href="misc-cluster-settings.html#cluster-max-shards-per-node"><code class="literal">cluster.max_shards_per_node</code></a> cluster
setting limits the maximum number of open shards for a cluster, only counting data nodes
that do not belong to the frozen tier.</p>
<p>This symptom indicates that action should be taken, otherwise, either the creation of new
indices or upgrading the cluster could be blocked.</p>
<p>If you&#8217;re confident your changes won&#8217;t destabilize the cluster, you can
temporarily increase the limit using the <a class="xref" href="cluster-update-settings.html" title="Cluster update settings API">cluster update settings API</a>:</p>
<div class="tabs" data-tab-group="host">
  <div role="tablist" aria-label="Troubleshoot shards capacity for non-frozen nodes">
    <button role="tab"
            aria-selected="true"
            aria-controls="cloud-tab-shards-capacity-non-frozen"
            id="cloud-shards-capacity-non-frozen">
      Elasticsearch Service
    </button>
    <button role="tab"
            aria-selected="false"
            aria-controls="self-managed-tab-shards-capacity-non-frozen"
            id="self-managed-shards-capacity-non-frozen"
            tabindex="-1">
      Self-managed
    </button>
  </div>
  <div tabindex="0"
       role="tabpanel"
       id="cloud-tab-shards-capacity-non-frozen"
       aria-labelledby="cloud-shards-capacity-non-frozen">
<p><span class="strong strong"><strong>Use Kibana</strong></span></p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Log in to the <a href="https://cloud.elastic.co?baymax=docs-body&amp;elektra=docs" class="ulink" target="_top">Elastic Cloud console</a>.
</li>
<li class="listitem">
<p>On the <span class="strong strong"><strong>Elasticsearch Service</strong></span> panel, click the name of your deployment.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>If the name of your deployment is disabled your Kibana instances might be
unhealthy, in which case please contact <a href="https://support.elastic.co" class="ulink" target="_top">Elastic Support</a>.
If your deployment doesn&#8217;t include Kibana, all you need to do is
<a href="/guide/en/cloud/current/ec-access-kibana.html" class="ulink" target="_top">enable it first</a>.</p>
</div>
</div>
</li>
<li class="listitem">
<p>Open your deployment&#8217;s side navigation menu (placed under the Elastic logo in the upper left corner)
and go to <span class="strong strong"><strong>Dev Tools &gt; Console</strong></span>.</p>
<div class="imageblock text-center screenshot">
<div class="content">
<img src="images/kibana-console.png" alt="Kibana Console">
</div>
</div>
</li>
<li class="listitem">
<p>Check the current status of the cluster according the shards capacity indicator:</p>
<div class="pre_wrapper lang-ruby alternative">
<pre class="programlisting prettyprint lang-ruby alternative">response = client.health_report(
  feature: 'shards_capacity'
)
puts response</pre>
</div>
<a id="5797df4b8e71d821a1488cbb63481104"></a>
<div class="pre_wrapper lang-console default has-ruby">
<pre class="programlisting prettyprint lang-console default has-ruby">GET _health_report/shards_capacity</pre>
</div>
<div class="console_widget has-ruby" data-snippet="snippets/2054.console"></div>
<p>The response will look like this:</p>
<a id="d1525f4575ac704dcdf4a69bdcfd5f94"></a>
<div class="pre_wrapper lang-console-result">
<pre class="programlisting prettyprint lang-console-result">{
  "cluster_name": "...",
  "indicators": {
    "shards_capacity": {
      "status": "yellow",
      "symptom": "Cluster is close to reaching the configured maximum number of shards for data nodes.",
      "details": {
        "data": {
          "max_shards_in_cluster": 1000, <a id="CO666-1"></a><i class="conum" data-value="1"></i>
          "current_used_shards": 988 <a id="CO666-2"></a><i class="conum" data-value="2"></i>
        },
        "frozen": {
          "max_shards_in_cluster": 3000,
          "current_used_shards": 0
        }
      },
      "impacts": [
        ...
      ],
      "diagnosis": [
        ...
    }
  }
}</pre>
</div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO666-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>Current value of the setting <code class="literal">cluster.max_shards_per_node</code></p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO666-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p>Current number of open shards across the cluster</p>
</td>
</tr>
</table>
</div>
</li>
<li class="listitem">
<p>Update the <a class="xref" href="misc-cluster-settings.html#cluster-max-shards-per-node"><code class="literal">cluster.max_shards_per_node</code></a> setting with a proper value:</p>
<div class="pre_wrapper lang-ruby alternative">
<pre class="programlisting prettyprint lang-ruby alternative">response = client.cluster.put_settings(
  body: {
    persistent: {
      "cluster.max_shards_per_node": 1200
    }
  }
)
puts response</pre>
</div>
<a id="96b9289c3c4c6b135ab3386562c4ee8d"></a>
<div class="pre_wrapper lang-console default has-ruby">
<pre class="programlisting prettyprint lang-console default has-ruby">PUT _cluster/settings
{
  "persistent" : {
    "cluster.max_shards_per_node": 1200
  }
}</pre>
</div>
<div class="console_widget has-ruby" data-snippet="snippets/2055.console"></div>
<p>This increase should only be temporary. As a long-term solution, we recommend
you add nodes to the oversharded data tier or
<a class="xref" href="size-your-shards.html#reduce-cluster-shard-count" title="Reduce a cluster&#8217;s shard count">reduce your cluster&#8217;s shard count</a> on nodes that do not belong
to the frozen tier.</p>
</li>
<li class="listitem">
<p>To verify that the change has fixed the issue, you can get the current
status of the <code class="literal">shards_capacity</code> indicator by checking the <code class="literal">data</code> section of the
<a class="xref" href="health-api.html#health-api-example" title="Examples">health API</a>:</p>
<div class="pre_wrapper lang-ruby alternative">
<pre class="programlisting prettyprint lang-ruby alternative">response = client.health_report(
  feature: 'shards_capacity'
)
puts response</pre>
</div>
<a id="5797df4b8e71d821a1488cbb63481104"></a>
<div class="pre_wrapper lang-console default has-ruby">
<pre class="programlisting prettyprint lang-console default has-ruby">GET _health_report/shards_capacity</pre>
</div>
<div class="console_widget has-ruby" data-snippet="snippets/2056.console"></div>
<p>The response will look like this:</p>
<a id="2e3292fe3ab645da12a5e96157cff635"></a>
<div class="pre_wrapper lang-console-result">
<pre class="programlisting prettyprint lang-console-result">{
  "cluster_name": "...",
  "indicators": {
    "shards_capacity": {
      "status": "green",
      "symptom": "The cluster has enough room to add new shards.",
      "details": {
        "data": {
          "max_shards_in_cluster": 1000
        },
        "frozen": {
          "max_shards_in_cluster": 3000
        }
      }
    }
  }
}</pre>
</div>
</li>
<li class="listitem">
<p>When a long-term solution is in place, we recommend you reset the
<code class="literal">cluster.max_shards_per_node</code> limit.</p>
<div class="pre_wrapper lang-ruby alternative">
<pre class="programlisting prettyprint lang-ruby alternative">response = client.cluster.put_settings(
  body: {
    persistent: {
      "cluster.max_shards_per_node": nil
    }
  }
)
puts response</pre>
</div>
<a id="3af10fde8138d9d95df127d39d9a0ed2"></a>
<div class="pre_wrapper lang-console default has-ruby">
<pre class="programlisting prettyprint lang-console default has-ruby">PUT _cluster/settings
{
  "persistent" : {
    "cluster.max_shards_per_node": null
  }
}</pre>
</div>
<div class="console_widget has-ruby" data-snippet="snippets/2057.console"></div>
</li>
</ol>
</div>
  </div>
  <div tabindex="0"
       role="tabpanel"
       id="self-managed-tab-shards-capacity-non-frozen"
       aria-labelledby="self-managed-shards-capacity-non-frozen"
       hidden="">
<p>Check the current status of the cluster according the shards capacity indicator:</p>
<div class="pre_wrapper lang-ruby alternative">
<pre class="programlisting prettyprint lang-ruby alternative">response = client.health_report(
  feature: 'shards_capacity'
)
puts response</pre>
</div>
<a id="5797df4b8e71d821a1488cbb63481104"></a>
<div class="pre_wrapper lang-console default has-ruby">
<pre class="programlisting prettyprint lang-console default has-ruby">GET _health_report/shards_capacity</pre>
</div>
<div class="console_widget has-ruby" data-snippet="snippets/2058.console"></div>
<p>The response will look like this:</p>
<a id="64e0b312d7662096f8a1676082425dd2"></a>
<div class="pre_wrapper lang-console-result">
<pre class="programlisting prettyprint lang-console-result">{
  "cluster_name": "...",
  "indicators": {
    "shards_capacity": {
      "status": "yellow",
      "symptom": "Cluster is close to reaching the configured maximum number of shards for data nodes.",
      "details": {
        "data": {
          "max_shards_in_cluster": 1000, <a id="CO667-1"></a><i class="conum" data-value="1"></i>
          "current_used_shards": 988 <a id="CO667-2"></a><i class="conum" data-value="2"></i>
        },
        "frozen": {
          "max_shards_in_cluster": 3000
        }
      },
      "impacts": [
        ...
      ],
      "diagnosis": [
        ...
    }
  }
}</pre>
</div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO667-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>Current value of the setting <code class="literal">cluster.max_shards_per_node</code></p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO667-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p>Current number of open shards across the cluster</p>
</td>
</tr>
</table>
</div>
<p>Using the <a class="xref" href="cluster-update-settings.html" title="Cluster update settings API"><code class="literal">cluster settings API</code></a>, update the
<a class="xref" href="misc-cluster-settings.html#cluster-max-shards-per-node"><code class="literal">cluster.max_shards_per_node</code></a> setting:</p>
<div class="pre_wrapper lang-ruby alternative">
<pre class="programlisting prettyprint lang-ruby alternative">response = client.cluster.put_settings(
  body: {
    persistent: {
      "cluster.max_shards_per_node": 1200
    }
  }
)
puts response</pre>
</div>
<a id="96b9289c3c4c6b135ab3386562c4ee8d"></a>
<div class="pre_wrapper lang-console default has-ruby">
<pre class="programlisting prettyprint lang-console default has-ruby">PUT _cluster/settings
{
  "persistent" : {
    "cluster.max_shards_per_node": 1200
  }
}</pre>
</div>
<div class="console_widget has-ruby" data-snippet="snippets/2059.console"></div>
<p>This increase should only be temporary. As a long-term solution, we recommend
you add nodes to the oversharded data tier or
<a class="xref" href="size-your-shards.html#reduce-cluster-shard-count" title="Reduce a cluster&#8217;s shard count">reduce your cluster&#8217;s shard count</a> on nodes that do not belong
to the frozen tier. To verify that the change has fixed the issue, you can get the current
status of the <code class="literal">shards_capacity</code> indicator by checking the <code class="literal">data</code> section of the
<a class="xref" href="health-api.html#health-api-example" title="Examples">health API</a>:</p>
<div class="pre_wrapper lang-ruby alternative">
<pre class="programlisting prettyprint lang-ruby alternative">response = client.health_report(
  feature: 'shards_capacity'
)
puts response</pre>
</div>
<a id="5797df4b8e71d821a1488cbb63481104"></a>
<div class="pre_wrapper lang-console default has-ruby">
<pre class="programlisting prettyprint lang-console default has-ruby">GET _health_report/shards_capacity</pre>
</div>
<div class="console_widget has-ruby" data-snippet="snippets/2060.console"></div>
<p>The response will look like this:</p>
<a id="e8ba57391f3b24a4b1860a578893150b"></a>
<div class="pre_wrapper lang-console-result">
<pre class="programlisting prettyprint lang-console-result">{
  "cluster_name": "...",
  "indicators": {
    "shards_capacity": {
      "status": "green",
      "symptom": "The cluster has enough room to add new shards.",
      "details": {
        "data": {
          "max_shards_in_cluster": 1200
        },
        "frozen": {
          "max_shards_in_cluster": 3000
        }
      }
    }
  }
}</pre>
</div>
<p>When a long-term solution is in place, we recommend you reset the
<code class="literal">cluster.max_shards_per_node</code> limit.</p>
<div class="pre_wrapper lang-ruby alternative">
<pre class="programlisting prettyprint lang-ruby alternative">response = client.cluster.put_settings(
  body: {
    persistent: {
      "cluster.max_shards_per_node": nil
    }
  }
)
puts response</pre>
</div>
<a id="3af10fde8138d9d95df127d39d9a0ed2"></a>
<div class="pre_wrapper lang-console default has-ruby">
<pre class="programlisting prettyprint lang-console default has-ruby">PUT _cluster/settings
{
  "persistent" : {
    "cluster.max_shards_per_node": null
  }
}</pre>
</div>
<div class="console_widget has-ruby" data-snippet="snippets/2061.console"></div>
  </div>
</div>
<h3><a id="_cluster_is_close_to_reaching_the_configured_maximum_number_of_shards_for_frozen_nodes"></a>Cluster is close to reaching the configured maximum number of shards for frozen nodes.<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.9/docs/reference/tab-widgets/troubleshooting/troubleshooting-shards-capacity-widget.asciidoc">edit</a></h3>
<p>The <a class="xref" href="misc-cluster-settings.html#cluster-max-shards-per-node-frozen"><code class="literal">cluster.max_shards_per_node.frozen</code></a> cluster
setting limits the maximum number of open shards for a cluster, only counting data nodes
that belong to the frozen tier.</p>
<p>This symptom indicates that action should be taken, otherwise, either the creation of new
indices or upgrading the cluster could be blocked.</p>
<p>If you&#8217;re confident your changes won&#8217;t destabilize the cluster, you can
temporarily increase the limit using the <a class="xref" href="cluster-update-settings.html" title="Cluster update settings API">cluster update settings API</a>:</p>
<div class="tabs" data-tab-group="host">
  <div role="tablist" aria-label="Troubleshoot shards capacity for frozen nodes">
    <button role="tab"
            aria-selected="true"
            aria-controls="cloud-tab-shards-capacity-frozen"
            id="cloud-shards-capacity">
      Elasticsearch Service
    </button>
    <button role="tab"
            aria-selected="false"
            aria-controls="self-managed-tab-shards-capacity-frozen"
            id="self-managed-shards-capacity-frozen"
            tabindex="-1">
      Self-managed
    </button>
  </div>
  <div tabindex="0"
       role="tabpanel"
       id="cloud-tab-shards-capacity-frozen"
       aria-labelledby="cloud-shards-capacity-frozen">
<p><span class="strong strong"><strong>Use Kibana</strong></span></p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Log in to the <a href="https://cloud.elastic.co?baymax=docs-body&amp;elektra=docs" class="ulink" target="_top">Elastic Cloud console</a>.
</li>
<li class="listitem">
<p>On the <span class="strong strong"><strong>Elasticsearch Service</strong></span> panel, click the name of your deployment.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>If the name of your deployment is disabled your Kibana instances might be
unhealthy, in which case please contact <a href="https://support.elastic.co" class="ulink" target="_top">Elastic Support</a>.
If your deployment doesn&#8217;t include Kibana, all you need to do is
<a href="/guide/en/cloud/current/ec-access-kibana.html" class="ulink" target="_top">enable it first</a>.</p>
</div>
</div>
</li>
<li class="listitem">
<p>Open your deployment&#8217;s side navigation menu (placed under the Elastic logo in the upper left corner)
and go to <span class="strong strong"><strong>Dev Tools &gt; Console</strong></span>.</p>
<div class="imageblock text-center screenshot">
<div class="content">
<img src="images/kibana-console.png" alt="Kibana Console">
</div>
</div>
</li>
<li class="listitem">
<p>Check the current status of the cluster according the shards capacity indicator:</p>
<div class="pre_wrapper lang-ruby alternative">
<pre class="programlisting prettyprint lang-ruby alternative">response = client.health_report(
  feature: 'shards_capacity'
)
puts response</pre>
</div>
<a id="5797df4b8e71d821a1488cbb63481104"></a>
<div class="pre_wrapper lang-console default has-ruby">
<pre class="programlisting prettyprint lang-console default has-ruby">GET _health_report/shards_capacity</pre>
</div>
<div class="console_widget has-ruby" data-snippet="snippets/2062.console"></div>
<p>The response will look like this:</p>
<a id="7409d9055a73dcae63b36230fbf4a383"></a>
<div class="pre_wrapper lang-console-result">
<pre class="programlisting prettyprint lang-console-result">{
  "cluster_name": "...",
  "indicators": {
    "shards_capacity": {
      "status": "yellow",
      "symptom": "Cluster is close to reaching the configured maximum number of shards for frozen nodes.",
      "details": {
        "data": {
          "max_shards_in_cluster": 1000
        },
        "frozen": {
          "max_shards_in_cluster": 3000, <a id="CO668-1"></a><i class="conum" data-value="1"></i>
          "current_used_shards": 2998 <a id="CO668-2"></a><i class="conum" data-value="2"></i>
        }
      },
      "impacts": [
        ...
      ],
      "diagnosis": [
        ...
    }
  }
}</pre>
</div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO668-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>Current value of the setting <code class="literal">cluster.max_shards_per_node.frozen</code></p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO668-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p>Current number of open shards used by frozen nodes across the cluster</p>
</td>
</tr>
</table>
</div>
</li>
<li class="listitem">
<p>Update the <a class="xref" href="misc-cluster-settings.html#cluster-max-shards-per-node-frozen"><code class="literal">cluster.max_shards_per_node.frozen</code></a> setting:</p>
<div class="pre_wrapper lang-ruby alternative">
<pre class="programlisting prettyprint lang-ruby alternative">response = client.cluster.put_settings(
  body: {
    persistent: {
      "cluster.max_shards_per_node.frozen": 3200
    }
  }
)
puts response</pre>
</div>
<a id="51f6cb682424e110f289af79c106f4c7"></a>
<div class="pre_wrapper lang-console default has-ruby">
<pre class="programlisting prettyprint lang-console default has-ruby">PUT _cluster/settings
{
  "persistent" : {
    "cluster.max_shards_per_node.frozen": 3200
  }
}</pre>
</div>
<div class="console_widget has-ruby" data-snippet="snippets/2063.console"></div>
<p>This increase should only be temporary. As a long-term solution, we recommend
you add nodes to the oversharded data tier or
<a class="xref" href="size-your-shards.html#reduce-cluster-shard-count" title="Reduce a cluster&#8217;s shard count">reduce your cluster&#8217;s shard count</a> on nodes that belong
to the frozen tier.</p>
</li>
<li class="listitem">
<p>To verify that the change has fixed the issue, you can get the current
status of the <code class="literal">shards_capacity</code> indicator by checking the <code class="literal">data</code> section of the
<a class="xref" href="health-api.html#health-api-example" title="Examples">health API</a>:</p>
<div class="pre_wrapper lang-ruby alternative">
<pre class="programlisting prettyprint lang-ruby alternative">response = client.health_report(
  feature: 'shards_capacity'
)
puts response</pre>
</div>
<a id="5797df4b8e71d821a1488cbb63481104"></a>
<div class="pre_wrapper lang-console default has-ruby">
<pre class="programlisting prettyprint lang-console default has-ruby">GET _health_report/shards_capacity</pre>
</div>
<div class="console_widget has-ruby" data-snippet="snippets/2064.console"></div>
<p>The response will look like this:</p>
<a id="60002bbd41921affb35f3658ac6b0c59"></a>
<div class="pre_wrapper lang-console-result">
<pre class="programlisting prettyprint lang-console-result">{
  "cluster_name": "...",
  "indicators": {
    "shards_capacity": {
      "status": "green",
      "symptom": "The cluster has enough room to add new shards.",
      "details": {
        "data": {
          "max_shards_in_cluster": 1000
        },
        "frozen": {
          "max_shards_in_cluster": 3200
        }
      }
    }
  }
}</pre>
</div>
</li>
<li class="listitem">
<p>When a long-term solution is in place, we recommend you reset the
<code class="literal">cluster.max_shards_per_node.frozen</code> limit.</p>
<div class="pre_wrapper lang-ruby alternative">
<pre class="programlisting prettyprint lang-ruby alternative">response = client.cluster.put_settings(
  body: {
    persistent: {
      "cluster.max_shards_per_node.frozen": nil
    }
  }
)
puts response</pre>
</div>
<a id="b638e11d6a8a084290f8934d224abd52"></a>
<div class="pre_wrapper lang-console default has-ruby">
<pre class="programlisting prettyprint lang-console default has-ruby">PUT _cluster/settings
{
  "persistent" : {
    "cluster.max_shards_per_node.frozen": null
  }
}</pre>
</div>
<div class="console_widget has-ruby" data-snippet="snippets/2065.console"></div>
</li>
</ol>
</div>
  </div>
  <div tabindex="0"
       role="tabpanel"
       id="self-managed-tab-shards-capacity-frozen"
       aria-labelledby="self-managed-shards-capacity-frozen"
       hidden="">
<p>Check the current status of the cluster according the shards capacity indicator:</p>
<div class="pre_wrapper lang-ruby alternative">
<pre class="programlisting prettyprint lang-ruby alternative">response = client.health_report(
  feature: 'shards_capacity'
)
puts response</pre>
</div>
<a id="5797df4b8e71d821a1488cbb63481104"></a>
<div class="pre_wrapper lang-console default has-ruby">
<pre class="programlisting prettyprint lang-console default has-ruby">GET _health_report/shards_capacity</pre>
</div>
<div class="console_widget has-ruby" data-snippet="snippets/2066.console"></div>
<a id="7409d9055a73dcae63b36230fbf4a383"></a>
<div class="pre_wrapper lang-console-result">
<pre class="programlisting prettyprint lang-console-result">{
  "cluster_name": "...",
  "indicators": {
    "shards_capacity": {
      "status": "yellow",
      "symptom": "Cluster is close to reaching the configured maximum number of shards for frozen nodes.",
      "details": {
        "data": {
          "max_shards_in_cluster": 1000
        },
        "frozen": {
          "max_shards_in_cluster": 3000, <a id="CO669-1"></a><i class="conum" data-value="1"></i>
          "current_used_shards": 2998 <a id="CO669-2"></a><i class="conum" data-value="2"></i>
        }
      },
      "impacts": [
        ...
      ],
      "diagnosis": [
        ...
    }
  }
}</pre>
</div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO669-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>Current value of the setting <code class="literal">cluster.max_shards_per_node.frozen</code>.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO669-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p>Current number of open shards used by frozen nodes across the cluster.</p>
</td>
</tr>
</table>
</div>
<p>Using the <a class="xref" href="cluster-update-settings.html" title="Cluster update settings API"><code class="literal">cluster settings API</code></a>, update the
<a class="xref" href="misc-cluster-settings.html#cluster-max-shards-per-node-frozen"><code class="literal">cluster.max_shards_per_node.frozen</code></a> setting:</p>
<div class="pre_wrapper lang-ruby alternative">
<pre class="programlisting prettyprint lang-ruby alternative">response = client.cluster.put_settings(
  body: {
    persistent: {
      "cluster.max_shards_per_node.frozen": 3200
    }
  }
)
puts response</pre>
</div>
<a id="51f6cb682424e110f289af79c106f4c7"></a>
<div class="pre_wrapper lang-console default has-ruby">
<pre class="programlisting prettyprint lang-console default has-ruby">PUT _cluster/settings
{
  "persistent" : {
    "cluster.max_shards_per_node.frozen": 3200
  }
}</pre>
</div>
<div class="console_widget has-ruby" data-snippet="snippets/2067.console"></div>
<p>This increase should only be temporary. As a long-term solution, we recommend
you add nodes to the oversharded data tier or
<a class="xref" href="size-your-shards.html#reduce-cluster-shard-count" title="Reduce a cluster&#8217;s shard count">reduce your cluster&#8217;s shard count</a> on nodes that belong
to the frozen tier. To verify that the change has fixed the issue, you can get the current
status of the <code class="literal">shards_capacity</code> indicator by checking the <code class="literal">data</code> section of the
<a class="xref" href="health-api.html#health-api-example" title="Examples">health API</a>:</p>
<div class="pre_wrapper lang-ruby alternative">
<pre class="programlisting prettyprint lang-ruby alternative">response = client.health_report(
  feature: 'shards_capacity'
)
puts response</pre>
</div>
<a id="5797df4b8e71d821a1488cbb63481104"></a>
<div class="pre_wrapper lang-console default has-ruby">
<pre class="programlisting prettyprint lang-console default has-ruby">GET _health_report/shards_capacity</pre>
</div>
<div class="console_widget has-ruby" data-snippet="snippets/2068.console"></div>
<p>The response will look like this:</p>
<a id="60002bbd41921affb35f3658ac6b0c59"></a>
<div class="pre_wrapper lang-console-result">
<pre class="programlisting prettyprint lang-console-result">{
  "cluster_name": "...",
  "indicators": {
    "shards_capacity": {
      "status": "green",
      "symptom": "The cluster has enough room to add new shards.",
      "details": {
        "data": {
          "max_shards_in_cluster": 1000
        },
        "frozen": {
          "max_shards_in_cluster": 3200
        }
      }
    }
  }
}</pre>
</div>
<p>When a long-term solution is in place, we recommend you reset the
<code class="literal">cluster.max_shards_per_node.frozen</code> limit.</p>
<div class="pre_wrapper lang-ruby alternative">
<pre class="programlisting prettyprint lang-ruby alternative">response = client.cluster.put_settings(
  body: {
    persistent: {
      "cluster.max_shards_per_node.frozen": nil
    }
  }
)
puts response</pre>
</div>
<a id="b638e11d6a8a084290f8934d224abd52"></a>
<div class="pre_wrapper lang-console default has-ruby">
<pre class="programlisting prettyprint lang-console default has-ruby">PUT _cluster/settings
{
  "persistent" : {
    "cluster.max_shards_per_node.frozen": null
  }
}</pre>
</div>
<div class="console_widget has-ruby" data-snippet="snippets/2069.console"></div>
  </div>
</div>
</div>
<div class="navfooter">
<span class="prev">
<a href="troubleshooting-searches.html">« Troubleshooting searches</a>
</span>
<span class="next">
<a href="rest-apis.html">REST APIs »</a>
</span>
</div>
</div>

                  <!-- end body -->
                </div>

                <div class="col-12 order-3 col-lg-2 order-lg-3 h-almost-full-lg sticky-top-lg" id="right_col">
                  <div id="sticky_content">
                    <!-- The OTP is appended here -->
                    <div class="row">
                      <div class="col-0 col-md-4 col-lg-0" id="bottom_left_col"></div>
                      <div class="col-12 col-md-8 col-lg-12">
                        <div id="rtpcontainer">
                          <div class="mktg-promo" id="most-popular">
                            <p class="aside-heading">Most Popular</p>
                            <div class="pb-2">
                              <p class="media-type">Video</p>
                              <a href="https://www.elastic.co/webinars/getting-started-elasticsearch?baymax=default&elektra=docs&storm=top-video">
                                <p class="mb-0">Get Started with Elasticsearch</p>
                              </a>
                            </div>
                            <div class="pb-2">
                              <p class="media-type">Video</p>
                              <a href="https://www.elastic.co/webinars/getting-started-kibana?baymax=default&elektra=docs&storm=top-video">
                                <p class="mb-0">Intro to Kibana</p>
                              </a>
                            </div>
                            <div class="pb-2">
                              <p class="media-type">Video</p>
                              <a href="https://www.elastic.co/webinars/introduction-elk-stack?baymax=default&elektra=docs&storm=top-video">
                                <p class="mb-0">ELK for Logs & Metrics</p>
                              </a>
                            </div>
                          </div>
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </section>

        </div>


<div id='elastic-footer'></div>
<script src='https://www.elastic.co/elastic-footer.js'></script>
<!-- Footer Section end-->

      </section>
    </div>

<script src="/guide/static/jquery.js"></script>
<script type="text/javascript" src="/guide/static/docs.js"></script>
<script type="text/javascript">
  window.initial_state = {"alternatives":{"console":{"php":{"hasAny":true},"python":{"hasAny":true},"ruby":{"hasAny":true},"go":{"hasAny":true},"js":{"hasAny":true}}}}</script>
  </body>
</html>
