<!DOCTYPE html>
<html lang="en-us">
  <head>
    
<meta charset="UTF-8">
<title>Operate the Universal Profiling backend | Elastic Observability [8.15] | Elastic</title>
<meta class="elastic" name="content" content="Operate the Universal Profiling backend | Elastic Observability [8.15]">

<link rel="home" href="index.html" title="Elastic Observability [8.15]"/>
<link rel="up" href="profiling-self-managed.html" title="Run Universal Profiling on self-hosted Elastic stack"/>
<link rel="prev" href="profiling-self-managed-install-next-steps.html" title="Step 5: Next steps"/>
<link rel="next" href="profiling-self-managed-troubleshooting.html" title="Troubleshoot the Universal Profiling backend"/>
<meta class="elastic" name="product_version" content="8.15"/>
<meta class="elastic" name="product_name" content="Observability"/>
<meta class="elastic" name="website_area" content="documentation"/>
<meta name="DC.type" content="Learn/Docs/Observability/Guide/8.15"/>
<meta name="DC.subject" content="Observability"/>
<meta name="DC.identifier" content="8.15"/>

    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script src="https://cdn.optimizely.com/js/18132920325.js"></script>
    <link rel="apple-touch-icon" sizes="57x57" href="/apple-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="60x60" href="/apple-icon-60x60.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/apple-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="76x76" href="/apple-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/apple-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="120x120" href="/apple-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/apple-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="152x152" href="/apple-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-icon-180x180.png">
    <link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/android-chrome-192x192.png" sizes="192x192">
    <link rel="icon" type="image/png" href="/favicon-96x96.png" sizes="96x96">
    <link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
    <link rel="manifest" href="/manifest.json">
    <meta name="apple-mobile-web-app-title" content="Elastic">
    <meta name="application-name" content="Elastic">
    <meta name="msapplication-TileColor" content="#ffffff">
    <meta name="msapplication-TileImage" content="/mstile-144x144.png">
    <meta name="theme-color" content="#ffffff">
    <meta name="naver-site-verification" content="936882c1853b701b3cef3721758d80535413dbfd" />
    <meta name="yandex-verification" content="d8a47e95d0972434" />
    <meta name="localized" content="true" />
    <meta name="st:robots" content="follow,index" />
    <meta property="og:image" content="https://static-www.elastic.co/v3/assets/bltefdd0b53724fa2ce/blt280217a63b82a734/6202d3378b1f312528798412/elastic-logo.svg" />
    <meta property="og:image:width" content="500" />
    <meta property="og:image:height" content="172" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <link rel="apple-touch-icon-precomposed" sizes="64x64" href="/favicon_64x64_16bit.png">
    <link rel="apple-touch-icon-precomposed" sizes="32x32" href="/favicon_32x32.png">
    <link rel="apple-touch-icon-precomposed" sizes="16x16" href="/favicon_16x16.png">
    <!-- Give IE8 a fighting chance -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <link rel="stylesheet" type="text/css" href="/guide/static/styles-v1.css" />
  </head>

  <!--© 2015-2025 Elasticsearch B.V. -->
  <!-- All Elastic documentation is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License. -->
  <!-- http://creativecommons.org/licenses/by-nc-nd/4.0/ -->

  <body>
    <!-- Google Tag Manager -->
    <script>dataLayer = [];</script><noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-58RLH5" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start': new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0], j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src= '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f); })(window,document,'script','dataLayer','GTM-58RLH5');</script>
    <!-- End Google Tag Manager -->

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-12395217-16"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-12395217-16');
    </script>

    <!-- Google Tag Manager for GA4 -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start': new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0], j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-KNJMG2M');</script>
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-KNJMG2M" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager for GA4-->

    <div id='elastic-nav' style="display:none;"></div>
    <script src='https://www.elastic.co/elastic-nav.js'></script>

    <div class="main-container">
      <section id="content" >
        <div class="content-wrapper">

          <section id="guide" lang="en">
            <div class="container-fluid">
              <div class="row pb-3">
                <div class="col-12 order-2 col-md-4 order-md-1 col-lg-3 h-almost-full-md sticky-top-md" id="left_col">
                  <!-- The TOC is appended here -->
                </div>

                <div class="col-12 order-1 col-md-8 order-md-2 col-lg-7 order-lg-2 guide-section" id="middle_col">
                  <!-- start body -->
                  
<div class="navheader">
<span class="prev">
<a href="profiling-self-managed-install-next-steps.html">« Step 5: Next steps</a>
</span>
<span class="next">
<a href="profiling-self-managed-troubleshooting.html">Troubleshoot the Universal Profiling backend »</a>
</span>
</div>
<div class="book" lang="en">
<div class="titlepage">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="index.html">Elastic Observability [8.15]</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="universal-profiling.html">Universal Profiling</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="profiling-self-managed.html">Run Universal Profiling on self-hosted Elastic stack</a></span>
</div>
<div>
<div><h1 class="title"><a id="id-1"></a>Operate the Universal Profiling backend</h1><a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/8.15/docs/en/observability/profiling-self-managed-ops.asciidoc">edit</a></div>
</div>
<!--EXTRA-->
</div>
<div id="content">
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="profiling-self-managed-ops"></a>Operate the Universal Profiling backend<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/8.15/docs/en/observability/profiling-self-managed-ops.asciidoc">edit</a></h3>
</div></div></div>

<p>This page outlines operating the backend when running Universal Profiling on a self-managed version of the Elastic Stack. Here you&#8217;ll find information on:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="profiling-self-managed-ops.html#profiling-self-managed-ops-sizing-guidance" title="Resource guide">Resource sizing</a>
</li>
<li class="listitem">
<a class="xref" href="profiling-self-managed-ops.html#profiling-self-managed-ops-configuration" title="Configure the collector and symbolizer">Configuring your collector and symbolizer</a>
</li>
<li class="listitem">
<a class="xref" href="profiling-self-managed-ops.html#profiling-self-managed-ops-monitoring" title="Monitoring">Monitoring your collector and symbolizer</a>
</li>
<li class="listitem">
<a class="xref" href="profiling-self-managed-ops.html#profiling-scaling-backend-resources" title="Scale resources">Scaling your resources</a>
</li>
<li class="listitem">
<a class="xref" href="profiling-self-managed-ops.html#profiling-self-managed-upgrade" title="Upgrade a self-hosted stack">Upgrading backend binaries</a>
</li>
<li class="listitem">
<a class="xref" href="profiling-self-managed-ops.html#profiling-self-managed-kubernetes-tips" title="Kubernetes tips">Kubernetes tips</a>
</li>
</ul>
</div>
<h5><a id="profiling-self-managed-ops-sizing-guidance"></a>Resource guide<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/8.15/docs/en/observability/profiling-self-managed-ops.asciidoc">edit</a></h5>
<p>The resources needed to ingest and query Universal Profiling data vary based on the total number of CPU cores you&#8217;re profiling.
The number of cores comes from the sum of all <em>virtual</em> cores as recorded in <code class="literal">/proc/cpuinfo</code>, adding up all the machines you&#8217;ll deploy the Universal Profiling Agent to.</p>
<p>Ingestion and query resource demand is almost directly proportional to the amount of data the Universal Profiling Agents generate.
Calculate the data generated by the Universal Profiling Agents using the number of CPU samples collected, the number of executables processed, and the executables' debug metadata size. While the number of CPU samples collected is predictable, the number of executables processed and the executables' debug metadata size is not.</p>
<p>The following table provides recommended resources for ingesting and querying Universal Profiling data based on your number of CPU cores:</p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
<col class="col_4"/>
<col class="col_5"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top"># of CPU cores</th>
<th align="left" valign="top">Elasticsearch total memory</th>
<th align="left" valign="top">Elasticsearch total storage (60 days retention)</th>
<th align="left" valign="top">Profiling Backend</th>
<th align="left" valign="top">Kibana memory</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p>1–100</p></td>
<td align="left" valign="top"><p>4GB–8GB</p></td>
<td align="left" valign="top"><p>250GB</p></td>
<td align="left" valign="top"><p>1 Collector 2GB, 1 Symbolizer 2GB</p></td>
<td align="left" valign="top"><p>2GB</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>100–1000</p></td>
<td align="left" valign="top"><p>8GB–32GB</p></td>
<td align="left" valign="top"><p>250GB–2TB</p></td>
<td align="left" valign="top"><p>1 Collector 4GB, 1 Symbolizer 4GB</p></td>
<td align="left" valign="top"><p>2GB</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>1000–10,000</p></td>
<td align="left" valign="top"><p>32GB–128GB</p></td>
<td align="left" valign="top"><p>2TB–8TB</p></td>
<td align="left" valign="top"><p>2 Collector 4GB, 1 Symbolizer 8GB</p></td>
<td align="left" valign="top"><p>4GB</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>10,000–50,000</p></td>
<td align="left" valign="top"><p>128GB–512GB</p></td>
<td align="left" valign="top"><p>8TB–16TB</p></td>
<td align="left" valign="top"><p>3+ Collector 4GB, 1 Symbolizer 8GB</p></td>
<td align="left" valign="top"><p>8GB</p></td>
</tr>
</tbody>
</table>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>This table is derived from benchmarks performed on Universal Profiling with ingestion of up to 15,000 CPU cores.
The profiled machines had a near-constant load of 75% CPU utilization.
The deployment used 3 Elasticsearch nodes with 64 GB memory each, 8 vCPU, and 1.5 TB NVMe disk drives.</p>
</div>
</div>
<p>Resource demand is nearly proportional to the amount of data the Universal Profiling Agents generate.
Therefore, you can calculate the necessary resources for use cases beyond those in the table by comparing your actual number of cores profiled with the number of cores in the table.
When calculating, consider the following:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
The average load of the machines being profiled: The average load directly impacts the amount of CPU samples collected. For example, on a system that is mostly idle, not all CPUs will be scheduling tasks during the sampling intervals.
</li>
<li class="listitem">
The rate of change of the executables being profiled—for example, how often you deploy new versions of your software: The rate of change impacts the amount of debug metadata stored in Elasticsearch as a result of symbolization; the more different executables the Universal Profiling Agent collects, the more debug data will be stored in Elasticsearch. Note that two different builds of the same application still result in two different executables, as the Universal Profiling Agent will treat each ELF file independently.
</li>
</ul>
</div>
<p>Storage considerations: the Elasticsearch disks' bandwidth and latency will affect the latency of ingesting and querying the profiling data.
Allocate data to hot nodes for best performance and user experience.
If storage becomes a concern, tune the data retention by customizing the Universal Profiling <a class="xref" href="profiling-index-lifecycle-management.html#profiling-ilm-custom-policy" title="Configure a custom index lifecycle policy">index lifecycle management policy</a>.</p>
<h5><a id="profiling-self-managed-ops-configuration"></a>Configure the collector and symbolizer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/8.15/docs/en/observability/profiling-self-managed-ops.asciidoc">edit</a></h5>
<p>You can configure the collector and symbolizer using the YAML file and CLI flags, with the CLI flags taking precedence over the YAML file.
The configuration files are created during the installation process, as seen in <a class="xref" href="profiling-self-managed-running.html#profiling-self-managed-running-linux-configfile" title="Create configuration files">Create configuration files section</a>.
Comments in the configuration files explain the purpose of each configuration option.</p>
<p>Restart the backend binaries after modifying the configuration files for changes to take effect.</p>
<h6><a id="profiling-self-managed-ops-configuration-cli-overrides"></a>Use CLI flags to override configuration file values<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/8.15/docs/en/observability/profiling-self-managed-ops.asciidoc">edit</a></h6>
<p>When building configuration options for each of the backend binaries, you can use CLI flags to override the values in the YAML configuration file.
The overrides <span class="strong strong"><strong>must</strong></span> contain the full path to the configuration option and must be in a key=value format. For example, <code class="literal">-E application.field.key=value</code>, where <code class="literal">application</code> is the name of the binary.</p>
<p>For example, to enable TLS in the HTTP server of the collector, you can pass the <code class="literal">-E pf-elastic-collector.ssl.enabled=true</code> flag.
This will override the <code class="literal">ssl.enabled</code> option found in the YAML configuration file.</p>
<h5><a id="profiling-self-managed-ops-monitoring"></a>Monitoring<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/8.15/docs/en/observability/profiling-self-managed-ops.asciidoc">edit</a></h5>
<p>Monitor the collector and symbolizer through <a class="xref" href="profiling-self-managed-ops.html#profiling-self-managed-ops-monitoring-logs" title="Logs">Logs</a> and <a class="xref" href="profiling-self-managed-ops.html#profiling-self-managed-ops-monitoring-metrics" title="Metrics">Metrics</a> to ensure the services are running and healthy.
Without both services running, profiling data will not be ingested and symbolized,
and querying Kibana won&#8217;t return data.</p>
<h6><a id="profiling-self-managed-ops-monitoring-logs"></a>Logs<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/8.15/docs/en/observability/profiling-self-managed-ops.asciidoc">edit</a></h6>
<p>The collector and symbolizer always log to standard output.
You can turn on debug logs by setting the <code class="literal">verbose</code> configuration option to <code class="literal">true</code> in the YAML configuration file.</p>
<p>Avoid using debug logs in production, as they can be very verbose and impact backend performance.
Only enable debug logs when troubleshooting a failed deployment or when instructed to do so by support.</p>
<p>Logs are formatted as "key=value" pairs, and Elasticsearch and Kibana can automatically parse them into fields.</p>
<p>A log collector, such as Filebeat, can collect and send logs to Elasticsearch for indexing and analysis.
Depending on how it&#8217;s installed, a Filebeat input of type <code class="literal">journald</code> (for OS packages), <code class="literal">log</code> (for binaries), or <code class="literal">container</code> can be used to process the logs.
Refer to the <a href="/guide/en/beats/filebeat/8.15/configuring-howto-filebeat.html" class="ulink" target="_top">filebeat documentation</a> for more information.</p>
<h6><a id="profiling-self-managed-ops-monitoring-metrics"></a>Metrics<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/8.15/docs/en/observability/profiling-self-managed-ops.asciidoc">edit</a></h6>
<p>Metrics are not exposed by default. Enable metrics in the <code class="literal">metrics</code> section in the YAML configuration files.
The collector and symbolizer can expose metrics in both JSON and Prometheus formats.</p>
<p>Metrics in JSON format can be exposed through an HTTP server or a Unix domain socket.
Prometheus metrics can only be exposed through an HTTP server.
Customize where the metrics are exposed using the <code class="literal">metrics.prometheus_host</code> and <code class="literal">metrics.expvar_host</code> configuration options.</p>
<p>You can use Metricbeat to scrape metrics.
Consume the JSON directly through the <code class="literal">http</code> module.
Consume the Prometheus endpoint using the <code class="literal">prometheus</code> module.
When using an HTTP server for either format, the URI to scrape metrics from is <code class="literal">/metrics</code>.</p>
<p>For example, the following collector configuration would expose metrics in Prometheus format on port 9090 and in JSON format on port 9191.
You can then scrape them by connecting to <code class="literal">http://127.0.0.1:9090/metrics</code> and <code class="literal">http://127.0.0.1:9191/metrics</code> respectively.</p>
<div class="pre_wrapper lang-yaml">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-yaml">pf-elastic-collector:
  metrics:
    prometheus_host: ":9090"
    expvar_host: ":9191"</pre>
</div>
<p>Optionally, you can also expose the <code class="literal">expvar</code> format over a Unix domain socket, by setting the <code class="literal">expvar_socket</code> configuration option to a valid path.
For example, the following collector configuration would expose metrics in Prometheus format on port 9090 and in JSON format over a Unix domain socket at <code class="literal">/tmp/collector.sock</code>.</p>
<div class="pre_wrapper lang-yaml">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-yaml">pf-elastic-collector:
  metrics:
    prometheus_host: ":9090"
    expvar_host: "/tmp/collector.sock"</pre>
</div>
<p>The following sections show the most relevant metrics exposed by the backend binaries.
Include these metrics in your monitoring dashboards to detect backend issues.</p>
<p><span class="strong strong"><strong>Common runtime metrics</strong></span></p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">process_cpu_seconds_total</code>: track the amount of CPU time used by the process.
</li>
<li class="listitem">
<code class="literal">process_resident_memory_bytes</code>: track the amount of RAM used by the process.
</li>
<li class="listitem">
<code class="literal">go_memstats_heap_sys_bytes</code>: track the amount of heap memory.
</li>
<li class="listitem">
<code class="literal">go_memstats_stack_sys_bytes</code>: track the amount of stack memory.
</li>
<li class="listitem">
<code class="literal">go_threads</code>: number of OS threads created by the runtime.
</li>
<li class="listitem">
<code class="literal">go_goroutines</code>: number of active goroutines.
</li>
</ul>
</div>
<p><span class="strong strong"><strong>Collector metrics</strong></span></p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">collection_agent.indexing.bulk_indexer_failure_count</code>: number of times the bulk indexer failed to ingest data in Elasticsearch.
</li>
<li class="listitem">
<code class="literal">collection_agent.indexing.document_count.*</code>: counter that represents the number of documents ingested in Elasticsearch for each index; can be used to calculate the rate of ingestion for each index.
</li>
<li class="listitem">
<code class="literal">grpc_server_handling_seconds</code>: histogram of the time spent by the gRPC server to handle requests.
</li>
<li class="listitem">
`grpc_server_msg_received_total: count of messages received by the gRPC server; can be used to calculate the rate of ingestion for each RPC.
</li>
<li class="listitem">
<code class="literal">grpc_server_handled_total</code>: count of messages processed by the gRPC server; can be used to calculate the availability of the gRPC server for each RPC.
</li>
</ul>
</div>
<p><span class="strong strong"><strong>Symbolizer metrics</strong></span></p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">symbols_app.indexing.bulk_indexer_failure_count</code>: number of times the bulk indexer failed to ingest data in Elasticsearch.
</li>
<li class="listitem">
<code class="literal">symbols_app.indexing.document_count.*</code>: counter that represents the number of documents ingested in Elasticsearch for each index; can be used to calculate the rate of ingestion for each index.
</li>
<li class="listitem">
<code class="literal">symbols_app.user_client.document_count.update.*</code>: counter that represents the number of existing documents that were updated in Elasticsearch for each index; when the rate increases, it can impact Elasticsearch performance.
</li>
</ul>
</div>
<p><span class="strong strong"><strong>Health checks</strong></span></p>
<p>The backend binaries expose two health check endpoints, <code class="literal">/live</code> and <code class="literal">/ready</code>, that you can use to monitor the health of the application.
The endpoints return a <code class="literal">200 OK</code> HTTP status code when the checks are successful.</p>
<p>The health check endpoints are hosted in the same HTTP server that accepts the incoming profiling data.
This endpoint is configured through the application&#8217;s <code class="literal">host</code> configuration option.</p>
<p>For example, if the collector is configured with the default value <code class="literal">host: 0.0.0.0:8260</code>, you can check the health of the application by running <code class="literal">curl -i localhost:8260/live</code> and <code class="literal">curl -i localhost:8260/ready</code>.</p>
<h5><a id="profiling-scaling-backend-resources"></a>Scale resources<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/8.15/docs/en/observability/profiling-self-managed-ops.asciidoc">edit</a></h5>
<p>In the <a class="xref" href="profiling-self-managed-ops.html#profiling-self-managed-ops-sizing-guidance" title="Resource guide">resource guidance table</a>, no options use more than one replica for the symbolizer.
We do not recommend scaling the number of symbolizer replicas because of the technical limitations of the current implementation.
We recommend scaling the symbolizer vertically, by increasing the memory and CPU cores it uses to process data.</p>
<p>You can increase the number of collector replicas at will, keeping their vertical sizing smaller, if this is more convenient for your deployment use case.
The collector has a linear increase in memory usage and CPU threads with the number of Universal Profiling Agents that it serves.
Keep in mind that since the Universal Profiling Agent/collector communication happens via gRPC, there may be long-lived TCP sessions that are bound to a single collector replica.
When scaling out the number of replicas, depending on the load balancer that you have in place fronting the collector&#8217;s endpoint, you may want to shut down the older replicas after adding new replicas.
This ensures that the load is evenly distributed across all replicas.</p>
<h5><a id="profiling-self-managed-upgrade"></a>Upgrade a self-hosted stack<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/8.15/docs/en/observability/profiling-self-managed-ops.asciidoc">edit</a></h5>
<p>Upgrading a self-hosted stack involves upgrading the backend applications and the agent.
We recommend upgrading the backend first, followed by the agent. This way, if you encounter problems with the backend, you can roll back to the previous version without needing to downgrade the agent.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>We recommend having the same version of the agent and the backend deployed.</p>
</div>
</div>
<p>We strive to maintain backward compatibility between minor versions.
Occasionally, changes to the data format may require having the same version of the agent and backend deployed.
When a breaking change in the protocol is introduced, the profiling agents that are not up to date will stop sending data.
The agent logs will report an error message indicating that the backend is not compatible with the agent (or vice versa).</p>
<p>The upgrade process steps vary depending on the installation method used.
You may have a combination of installation methods. For example, you might deploy the backend on ECE and the agents on Kubernetes.
In that case, refer to the specific sections (backend/agent) in each method.</p>
<div class="important admon">
<div class="icon"></div>
<div class="admon_content">
<p>Depending on your infrastructure setup, upgrading the backend may also update the endpoint exposed by the collector.
In this case, amend the agent configuration to connect to the new endpoint upon upgrade.</p>
</div>
</div>
<h6><a id="profiling-self-managed-upgrade-ece"></a>ECE<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/8.15/docs/en/observability/profiling-self-managed-ops.asciidoc">edit</a></h6>
<p>When using ECE, the upgrade process of the backend is part of the installation of a new ECE release.
You don&#8217;t need to perform any action to upgrade the backend applications, as they will be upgraded automatically.</p>
<p>For the agent deployment, you can upgrade the Fleet integration installed on the Elastic Agent if that&#8217;s how you&#8217;re deploying the agent.</p>
<h6><a id="profiling-self-managed-upgrade-k8s"></a>ECK or generic Kubernetes<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/8.15/docs/en/observability/profiling-self-managed-ops.asciidoc">edit</a></h6>
<p>Perform a helm upgrade of the backend charts using the <code class="literal">helm upgrade</code> command.
You may reuse existing values or provide the full values YAML file on each upgrade.</p>
<p>For the agent deployment, upgrading through the Helm chart is also the simplest option.</p>
<div class="important admon">
<div class="icon"></div>
<div class="admon_content">
<p>starting with version 8.15 the agent Helm chart has been renamed from <code class="literal">pf-host-agent</code> to <code class="literal">profiling-agent</code>.</p>
</div>
</div>
<p>When <span class="strong strong"><strong>upgrading to 8.15 from 8.14 or lower</strong></span>, follow these additional instructions:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
<p>Fetch the currently applied Helm values:</p>
<pre class="literallayout">helm -n universal-profiling get values pf-host-agent -oyaml &gt; profiling-agent-values.yaml</pre>

</li>
<li class="listitem">
<p>Update the repo to find the new chart:</p>
<pre class="literallayout">helm repo update</pre>

</li>
<li class="listitem">
<p>Uninstall the old chart:</p>
<pre class="literallayout">helm -n uninstall pf-host-agent</pre>

</li>
<li class="listitem">
<p>Install the new chart by following the instructions displayed in the Universal Profiling "Add Data" page or with the following command:</p>
<pre class="literallayout">helm install -n universal-profiling universal-profiling-agent elastic/profiling-agent -f profiling-agent-values.yaml</pre>

</li>
</ol>
</div>
<h6><a id="profiling-self-managed-upgrade-os"></a>OS packages<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/8.15/docs/en/observability/profiling-self-managed-ops.asciidoc">edit</a></h6>
<p>Upgrade the package version using the OS package manager.
You will find the name and links to the new packages in the "Add Data" page.</p>
<p>Not all package managers will call into <code class="literal">systemd</code> to restart the service,
so you may need to restart the service manually or through any other automation in place.</p>
<h6><a id="profiling-self-managed-upgrade-binaries"></a>Binaries<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/8.15/docs/en/observability/profiling-self-managed-ops.asciidoc">edit</a></h6>
<p>Download the corresponding binary version and replace the existing one, using the command seen in the <a class="xref" href="profiling-self-managed-running.html#profiling-self-managed-running-linux-binary" title="Binary">Binary</a> section of the setup guide.
Replace the old binary and restart the services.</p>
<p>You will find the links to the new binaries in the "Add Data" page, under the "Binary" tab.</p>
<h5><a id="profiling-self-managed-kubernetes-tips"></a>Kubernetes tips<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/8.15/docs/en/observability/profiling-self-managed-ops.asciidoc">edit</a></h5>
<p>When deploying the Universal Profiling backend on Kubernetes, there are some best practices to follow.</p>
<h6><a id="_ingress_configuration"></a>Ingress configuration<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/8.15/docs/en/observability/profiling-self-managed-ops.asciidoc">edit</a></h6>
<p>If you are using an ingress controller, the connection routing to the collector Service should be configured to use the gRPC protocol.</p>
<p>We provide an <code class="literal">Ingress</code> resource as part of the Helm chart. Because the ingress can be any implementation,
you must configure the controller with a class name and
any necessary annotations using the <code class="literal">ingress.annotations</code> field.</p>
<p>For example, when using an NGINX ingress controller,
set the annotation <code class="literal">nginx.ingress.kubernetes.io/backend-protocol: "GRPC"</code>, as shown in the following example:</p>
<div class="pre_wrapper lang-yaml">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-yaml">ingress:
  create: true
  ingressClassName: "nginx"
  annotations:
    nginx.ingress.kubernetes.io/backend-protocol: "GRPC"</pre>
</div>
<p>For symbolizer, the connection routing should be configured to use the HTTP protocol.
There is usually no need to customize annotations for this type of service, but the chart provides similar configuration options.</p>
<h6><a id="_output_tls_configuration"></a>Output TLS configuration<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/8.15/docs/en/observability/profiling-self-managed-ops.asciidoc">edit</a></h6>
<p>You can secure the communication between the Universal Profiling backend and the Elasticsearch cluster by enabling TLS
in the <code class="literal">output.elasticsearch</code> section of the collector and symbolizer configuration files.</p>
<p>To do so, Kubernetes secrets containing the TLS key pairs should be provisioned in the namespace where the backend is installed.
In case of self-signed certificates, the CA bundle used to validate Elasticsearch&#8217;s certificates should also be part of the secret.</p>
<p>Create two secrets, one for the collector and one for the symbolizer, with the names <code class="literal">pf-symbolizer-tls-certificate</code> and <code class="literal">pf-collector-tls-certificate</code>.
The secrets should contain the following keys:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">tls.key</code>: the certificate private key
</li>
<li class="listitem">
<code class="literal">tls.cert</code>: the certificate public key
</li>
<li class="listitem">
<code class="literal">ca.cert</code> (optional): the certificate CA bundle
</li>
</ul>
</div>
<p>Follow these steps to enable TLS connection from collector/symbolizer to Elasticsearch:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
<p>Create secrets with the TLS key pairs (omit the <code class="literal">ca.pem</code> field if you are not using a self-signed CA):</p>
<div class="pre_wrapper lang-terminal">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-terminal">kubectl -n universal-profiling create secret generic pf-collector-tls-certificate --from-file=tls.key=/path/to/key.pem \
--from-file=tls.cert=/path/to/cert.pem --from-file=ca.pem=/path/to/ca.crt</pre>
</div>
<div class="pre_wrapper lang-terminal">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-terminal">kubectl -n universal-profiling create secret generic pf-symbolizer-tls-certificate --from-file=tls.key=/path/to/key.pem \
--from-file=tls.cert=/path/to/cert.pem --from-file=ca.pem=/path/to/ca.crt</pre>
</div>
</li>
<li class="listitem">
<p>Update the collector and symbolizer Helm values files to enable the use of TLS configuration, uncommenting the <code class="literal">output.elasticsearch.ssl</code> section:</p>
<div class="pre_wrapper lang-yaml">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-yaml">output:
  elasticsearch:
    ssl:
      enabled: true</pre>
</div>
</li>
<li class="listitem">
Upgrade the charts using the <code class="literal">helm upgrade</code> command, providing the updated values file.
</li>
</ol>
</div>
<h6><a id="_horizontal_scaling"></a>Horizontal scaling<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/8.15/docs/en/observability/profiling-self-managed-ops.asciidoc">edit</a></h6>
<p>When scaling the Universal Profiling backend on Kubernetes, you can increase the number of replicas for the collector, or
enable Horizontal Pod Autoscaling V2.</p>
<p>To enable HPAv2 for the collector or symbolizer, you can set the <code class="literal">autoscalingV2</code> dictionary in each Helm values file.</p>
<p>At the moment, <span class="strong strong"><strong>it is not recommended to enable an autoscaler for symbolizer</strong></span>.
Due to a current limitation on how symbolizer replicas can synchronize their workloads, it is best
to only use a single replica for the symbolizer.
Scale the symbolizer vertically first.
Only in case of high latency in symbolizing native frames (10+ minutes) you can evaluate adding more replicas.</p>
</div>
</div>
</div><div class="navfooter">
<span class="prev">
<a href="profiling-self-managed-install-next-steps.html">« Step 5: Next steps</a>
</span>
<span class="next">
<a href="profiling-self-managed-troubleshooting.html">Troubleshoot the Universal Profiling backend »</a>
</span>
</div>

                  <!-- end body -->
                </div>

                <div class="col-12 order-3 col-lg-2 order-lg-3 h-almost-full-lg sticky-top-lg" id="right_col">
                  <div id="sticky_content">
                    <!-- The OTP is appended here -->
                    <div class="row">
                      <div class="col-0 col-md-4 col-lg-0" id="bottom_left_col"></div>
                      <div class="col-12 col-md-8 col-lg-12">
                        <div id="rtpcontainer">
                          <div class="mktg-promo" id="most-popular">
                            <p class="aside-heading">Most Popular</p>
                            <div class="pb-2">
                              <p class="media-type">Video</p>
                              <a href="https://www.elastic.co/webinars/getting-started-elasticsearch?page=docs&placement=top-video">
                                <p class="mb-0">Get Started with Elasticsearch</p>
                              </a>
                            </div>
                            <div class="pb-2">
                              <p class="media-type">Video</p>
                              <a href="https://www.elastic.co/webinars/getting-started-kibana?page=docs&placement=top-video">
                                <p class="mb-0">Intro to Kibana</p>
                              </a>
                            </div>
                            <div class="pb-2">
                              <p class="media-type">Video</p>
                              <a href="https://www.elastic.co/webinars/introduction-elk-stack?page=docs&placement=top-video">
                                <p class="mb-0">ELK for Logs & Metrics</p>
                              </a>
                            </div>
                          </div>
                        </div>

                        <!-- Feedback widget -->
                        <div id="feedbackWidgetContainer"></div>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </section>

        </div>


        <div id='elastic-footer'></div>
        <script src='https://www.elastic.co/elastic-footer.js'></script>
        <!-- Footer Section end-->

      </section>
    </div>

    <!-- Feedback modal -->
    <div id="feedbackModalContainer"></div>

    <script src="/guide/static/jquery.js"></script>
    <script type="text/javascript" src="/guide/static/docs-v1.js"></script>
    <script type="text/javascript">
  window.initial_state = {}</script>
  </body>
</html>
