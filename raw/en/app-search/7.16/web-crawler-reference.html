<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Web crawler reference | Elastic App Search Documentation [7.16] | Elastic</title>
<link rel="home" href="index.html" title="Elastic App Search Documentation [7.16]"/>
<link rel="up" href="guides.html" title="Guides"/>
<link rel="prev" href="web-crawler-faq.html" title="Web crawler FAQ"/>
<link rel="next" href="web-crawler-events-logs-reference.html" title="Web crawler events logs reference"/>
<meta name="DC.type" content="Learn/Docs/App Search/Guide/7.16"/>
<meta name="DC.subject" content="App Search"/>
<meta name="DC.identifier" content="7.16"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body><div class="page_header">
<strong>IMPORTANT</strong>: No additional bug fixes or documentation updates
will be released for this version. For the latest information, see the
<a href="../current/index.html">current release documentation</a>.
</div>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="index.html">Elastic App Search Documentation [7.16]</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="guides.html">Guides</a></span>
</div>
<div class="navheader">
<span class="prev">
<a href="web-crawler-faq.html">« Web crawler FAQ</a>
</span>
<span class="next">
<a href="web-crawler-events-logs-reference.html">Web crawler events logs reference »</a>
</span>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="web-crawler-reference"></a>Web crawler reference<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h2>
</div></div></div>
<p>See <a class="xref" href="web-crawler.html" title="Web crawler">Web crawler</a> for a guided introduction to the Elastic Enterprise Search web crawler.</p>
<p>Refer to the following sections for terms, definitions, tables, and other detailed information.</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl" title="Crawl">Crawl</a>
</li>
<li class="listitem">
<p><a class="xref" href="web-crawler-reference.html#web-crawler-reference-content-discovery" title="Content discovery">Content discovery</a></p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl-state" title="Crawl state">Crawl state</a>
</li>
</ul>
</div>
</li>
<li class="listitem">
<p><a class="xref" href="web-crawler-reference.html#web-crawler-reference-content-extraction-and-indexing" title="Content extraction and indexing">Content extraction and indexing</a></p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-content-deduplication" title="Duplicate document handling">Duplicate document handling</a>
</li>
</ul>
</div>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-content-inclusion-and-exclusion" title="Content inclusion and exclusion">Content inclusion and exclusion</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-content-deletion" title="Content deletion">Content deletion</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-domain" title="Domain">Domain</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-entry-point" title="Entry point">Entry point</a>
</li>
<li class="listitem">
<p><a class="xref" href="web-crawler-reference.html#web-crawler-reference-sitemap" title="Sitemap">Sitemap</a></p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-sitemap-discovery-management" title="Sitemap discovery and management">Sitemap discovery and management</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-sitemap-format" title="Sitemap format and technical specification">Sitemap format and technical specification</a>
</li>
</ul>
</div>
</li>
<li class="listitem">
<p><a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl-rule" title="Crawl rule">Crawl rule</a></p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl-rule-rules" title="Crawl rule logic (rules)">Crawl rule logic (rules)</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl-rule-matching" title="Crawl rule matching">Crawl rule matching</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl-rule-order" title="Crawl rule order">Crawl rule order</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl-rule-restricting-paths" title="Restricting paths using crawl rules">Restricting paths using crawl rules</a>
</li>
</ul>
</div>
</li>
<li class="listitem">
<p><a class="xref" href="web-crawler-reference.html#web-crawler-reference-robots-txt-file" title="Robots.txt file">Robots.txt file</a></p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-robots-txt-file-discovery-management" title="Robots.txt file discovery and management">Robots.txt file discovery and management</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-robots-txt-file-format" title="Robots.txt file format and technical specification">Robots.txt file format and technical specification</a>
</li>
</ul>
</div>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-canonical-url-link-tag" title="Canonical URL link tag">Canonical URL link tag</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-robots-meta-tags" title="Robots meta tags">Robots meta tags</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-meta-tags-content-extraction" title="Meta tags and data attributes to extract custom fields">Meta tags and data attributes to extract custom fields</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-nofollow-link" title="Nofollow link">Nofollow link</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl-status" title="Crawl status">Crawl status</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-process-crawl" title="Process crawl">Process crawl</a>
</li>
<li class="listitem">
<p><a class="xref" href="web-crawler-reference.html#web-crawler-reference-http-response-status-codes" title="HTTP response status codes handling">HTTP response status codes handling</a></p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-http-response-status-codes-html" title="HTML response code handling">HTML response code handling</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-http-response-status-codes-robots-txt" title="Robots.txt file response code handling">Robots.txt file response code handling</a>
</li>
</ul>
</div>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-web-crawler-schema" title="Web crawler schema">Web crawler schema</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-web-crawler-events-logs" title="Web crawler events logs">Web crawler events logs</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-web-crawler-configuration-settings" title="Web crawler configuration settings">Web crawler configuration settings</a>
</li>
</ul>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-crawl"></a>Crawl<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>A <em>crawl</em> is a process, associated with an engine, by which the web crawler <a class="xref" href="web-crawler-reference.html#web-crawler-reference-content-discovery" title="Content discovery">discovers web content</a>, and <a class="xref" href="web-crawler-reference.html#web-crawler-reference-content-extraction-and-indexing" title="Content extraction and indexing">extracts and indexes that content</a> into the engine as search documents.</p>
<p>During a crawl, the web crawler stays within user-defined <a class="xref" href="web-crawler-reference.html#web-crawler-reference-domain" title="Domain">domains</a>, starting from specific <a class="xref" href="web-crawler-reference.html#web-crawler-reference-entry-point" title="Entry point">entry points</a>, and it discovers additional content according to <a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl-rule" title="Crawl rule">crawl rules</a>.
Operators and administrators <a class="xref" href="crawl-web-content.html#crawl-web-content-manage-crawl" title="Manage crawl">manage crawls</a> through the App Search dashboard.</p>
<p>Each crawl also respects instructions within <a class="xref" href="web-crawler-reference.html#web-crawler-reference-robots-txt-file" title="Robots.txt file">robots.txt files</a> and <a class="xref" href="web-crawler-reference.html#web-crawler-reference-sitemap" title="Sitemap">sitemaps</a>, and instructions embedded within HTML files using <a class="xref" href="web-crawler-reference.html#web-crawler-reference-canonical-url-link-tag" title="Canonical URL link tag">canonical link tags</a>, <a class="xref" href="web-crawler-reference.html#web-crawler-reference-robots-meta-tags" title="Robots meta tags">robots meta tags</a>, and <a class="xref" href="web-crawler-reference.html#web-crawler-reference-nofollow-link" title="Nofollow link">nofollow links</a>.</p>
<p>Operators can <a class="xref" href="crawl-web-content.html#crawl-web-content-monitor-crawl" title="Monitor crawl">monitor crawls</a> using the ID and status of each crawl, and using the web crawler events logs.</p>
<p>Each engine may run one active crawl at a time, but multiple engines can run crawls concurrently.</p>
<p>Each re-crawl is a full crawl.
Partial crawls are not supported at this time.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-content-discovery"></a>Content discovery<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>To begin each crawl, the web crawler populates a <em>crawl queue</em> with the <a class="xref" href="web-crawler-reference.html#web-crawler-reference-entry-point" title="Entry point">entry points</a> for the crawl and the URLs provided by any <a class="xref" href="web-crawler-reference.html#web-crawler-reference-sitemap" title="Sitemap">sitemaps</a>.
It then begins fetching and parsing each page in the queue.
The crawler handles each page according to its <a class="xref" href="web-crawler-reference.html#web-crawler-reference-http-response-status-codes" title="HTTP response status codes handling">HTTP response status code</a>.</p>
<p>As the web crawler discovers additional URLs, it uses the <a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl-rule" title="Crawl rule">crawl rules</a> from the crawl configuration, the directives from any <a class="xref" href="web-crawler-reference.html#web-crawler-reference-robots-txt-file" title="Robots.txt file">robots.txt files</a>, and instructions embedded within the content to determine which of those URLs it is allowed to crawl.
The crawler adds the allowed URLs to the crawl queue.</p>
<p>If a page is not linked from other pages, the web crawler will not discover it.
Refer to the web of pages in the following image.
The crawler will not discover the page outlined in red, unless an admin adds its URL as an entry point for the domain or includes the URL in a sitemap:</p>
<div class="imageblock">
<div class="content">
<img src="images/app-search/web-of-content.png" alt="web of content">
</div>
</div>
<p>The web crawler continues fetching and adding to the crawl queue until the URL queue is empty, the crawler hits a resource limit, or the crawl fails unexpectedly.</p>
<p>The crawler logs detailed events while it crawls, which allow monitoring and auditing of the content discovery process.</p>
<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="web-crawler-reference-crawl-state"></a>Crawl state<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h4>
</div></div></div>
<p>Each crawl persists its state to Elasticsearch, allowing the crawl to resume after a restart or shutdown.</p>
<p>The persisted data includes:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<em>Crawl URL queue:</em> A list of all outstanding URLs the crawler needs to visit before the end of the crawl.
</li>
<li class="listitem">
<em>Seen URLs list:</em> A list of all unique URLs the crawler has seen during the crawl.
The crawler uses this list to avoid visiting the same page twice.
</li>
</ul>
</div>
<p>Crawl state indexes are automatically removed at the end of each crawl.
The name of each of these indexes includes the ID of the corresponding crawl request.
You can use this information to clean up disk space if the automated cleanup process fails for any reason.</p>
<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<p>Depending on the size of your site, crawl state indexes may become large.
Each page you have will generate a record in at least one of those indexes, meaning sites with millions of pages will generate indexes with millions of records.</p>
<p>Monitor disk space on your Elasticsearch clusters, and ensure your deployment has enough disk space to avoid stability issues.</p>
</div>
</div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-content-extraction-and-indexing"></a>Content extraction and indexing<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p><em>Content extraction</em> is the process where the web crawler transforms an HTML document into a search document.
And <em>indexing</em> is the process where the crawler puts the search document into an App Search engine for searching.</p>
<p>During content extraction, the web crawler <a class="xref" href="web-crawler-reference.html#web-crawler-reference-content-deduplication" title="Duplicate document handling">handles duplicate documents</a>.
The crawler indexes duplicate web documents as a single search document.</p>
<p>The web crawler extracts each web document into a predefined <a class="xref" href="web-crawler-reference.html#web-crawler-reference-web-crawler-schema" title="Web crawler schema">schema</a> before indexing it as a search document.
Custom field extraction is supported via <a class="xref" href="web-crawler-reference.html#web-crawler-reference-meta-tags-content-extraction" title="Meta tags and data attributes to extract custom fields">meta tags and attributes</a>.</p>
<p>During a crawl, the web crawler uses <a class="xref" href="web-crawler-reference.html#web-crawler-reference-http-response-status-codes" title="HTTP response status codes handling">HTTP response status codes</a> and <a class="xref" href="web-crawler-reference.html#web-crawler-reference-robots-meta-tags" title="Robots meta tags">robots meta tags</a> to determine which documents it is allowed to index.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-content-deduplication"></a>Duplicate document handling<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>By default, the web crawler identifies groups of duplicate <em>web</em> documents and stores each group as a single <em>App Search</em> document within your engine.
Within the App Search document, the fields <code class="literal">url</code> and <code class="literal">additional_urls</code> represent all the URLs where the web crawler discovered the document&#8217;s content (or a sample of URLs if more than <code class="literal">100</code>).
The <code class="literal">url</code> field represents the <em>canonical URL</em>, which you can explicitly manage using <a class="xref" href="web-crawler-reference.html#web-crawler-reference-canonical-url-link-tag" title="Canonical URL link tag">canonical URL link tags</a>.</p>
<p>The crawler identifies duplicate content intelligently, ignoring insignificant differences such as navigation, whitespace, style, and scripts.
More specifically, the crawler combines the values of specific fields, and it <em>hashes</em> the result to create a unique "fingerprint" to represent the content of the web document.</p>
<p>The web crawler then checks your engine for an existing document with the same content hash.
If it doesn&#8217;t find one, it saves a new document to the engine.
If it <em>does</em> exist, the crawler updates the existing document instead of saving a new one.
The crawler adds to the document the additional URL at which the content was discovered.</p>
<p>You can manage which fields the web crawler uses to create the content hash.
You can also disable this feature and <em>allow</em> duplicate documents.</p>
<p>Manage these settings for each domain within the web crawler UI.
See <a class="xref" href="crawl-web-content.html#crawl-web-content-manage-duplicate-documents" title="Manage duplicate document handling">Manage duplicate document handling</a>.</p>
<p>Set the <em>default</em> fields for <em>all</em> domains using the following <a class="xref" href="web-crawler-reference.html#web-crawler-reference-web-crawler-configuration-settings" title="Web crawler configuration settings">configuration setting</a>: <code class="literal">crawler.extraction.default_deduplication_fields</code>.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-content-inclusion-and-exclusion"></a>Content inclusion and exclusion<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>Inject HTML <code class="literal">data</code> attributes into your web pages to instruct the web crawler to include or exclude particular sections from extracted content. For example, use this feature to exclude navigation and footer content when crawling, or to exclude sections of content only intended for screen readers.</p>
<p>For all pages that contain HTML tags with a <code class="literal">data-elastic-include</code> attribute, the crawler will only index content within those tags.</p>
<p>For all pages that contain HTML tags with a <code class="literal">data-elastic-exclude</code> attribute, the crawler will skip those tags from content extraction. You can nest <code class="literal">data-elastic-include</code> and <code class="literal">data-elastic-exclude</code> tags, too.</p>
<p>The web crawler will still crawl any links that appear inside excluded sections as long as the configured crawl rules allow them.</p>
<p>A simple content exclusion rule example:</p>
<div class="pre_wrapper lang-html">
<pre class="programlisting prettyprint lang-html">&lt;body&gt;
  &lt;p&gt;This is your page content, which will be indexed by the App Search web crawler.
  &lt;div data-elastic-exclude&gt;Content in this div will be excluded from the search index&lt;/div&gt;
&lt;/body&gt;</pre>
</div>
<p>In this more complex example with nested exclusion and inclusion rules, the web crawler will only extract "test1 test3 test5 test7" from the page.</p>
<div class="pre_wrapper lang-html">
<pre class="programlisting prettyprint lang-html">&lt;body&gt;
  test1
  &lt;div data-elastic-exclude&gt;
    test2
    &lt;p data-elastic-include&gt;
      test3
      &lt;span data-elastic-exclude&gt;
        test4
        &lt;span data-elastic-include&gt;test5&lt;/span&gt;
      &lt;/span&gt;
    &lt;/p&gt;
    test6
  &lt;/div&gt;
  test7
&lt;/body&gt;</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-content-deletion"></a>Content deletion<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>The web crawler must also delete documents from an engine to keep its documents in sync with the corresponding web content.</p>
<p>During a crawl, the web crawler uses <a class="xref" href="web-crawler-reference.html#web-crawler-reference-http-response-status-codes" title="HTTP response status codes handling">HTTP response status codes</a> to determine which documents to delete.
However, this process cannot delete stale documents in the engine that are no longer linked to on the web.</p>
<p>Therefore, at the conclusion of each crawl, the web crawler begins an additional "purge" crawl, which fetches URLs that exist in the engine, but have not been seen on the web during recent crawls.
The crawler deletes from the engine all documents that respond with <code class="literal">4xx</code> and <code class="literal">3xx</code> responses during the phase.</p>
<p>You can purge documents on demand by using a <a class="xref" href="web-crawler-reference.html#web-crawler-reference-process-crawl" title="Process crawl">process crawl</a>.
A process crawl removes from your engine all documents that are no longer allowed by your <a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl-rule" title="Crawl rule">crawl rules</a>.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-domain"></a>Domain<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>A <em>domain</em> is a website or other internet realm that is the target of a <a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl" title="Crawl">crawl</a>.
Each domain belongs to a crawl, and each crawl has one or more domains.</p>
<p>See <a class="xref" href="crawl-web-content.html#crawl-web-content-manage-domains" title="Manage domains">Manage domains</a> to manage domains for a crawl.</p>
<p>A crawl always stays <em>within</em> its domains when <a class="xref" href="web-crawler-reference.html#web-crawler-reference-content-discovery" title="Content discovery">discovering content</a>.
It cannot discover and index content outside of its domains.</p>
<p>Each domain has a <em>domain URL</em> that identifies the domain using a protocol and hostname.
The domain URL must not include a path.</p>
<p>Each unique combination of protocol and hostname is a separate domain.
Each of the following is its own domain:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">http://example.com</code>
</li>
<li class="listitem">
<code class="literal">https://example.com</code>
</li>
<li class="listitem">
<code class="literal">http://www.example.com</code>
</li>
<li class="listitem">
<code class="literal">https://www.example.com</code>
</li>
<li class="listitem">
<code class="literal">http://shop.example.com</code>
</li>
<li class="listitem">
<code class="literal">https://shop.example.com</code>
</li>
</ul>
</div>
<p>Each domain has:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
One or more <a class="xref" href="web-crawler-reference.html#web-crawler-reference-entry-point" title="Entry point">entry points</a>
</li>
<li class="listitem">
One or more <a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl-rule" title="Crawl rule">crawl rules</a>
</li>
<li class="listitem">
Zero or one <a class="xref" href="web-crawler-reference.html#web-crawler-reference-robots-txt-file" title="Robots.txt file">robots.txt files</a>
</li>
<li class="listitem">
Zero or more <a class="xref" href="web-crawler-reference.html#web-crawler-reference-sitemap" title="Sitemap">sitemaps</a>
</li>
</ul>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-entry-point"></a>Entry point<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>An <em>entry point</em> is a path within a <a class="xref" href="web-crawler-reference.html#web-crawler-reference-domain" title="Domain">domain</a> that serves as a starting point for a crawl.
Each entry point belongs to a domain, and each domain has one or more entry points.</p>
<p>See <a class="xref" href="crawl-web-content.html#crawl-web-content-manage-entry-points" title="Manage entry points">Manage entry points</a> to manage entry points for a domain.</p>
<p>Use entry points to instruct the crawler to fetch URLs it would otherwise not discover.
For example, if you&#8217;d like to index a page that isn&#8217;t linked from other pages, add the page&#8217;s URL as an entry point.</p>
<p>Ensure the entry points for each domain are allowed by the domain&#8217;s <a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl-rule" title="Crawl rule">crawl rules</a>, and the directives within the domain&#8217;s <a class="xref" href="web-crawler-reference.html#web-crawler-reference-robots-txt-file" title="Robots.txt file">robots.txt file</a>.
The web crawler will not fetch entry points that are disallowed by crawl rules or robots.txt directives.</p>
<p>You can also inform the crawler of URLs using <a class="xref" href="web-crawler-reference.html#web-crawler-reference-sitemap" title="Sitemap">sitemaps</a>.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-sitemap"></a>Sitemap<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>A sitemap is an XML file, associated with a <a class="xref" href="web-crawler-reference.html#web-crawler-reference-domain" title="Domain">domain</a>, that informs web crawlers about pages within that domain.
XML elements within the sitemap identify specific URLs that are available for crawling.</p>
<p>These elements are similar to <a class="xref" href="web-crawler-reference.html#web-crawler-reference-entry-point" title="Entry point">entry points</a>.
You can therefore choose to submit URLs to the Enterprise Search web crawler using sitemaps, entry points, or a combination of both.</p>
<p>You may prefer using sitemaps over entry points for any of the following reasons:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
You have already been publishing sitemaps for other web crawlers.
</li>
<li class="listitem">
You don&#8217;t have access to the web crawler UI.
</li>
<li class="listitem">
You prefer the sitemap file interface over the web crawler web interface.
</li>
</ul>
</div>
<p>Use sitemaps to inform the web crawler of pages you think are important, or pages that are isolated and not linked from other pages.
However, be aware the web crawler will visit only those pages from the sitemap that are allowed by the domain&#8217;s <a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl-rule" title="Crawl rule">crawl rules</a> and <a class="xref" href="web-crawler-reference.html#web-crawler-reference-robots-txt-file" title="Robots.txt file">robots.txt file</a> directives.</p>
<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="web-crawler-reference-sitemap-discovery-management"></a>Sitemap discovery and management<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h4>
</div></div></div>
<p>To add a sitemap to a domain, you can specify it within a <a class="xref" href="web-crawler-reference.html#web-crawler-reference-robots-txt-file" title="Robots.txt file">robots.txt file</a>.
At the start of each <a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl" title="Crawl">crawl</a>, the web crawler fetches and processes each domain&#8217;s robots.txt file and each sitemap specified within those robots.txt files.</p>
<p>See <a class="xref" href="crawl-web-content.html#crawl-web-content-manage-sitemaps" title="Manage sitemaps">Manage sitemaps</a> to manage sitemaps for your domains.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="web-crawler-reference-sitemap-format"></a>Sitemap format and technical specification<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h4>
</div></div></div>
<p>The <a href="https://www.sitemaps.org/index.html" class="ulink" target="_blank" rel="noopener">sitemaps standard</a> defines the format and technical specification for sitemaps.
Refer to the standard for the required and optional elements, character escaping, and other technical considerations and examples.</p>
<p>The Enterprise Search web crawler does not process optional meta data defined by the standard.
The crawler extracts a list of URLs from each sitemap and ignores all other information.</p>
<p>Ensure each URL within your sitemap matches the exact <a class="xref" href="web-crawler-reference.html#web-crawler-reference-domain" title="Domain">domain</a> (here defined as scheme + host + port) for your site.
Different subdomains (like <code class="literal">www.example.com</code> and <code class="literal">blog.example.com</code>) and different schemes (like <code class="literal">http://example.com</code> and <code class="literal">https://example.com</code>) require separate sitemaps.</p>
<p>The Enterprise Search web crawler also supports sitemap index files.
Refer to <a href="https://www.sitemaps.org/protocol.html#index" class="ulink" target="_blank" rel="noopener">Using sitemap index files</a> within the sitemap standard for sitemap index file details and examples.</p>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-crawl-rule"></a>Crawl rule<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>A <em>crawl rule</em> is a crawler instruction to <em>allow</em> or <em>disallow</em> specific paths within a <a class="xref" href="web-crawler-reference.html#web-crawler-reference-domain" title="Domain">domain</a>.
Each crawl rule belongs to a domain, and each domain has one or more crawl rules.</p>
<p>See <a class="xref" href="crawl-web-content.html#crawl-web-content-manage-crawl-rules" title="Manage crawl rules">Manage crawl rules</a> to manage crawl rules for a domain.
After modifying your crawl rules, you can re-apply the rules to your existing documents without waiting for a full re-crawl.
See <a class="xref" href="web-crawler-reference.html#web-crawler-reference-process-crawl" title="Process crawl">Process crawl</a>.</p>
<p>During <a class="xref" href="web-crawler-reference.html#web-crawler-reference-content-discovery" title="Content discovery">content discovery</a>, the web crawler discovers new URLs and must determine which it is allowed to follow.
Each URL has a <em>domain</em> (e.g. <code class="literal">https://example.com</code>) and a <em>path</em> (e.g. <code class="literal">/category/clothing</code> or <code class="literal">/c/Credit_Center</code>).</p>
<p>The web crawler looks up the crawl rules for the domain, and applies the path to the crawl rules to determine if the path is <em>allowed</em> or <em>disallowed</em>.
The crawler evaluates the crawl rules in order.
The <em>first</em> matching crawl rule determines the policy for the newly discovered URL.</p>
<p>Each crawl rule has a <em>path pattern</em>, a <em>rule</em>, and a <em>policy</em>.
To evaluate each rule, the web crawler compares a newly discovered <em>path</em> to the path pattern, using the logic represented by the rule, resulting in a policy.</p>
<p>The policy for each URL is also affected by directives in <a class="xref" href="web-crawler-reference.html#web-crawler-reference-robots-txt-file" title="Robots.txt file">robots.txt files</a>.
The web crawler will crawl only those URLs that are allowed by crawl rules <em>and</em> robots.txt directives.</p>
<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="web-crawler-reference-crawl-rule-rules"></a>Crawl rule logic (rules)<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h4>
</div></div></div>
<p>The logic for each rule is as follows:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
Begins with
</span>
</dt>
<dd>
<p>
The <em>path pattern</em> is a literal string <em>except</em> for the character <code class="literal">*</code>, which is a meta character that will match anything.
</p>
<p>The rule matches when the <em>path pattern</em> matches the <span class="strong strong"><strong>beginning</strong></span> of the <em>path</em> (which always begins with <code class="literal">/</code>).</p>
<p>If using this rule, begin your <em>path pattern</em> with <code class="literal">/</code>.</p>
</dd>
<dt>
<span class="term">
Ends with
</span>
</dt>
<dd>
<p>
The <em>path pattern</em> is a literal string <em>except</em> for the character <code class="literal">*</code>, which is a meta character that will match anything.
</p>
<p>The rule matches when the <em>path pattern</em> matches the <span class="strong strong"><strong>end</strong></span> of the <em>path</em>.</p>
</dd>
<dt>
<span class="term">
Contains
</span>
</dt>
<dd>
<p>
The <em>path pattern</em> is a literal string <em>except</em> for the character <code class="literal">*</code>, which is a meta character that will match anything.
</p>
<p>The rule matches when the <em>path pattern</em> matches anywhere <span class="strong strong"><strong>within</strong></span> the <em>path</em>.</p>
</dd>
<dt>
<span class="term">
Regex
</span>
</dt>
<dd>
<p>
The <em>path pattern</em> is a regular expression compatible with the Ruby language regular expression engine.
In addition to literal characters, the path pattern may include
<a href="https://ruby-doc.org/core-2.5.1/Regexp.html#class-Regexp-label-Metacharacters+and+Escapes" class="ulink" target="_blank" rel="noopener">metacharacters</a>, <a href="https://ruby-doc.org/core-2.5.1/Regexp.html#class-Regexp-label-Character+Classes" class="ulink" target="_blank" rel="noopener">character classes</a>, and <a href="https://ruby-doc.org/core-2.5.1/Regexp.html#class-Regexp-label-Repetition" class="ulink" target="_blank" rel="noopener">repetitions</a>.
You can test Ruby regular expressions using <a href="https://rubular.com" class="ulink" target="_blank" rel="noopener">Rubular</a>.
</p>
<p>The rule matches when the <em>path pattern</em> matches the <span class="strong strong"><strong>beginning</strong></span> of the <em>path</em> (which always begins with <code class="literal">/</code>).</p>
<p>If using this rule, begin your <em>path pattern</em> with <code class="literal">\/</code> or a metacharacter or character class that matches <code class="literal">/</code>.</p>
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="web-crawler-reference-crawl-rule-matching"></a>Crawl rule matching<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h4>
</div></div></div>
<p>The following table provides examples of crawl rule matching:</p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
<col class="col_4"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">URL path</th>
<th align="left" valign="top">Rule</th>
<th align="left" valign="top">Path pattern</th>
<th align="left" valign="top">Match?</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p><code class="literal">/foo/bar</code></p></td>
<td align="left" valign="top"><p><em>Begins with</em></p></td>
<td align="left" valign="top"><p><code class="literal">/foo</code></p></td>
<td align="left" valign="top"><p>YES</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/foo/bar</code></p></td>
<td align="left" valign="top"><p><em>Begins with</em></p></td>
<td align="left" valign="top"><p><code class="literal">/*oo</code></p></td>
<td align="left" valign="top"><p>YES</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/bar/foo</code></p></td>
<td align="left" valign="top"><p><em>Begins with</em></p></td>
<td align="left" valign="top"><p><code class="literal">/foo</code></p></td>
<td align="left" valign="top"><p>NO</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/foo/bar</code></p></td>
<td align="left" valign="top"><p><em>Begins with</em></p></td>
<td align="left" valign="top"><p><code class="literal">foo</code></p></td>
<td align="left" valign="top"><p>NO</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/blog/posts/hello-world</code></p></td>
<td align="left" valign="top"><p><em>Ends</em></p></td>
<td align="left" valign="top"><p><code class="literal">world</code></p></td>
<td align="left" valign="top"><p>YES</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/blog/posts/hello-world</code></p></td>
<td align="left" valign="top"><p><em>Ends</em></p></td>
<td align="left" valign="top"><p><code class="literal">hello-*</code></p></td>
<td align="left" valign="top"><p>YES</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/blog/world-hello</code></p></td>
<td align="left" valign="top"><p><em>Ends</em></p></td>
<td align="left" valign="top"><p><code class="literal">world</code></p></td>
<td align="left" valign="top"><p>NO</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/blog/world-hello</code></p></td>
<td align="left" valign="top"><p><em>Ends</em></p></td>
<td align="left" valign="top"><p><code class="literal">*world</code></p></td>
<td align="left" valign="top"><p>NO</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/fruits/bananas</code></p></td>
<td align="left" valign="top"><p><em>Contains</em></p></td>
<td align="left" valign="top"><p><code class="literal">banana</code></p></td>
<td align="left" valign="top"><p>YES</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/fruits/apples</code></p></td>
<td align="left" valign="top"><p><em>Contains</em></p></td>
<td align="left" valign="top"><p><code class="literal">banana</code></p></td>
<td align="left" valign="top"><p>NO</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/2020</code></p></td>
<td align="left" valign="top"><p><em>Regex</em></p></td>
<td align="left" valign="top"><p><code class="literal">\/[0-9]{3,5}</code></p></td>
<td align="left" valign="top"><p>YES</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/20</code></p></td>
<td align="left" valign="top"><p><em>Regex</em></p></td>
<td align="left" valign="top"><p><code class="literal">\/[0-9]{3,5}</code></p></td>
<td align="left" valign="top"><p>NO</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/2020</code></p></td>
<td align="left" valign="top"><p><em>Regex</em></p></td>
<td align="left" valign="top"><p><code class="literal">[0-9]{3,5}</code></p></td>
<td align="left" valign="top"><p>NO</p></td>
</tr>
</tbody>
</table>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="web-crawler-reference-crawl-rule-order"></a>Crawl rule order<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h4>
</div></div></div>
<p>The <em>first</em> crawl rule to match determines the policy for the URL.
Therefore, the order of the crawl rules is significant.</p>
<p>The following table demonstrates how crawl rule order affects the resulting policy for a path:</p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Path</th>
<th align="left" valign="top">Crawl rules</th>
<th align="left" valign="top">Resulting policy</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p><code class="literal">/blog/2021/foo-bar-baz</code></p></td>
<td align="left" valign="top"><p>1. <code class="literal">Disallow</code> if <code class="literal">Begins with</code> <code class="literal">/blog</code></p>
<p>2. <code class="literal">Allow</code> if <code class="literal">Regex</code> <code class="literal">.*</code></p></td>
<td align="left" valign="top"><p>DISALLOW</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/blog/2021/foo-bar-baz</code></p></td>
<td align="left" valign="top"><p>1. <code class="literal">Allow</code> if <code class="literal">Regex</code> <code class="literal">.*</code></p>
<p>2. <code class="literal">Disallow</code> if <code class="literal">Begins with</code> <code class="literal">/blog</code></p></td>
<td align="left" valign="top"><p>ALLOW</p></td>
</tr>
</tbody>
</table>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="web-crawler-reference-crawl-rule-restricting-paths"></a>Restricting paths using crawl rules<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h4>
</div></div></div>
<p>The domain dashboard adds a default crawl rule to each domain: <code class="literal">Allow</code> if <code class="literal">Regex</code> <code class="literal">.*</code>.
You cannot delete or re-order this rule through the dashboard.</p>
<p>This rule is permissive, allowing all paths within the domain.
To restrict paths, use either of the following techniques:</p>
<p>Add rules that disallow specific paths (e.g. disallow the blog):</p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Policy</th>
<th align="left" valign="top">Rule</th>
<th align="left" valign="top">Path pattern</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p><code class="literal">Disallow</code></p></td>
<td align="left" valign="top"><p><code class="literal">Begins with</code></p></td>
<td align="left" valign="top"><p><code class="literal">/blog</code></p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">Allow</code></p></td>
<td align="left" valign="top"><p><code class="literal">Regex</code></p></td>
<td align="left" valign="top"><p><code class="literal">.*</code></p></td>
</tr>
</tbody>
</table>
</div>
<p>Or, add rules that allow specific paths and disallow all others (e.g. allow <em>only</em> the blog):</p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Policy</th>
<th align="left" valign="top">Rule</th>
<th align="left" valign="top">Path pattern</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p><code class="literal">Allow</code></p></td>
<td align="left" valign="top"><p><code class="literal">Begins with</code></p></td>
<td align="left" valign="top"><p><code class="literal">/blog</code></p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">Disallow</code></p></td>
<td align="left" valign="top"><p><code class="literal">Regex</code></p></td>
<td align="left" valign="top"><p><code class="literal">.*</code></p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">Allow</code></p></td>
<td align="left" valign="top"><p><code class="literal">Regex</code></p></td>
<td align="left" valign="top"><p><code class="literal">.*</code></p></td>
</tr>
</tbody>
</table>
</div>
<p>When you restrict a crawl to specific paths, be sure to add <a class="xref" href="web-crawler-reference.html#web-crawler-reference-entry-point" title="Entry point">entry points</a> that allow the crawler to discover those paths.
For example, if your crawl rules restrict the crawler to <code class="literal">/blog</code>, add <code class="literal">/blog</code> as an entry point.</p>
<p>If you leave only the default entry point of <code class="literal">/</code>, the crawl will end immediately, since <code class="literal">/</code> is disallowed.</p>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-robots-txt-file"></a>Robots.txt file<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>A robots.txt file is a plain text file, associated with a <a class="xref" href="web-crawler-reference.html#web-crawler-reference-domain" title="Domain">domain</a>, that provides instructions to web crawlers.
The instructions within the file, also called directives, communicate which paths within that domain are disallowed (and allowed) for crawling.</p>
<p>The directives within a robots.txt file are similar to <a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl-rule" title="Crawl rule">crawl rules</a>.
You can therefore choose to manage inclusion and exclusion through robots.txt files, crawl rules, or a combination of both.
The Enterprise Search web crawler will crawl only those paths that are allowed by the crawl rules for the domain <span class="strong strong"><strong>and</strong></span> the directives within the robots.txt file for the domain.</p>
<p>You may prefer using a robots.txt file over crawl rules for any of the following reasons:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
You have already been publishing a robots.txt file for other web crawlers.
</li>
<li class="listitem">
You don&#8217;t have access to the web crawler UI.
</li>
<li class="listitem">
You prefer the robots.txt file interface over the web crawler web interface.
</li>
</ul>
</div>
<p>You can also use a robots.txt file to specify <a class="xref" href="web-crawler-reference.html#web-crawler-reference-sitemap" title="Sitemap">sitemaps</a> for a domain.</p>
<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="web-crawler-reference-robots-txt-file-discovery-management"></a>Robots.txt file discovery and management<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h4>
</div></div></div>
<p>At the start of each <a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl" title="Crawl">crawl</a>, the web crawler fetches and processes the robots.txt file for each domain.
When fetching a robots.txt file, the crawler handles 2xx, 4xx, and 5xx responses differenty.
See <a class="xref" href="web-crawler-reference.html#web-crawler-reference-http-response-status-codes-robots-txt" title="Robots.txt file response code handling">Robots.txt file response code handling</a> for an explanation of each response.</p>
<p>See <a class="xref" href="crawl-web-content.html#crawl-web-content-manage-robots-txt-files" title="Manage robots.txt files">Manage robots.txt files</a> to manage robots.txt files for your domains.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="web-crawler-reference-robots-txt-file-format"></a>Robots.txt file format and technical specification<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h4>
</div></div></div>
<p>See <a href="https://en.wikipedia.org/wiki/Robots_exclusion_standard" class="ulink" target="_blank" rel="noopener">Robots exclusion standard</a> on Wikipedia for a summary of robots.txt conventions.</p>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-canonical-url-link-tag"></a>Canonical URL link tag<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>A <em>canonical URL link tag</em> is an HTML element you can embed within pages that <a class="xref" href="web-crawler-reference.html#web-crawler-reference-content-deduplication" title="Duplicate document handling">duplicate the content of other pages</a>.
The canonical URL link tag specifies the canonical URL for that content.</p>
<p>The canonical URL is stored on the App Search document in the <code class="literal">url</code> <a class="xref" href="web-crawler-reference.html#web-crawler-reference-web-crawler-schema" title="Web crawler schema">field</a>, while the <code class="literal">additional_urls</code> field contains other URLs where the crawler discovered the same content.
If your site contains pages that duplicate the content of other pages, use canonical URL link tags to explicitly manage which URL is stored in the <code class="literal">url</code> field of the resulting App Search document.</p>
<p>Template:</p>
<div class="pre_wrapper lang-html">
<pre class="programlisting prettyprint lang-html">&lt;link rel="canonical" href="{CANONICAL_URL}"&gt;</pre>
</div>
<p>Example:</p>
<div class="pre_wrapper lang-html">
<pre class="programlisting prettyprint lang-html">&lt;link rel="canonical" href="https://example.com/categories/dresses/starlet-red-medium"&gt;</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-robots-meta-tags"></a>Robots meta tags<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p><em>Robots meta tags</em> are HTML elements you can embed within pages to prevent the crawler from following links or indexing content.</p>
<p>Template:</p>
<div class="pre_wrapper lang-html">
<pre class="programlisting prettyprint lang-html">&lt;meta name="robots" content="{DIRECTIVES}"&gt;</pre>
</div>
<p>Supported directives:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">noindex</code>
</span>
</dt>
<dd>
The web crawler will not index the page.
</dd>
<dt>
<span class="term">
<code class="literal">nofollow</code>
</span>
</dt>
<dd>
<p>
The web crawler will not follow links from the page (i.e. will not add links to the crawl queue).
The web crawler logs a <code class="literal">url_discover_denied</code> event for each link.
</p>
<p>The directive does not prevent the web crawler from indexing the page.</p>
</dd>
</dl>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>Currently, <a class="xref" href="web-crawler-reference.html#web-crawler-reference-content-deletion" title="Content deletion">content deletion</a> (purge or process crawl) does not honor the <code class="literal">noindex</code> and <code class="literal">nofollow</code> directives.
Crawls will not remove previously indexed pages that now have <code class="literal">noindex</code> and <code class="literal">nofollow</code> directives from the engine at the end of each crawl.
To manually remove obsolete content, create the appropriate <a class="xref" href="crawl-web-content.html#crawl-web-content-manage-crawl-rules" title="Manage crawl rules">crawl rules</a> to exclude the pages and run a <a class="xref" href="web-crawler-reference.html#web-crawler-reference-process-crawl" title="Process crawl">process crawl</a>.</p>
</div>
</div>
<p>Examples:</p>
<div class="pre_wrapper lang-html">
<pre class="programlisting prettyprint lang-html">&lt;meta name="robots" content="noindex"&gt;
&lt;meta name="robots" content="nofollow"&gt;
&lt;meta name="robots" content="noindex, nofollow"&gt;</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-meta-tags-content-extraction"></a>Meta tags and data attributes to extract custom fields<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>With <em>meta tags and data attributes</em> you can extract custom fields from your HTML pages.</p>
<p>Template:</p>
<div class="pre_wrapper lang-html">
<pre class="programlisting prettyprint lang-html">&lt;head&gt;
  &lt;meta class="elastic" name="{FIELD_NAME}" content="{FIELD_VALUE}"&gt;
&lt;/head&gt;
&lt;body&gt;
  &lt;div data-elastic-name="{FIELD_NAME}"&gt;{FIELD_VALUE}&lt;/div&gt;
&lt;/body&gt;</pre>
</div>
<p>The crawled document for this example</p>
<div class="pre_wrapper lang-html">
<pre class="programlisting prettyprint lang-html">&lt;head&gt;
  &lt;meta class="elastic" name="product_price" content="99.99"&gt;
&lt;/head&gt;
&lt;body&gt;
  &lt;h1 data-elastic-name="product_name"&gt;Printer&lt;/h1&gt;
&lt;/body&gt;</pre>
</div>
<p>will include 2 additional fields</p>
<div class="pre_wrapper lang-json">
<pre class="programlisting prettyprint lang-json">{
  "product_price": "99.99",
  "product_name": "Printer"
}</pre>
</div>
<p>For compatibility with Elastic Site Search, the crawler also supports the class value <code class="literal">swiftype</code> on meta elements and the data attribute <code class="literal">data-swiftype-name</code> on elements within the body.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-nofollow-link"></a>Nofollow link<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p><em>Nofollow links</em> are HTML links that instruct the crawler to not follow the URL.</p>
<p>The web crawler will not follow links that include <code class="literal">rel="nofollow"</code> (i.e. will not add links to the crawl queue).
The web crawler logs a <code class="literal">url_discover_denied</code> event for each link.</p>
<p>The link does not prevent the web crawler from indexing the page in which it appears.</p>
<p>Template:</p>
<div class="pre_wrapper lang-html">
<pre class="programlisting prettyprint lang-html">&lt;a rel="nofollow" href="{LINK_URL}"&gt;{LINK_TEXT}&lt;/a&gt;</pre>
</div>
<p>Example:</p>
<div class="pre_wrapper lang-html">
<pre class="programlisting prettyprint lang-html">&lt;a rel="nofollow" href="/admin/categories"&gt;Edit this category&lt;/a&gt;</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-crawl-status"></a>Crawl status<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>Each crawl has a <em>status</em>, which quickly communicates its state.</p>
<p>See <a class="xref" href="crawl-web-content.html#crawl-web-content-view-crawl-status" title="View crawl status">View crawl status</a> to view the status for a crawl.</p>
<p>All crawl statuses:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
Pending
</span>
</dt>
<dd>
The crawl is enqueued and will start after resources are available.
</dd>
<dt>
<span class="term">
Starting
</span>
</dt>
<dd>
The crawl is starting.
You may see this status briefly, while a crawl moves from <em>pending</em> to <em>running</em>.
</dd>
<dt>
<span class="term">
Running
</span>
</dt>
<dd>
The crawl is running.
</dd>
<dt>
<span class="term">
Success
</span>
</dt>
<dd>
The crawl completed without error.
The web crawler may stop a crawl due to hitting resource limits.
Such a crawl will report its status as <em>success</em>, as long as it completes without error.
</dd>
<dt>
<span class="term">
Canceling
</span>
</dt>
<dd>
The crawl is canceling.
You may see this status briefly, after choosing to cancel a crawl.
</dd>
<dt>
<span class="term">
Canceled
</span>
</dt>
<dd>
The crawl was intentionally canceled and therefore did not complete.
</dd>
<dt>
<span class="term">
Failed
</span>
</dt>
<dd>
The crawl ended unexpectedly.
<a class="xref" href="view-web-crawler-events-logs.html" title="View web crawler events logs">View the web crawler events logs</a> for a message providing an explanation.
</dd>
<dt>
<span class="term">
Skipped
</span>
</dt>
<dd>
An automatic crawl was skipped because another crawl was active at the time.
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-process-crawl"></a>Process crawl<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>A <em>process crawl</em> is an operation that re-processes the documents in your engine using the current <a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl-rule" title="Crawl rule">crawl rules</a>.
Essentially, it <em>re-applies</em> your crawl rules to your existing documents, removing the documents whose URLs are no longer allowed by the current crawl rules.</p>
<p>After updating your crawl rules, you can use a process crawl to remove unwanted documents without waiting for a full re-crawl to remove them.
You can request process crawls through the web crawler UI or API.</p>
<p>Using the UI, you can request a process crawl for <em>all</em> domains or a <em>specific</em> domain.
See <a class="xref" href="crawl-web-content.html#crawl-web-content-re-apply-crawl-rules" title="Re-apply crawl rules">Re-apply crawl rules</a>.</p>
<p>Using the API, you can request a process crawl for <em>all</em> domains or a <em>subset</em> of domains.
See <a class="xref" href="web-crawler-api-reference.html#web-crawler-apis-process-crawls" title="Process crawls">Process crawls</a>.</p>
<p>It is recommended to cancel any active web crawls before requesting a process crawl.
A web crawl that runs concurrently with a process crawl may continue to index fresh documents with out of date configuration; the process crawl will only apply to documents indexed at the time of the request.</p>
<p>Using the API, you can also complete a "dry run" process crawl.
When "dry run" is enabled, the process crawl will report which documents would be deleted by a normal process crawl, but it does not delete any documents from your engine.
Use a dry run to get quick feedback after <a class="xref" href="web-crawler-api-reference.html#web-crawler-apis-crawl-rules" title="Crawl rules">modifying crawl rules through the web crawler API</a>.
If the results of the dry run include documents you do not want to remove, further modify your crawl rules.
Repeat this process until you get the expected results.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-http-response-status-codes"></a>HTTP response status codes handling<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>The following sections describe how the web crawler handles HTTP response status codes for HTML documents and robots.txt files.</p>
<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="web-crawler-reference-http-response-status-codes-html"></a>HTML response code handling<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h4>
</div></div></div>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Code</th>
<th align="left" valign="top">Description</th>
<th align="left" valign="top">Web crawler behavior</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p><code class="literal">2xx</code></p></td>
<td align="left" valign="top"><p>Success</p></td>
<td align="left" valign="top"><p>The web crawler extracts and de-duplicates the page&#8217;s content into a search document.
Then it indexes the document into the engine, replacing an existing document with the same content if present.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">3xx</code></p></td>
<td align="left" valign="top"><p>Redirection</p></td>
<td align="left" valign="top"><p>The web crawler follows all redirects within configured domains recursively until it receives a <code class="literal">2xx</code>, <code class="literal">4xx</code>, or <code class="literal">5xx</code> response status code, detects a circular redirect or reaches a configured max redirects limit (which are handled as failures).
The crawler then handles that code as indicated in this table.
Each redirect response is logged as a separate event in the event log along with redirect information to help with long or circular redirect chain troubleshooting.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">4xx</code></p></td>
<td align="left" valign="top"><p>Permanent error</p></td>
<td align="left" valign="top"><p>The web crawler assumes this error is permanent.
It therefore does not index the document.
Furthermore, if the document is present in the engine, the web crawler deletes the document.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">5xx</code></p></td>
<td align="left" valign="top"><p>Temporary error</p></td>
<td align="left" valign="top"><p>The web crawler optimistically assumes this error will resolve in the future.
If the engine already contains a document representing the page, the document is updated to indicate the page was inaccessible.
After the page is inaccessible for three consecutive crawls, the document is deleted from the engine.</p></td>
</tr>
</tbody>
</table>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="web-crawler-reference-http-response-status-codes-robots-txt"></a>Robots.txt file response code handling<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h4>
</div></div></div>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Code</th>
<th align="left" valign="top">Description</th>
<th align="left" valign="top">Web crawler behavior</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p><code class="literal">2xx</code></p></td>
<td align="left" valign="top"><p>Success</p></td>
<td align="left" valign="top"><p>The web crawler processes and honors the directives for the domain within the <a class="xref" href="web-crawler-reference.html#web-crawler-reference-robots-txt-file" title="Robots.txt file">robots.txt file</a>.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">3xx</code></p></td>
<td align="left" valign="top"><p>Redirect</p></td>
<td align="left" valign="top"><p>The web crawler follows redirects (including redirects to external domains) until it receives a <code class="literal">2xx</code>, <code class="literal">4xx</code>, or <code class="literal">5xx</code> response status code, detects a circular redirect or reaches a configured max redirects limit (which are handled as failures).
The crawler then handles that code as indicated in this table.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">4xx</code></p></td>
<td align="left" valign="top"><p>Permanent error</p></td>
<td align="left" valign="top"><p>The web crawler assumes this error is permanent and no specific rules exist for the domain.
It therefore <span class="strong strong"><strong>allows all</strong></span> paths for the domain (subject to <a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl-rule" title="Crawl rule">crawl rules</a>).</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">5xx</code></p></td>
<td align="left" valign="top"><p>Temporary error</p></td>
<td align="left" valign="top"><p>The web crawler optimistically assumes this error will resolve in the future.
However, in the interim, the crawler does not know which paths are allowed and disallowed.
It therefore <span class="strong strong"><strong>disallows all</strong></span> paths for the domain.</p></td>
</tr>
</tbody>
</table>
</div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-web-crawler-schema"></a>Web crawler schema<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>The web crawler indexes search documents using the following schema.
All fields are strings or arrays of strings.</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">additional_urls</code>
</span>
</dt>
<dd>
The URLs of additional pages with the same content.
</dd>
<dt>
<span class="term">
<code class="literal">body_content</code>
</span>
</dt>
<dd>
The content of the page&#8217;s <code class="literal">&lt;body&gt;</code> tag with all HTML tags removed.
Truncated to <code class="literal">crawler.extraction.body_size.limit</code>.
</dd>
<dt>
<span class="term">
<code class="literal">content_hash</code>
</span>
</dt>
<dd>
A "fingerprint" to uniquely identify this content, which is used to handle duplicate documents.
See <a class="xref" href="web-crawler-reference.html#web-crawler-reference-content-deduplication" title="Duplicate document handling">Duplicate document handling</a>.
</dd>
<dt>
<span class="term">
<code class="literal">domains</code>
</span>
</dt>
<dd>
The domains in which this content appears.
</dd>
<dt>
<span class="term">
<code class="literal">headings</code>
</span>
</dt>
<dd>
The text of the page&#8217;s HTML headings (<code class="literal">h1</code> - <code class="literal">h6</code> elements).
Limited by <code class="literal">crawler.extraction.headings_count.limit</code>.
</dd>
<dt>
<span class="term">
<code class="literal">id</code>
</span>
</dt>
<dd>
The unique identifier for the page.
</dd>
<dt>
<span class="term">
<code class="literal">links</code>
</span>
</dt>
<dd>
Links found on the page.
Limited by <code class="literal">crawler.extraction.indexed_links_count.limit</code>.
</dd>
<dt>
<span class="term">
<code class="literal">meta_description</code>
</span>
</dt>
<dd>
The page&#8217;s description, taken from the <code class="literal">&lt;meta name="description"&gt;</code> tag.
Truncated to <code class="literal">crawler.extraction.description_size.limit</code>.
</dd>
<dt>
<span class="term">
<code class="literal">meta_keywords</code>
</span>
</dt>
<dd>
The page&#8217;s keywords, taken from the <code class="literal">&lt;meta name="keywords"&gt;</code> tag.
Truncated to <code class="literal">crawler.extraction.keywords_size.limit</code>.
</dd>
<dt>
<span class="term">
<code class="literal">title</code>
</span>
</dt>
<dd>
The title of the page, taken from the <code class="literal">&lt;title&gt;</code> tag.
Truncated to <code class="literal">crawler.extraction.title_size.limit</code>.
</dd>
<dt>
<span class="term">
<code class="literal">url</code>
</span>
</dt>
<dd>
The URL of the page.
</dd>
<dt>
<span class="term">
<code class="literal">url_host</code>
</span>
</dt>
<dd>
The hostname or IP from the page&#8217;s URL.
</dd>
<dt>
<span class="term">
<code class="literal">url_path</code>
</span>
</dt>
<dd>
The full pathname from the page&#8217;s URL.
</dd>
<dt>
<span class="term">
<code class="literal">url_path_dir1</code>
</span>
</dt>
<dd>
The first segment of the pathname from the page&#8217;s URL.
</dd>
<dt>
<span class="term">
<code class="literal">url_path_dir2</code>
</span>
</dt>
<dd>
The second segment of the pathname from the page&#8217;s URL.
</dd>
<dt>
<span class="term">
<code class="literal">url_path_dir3</code>
</span>
</dt>
<dd>
The third segment of the pathname from the page&#8217;s URL.
</dd>
<dt>
<span class="term">
<code class="literal">url_port</code>
</span>
</dt>
<dd>
The port number from the page&#8217;s URL (as a string).
</dd>
<dt>
<span class="term">
<code class="literal">url_scheme</code>
</span>
</dt>
<dd>
The scheme of the page&#8217;s URL.
</dd>
</dl>
</div>
<p>In addition to these predefined fields, you can also extract custom fields via <a class="xref" href="web-crawler-reference.html#web-crawler-reference-meta-tags-content-extraction" title="Meta tags and data attributes to extract custom fields">meta tags and attributes</a>.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-web-crawler-events-logs"></a>Web crawler events logs<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>The web crawler logs many events while discovering, extracting, and indexing web content.</p>
<p>To view these events, see <a class="xref" href="view-web-crawler-events-logs.html" title="View web crawler events logs">View web crawler events logs</a>.</p>
<p>For a complete reference of all events, see <a class="xref" href="web-crawler-events-logs-reference.html" title="Web crawler events logs reference">Web crawler events logs reference</a>.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-web-crawler-configuration-settings"></a>Web crawler configuration settings<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.16/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>Operators can configure several web crawler settings.</p>
<p>See <a href="/guide/en/enterprise-search/7.16/configuration.html#configuration-settings-elastic-crawler" class="ulink" target="_top">Elastic crawler</a> within the Enterprise Search configuration documentation.</p>
</div>

</div>
<div class="navfooter">
<span class="prev">
<a href="web-crawler-faq.html">« Web crawler FAQ</a>
</span>
<span class="next">
<a href="web-crawler-events-logs-reference.html">Web crawler events logs reference »</a>
</span>
</div>
</div>
</body>
</html>
