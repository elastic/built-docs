<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Web crawler events logs reference | Elastic App Search Documentation [8.1] | Elastic</title>
<meta class="elastic" name="content" content="Web crawler events logs reference | Elastic App Search Documentation [8.1]">

<link rel="home" href="index.html" title="Elastic App Search Documentation [8.1]"/>
<link rel="up" href="guides.html" title="Guides"/>
<link rel="prev" href="web-crawler-reference.html" title="Web crawler reference"/>
<link rel="next" href="api-reference.html" title="API Reference"/>
<meta class="elastic" name="product_version" content="8.1"/>
<meta class="elastic" name="product_name" content="App Search"/>
<meta class="elastic" name="website_area" content="documentation"/>
<meta name="DC.type" content="Learn/Docs/App Search/Guide/8.1"/>
<meta name="DC.subject" content="App Search"/>
<meta name="DC.identifier" content="8.1"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body><div class="page_header">
<strong>IMPORTANT</strong>: No additional bug fixes or documentation updates
will be released for this version. For the latest information, see the
<a href="../current/index.html">current release documentation</a>.
</div>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link">
<div id="related-products" class="dropdown">
<div class="related-products-title"></div>
<div class="dropdown-anchor" tabindex="0">Elastic App Search Documentation<span class="dropdown-icon"></span></div>
<div class="dropdown-content">
<ul>
<li class="dropdown-category">Enterprise Search guides</li>
<ul>
<li><a href="/guide/en/enterprise-search/current/index.html" target="_blank">Enterprise Search</a></li>
<li><a href="/guide/en/app-search/current/index.html" target="_blank">App Search</a></li>
<li><a href="/guide/en/workplace-search/current/index.html" target="_blank">Workplace Search</a></li>
</ul>
<li class="dropdown-category">Programming language clients</li>
<ul>
<li><a href="https://www.elastic.co/guide/en/enterprise-search-clients/enterprise-search-node/current/index.html" target="_blank">Node.js client</a></li>
<li><a href="https://www.elastic.co/guide/en/enterprise-search-clients/php/current/index.html" target="_blank">PHP client</a></li>
<li><a href="https://www.elastic.co/guide/en/enterprise-search-clients/python/current/index.html" target="_blank">Python client</a></li>
<li><a href="https://www.elastic.co/guide/en/enterprise-search-clients/ruby/current/index.html" target="_blank">Ruby client</a></li>
</ul>
</ul>
</div>
</div>
</span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="guides.html">Guides</a></span>
</div>
<div class="navheader">
<span class="prev">
<a href="web-crawler-reference.html">« Web crawler reference</a>
</span>
<span class="next">
<a href="api-reference.html">API Reference »</a>
</span>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="web-crawler-events-logs-reference"></a>Web crawler events logs reference<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.1/app-search-docs/guides/web-crawler-events-logs-reference.asciidoc">edit</a></h2>
</div></div></div>
<p>The Enterprise Search <a class="xref" href="web-crawler.html" title="Web crawler">web crawler</a> logs many events while discovering, extracting, and indexing web content.</p>
<p>Enterprise Search records these events using <a href="/guide/en/ecs/8.1" class="ulink" target="_top">Elastic Common Schema (ECS)</a>, including a custom field set called <code class="literal">crawler.*</code> for crawler-specific data (like <code class="literal">crawl_id</code>).</p>
<p>To view these events, see <a class="xref" href="view-web-crawler-events-logs.html" title="View web crawler events logs">View web crawler events logs</a>.</p>
<p>This document provides a reference to these events and their fields.</p>
<p>First, the reference describes the <a class="xref" href="web-crawler-events-logs-reference.html#web-crawler-events-logs-reference-common" title="Fields common to all web crawler events">fields common to all web crawler events</a>, including:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="web-crawler-events-logs-reference.html#web-crawler-events-logs-reference-common-crawler-specific" title="Crawler-specific fields">Crawler-specific fields</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-events-logs-reference.html#web-crawler-events-logs-reference-common-base" title="Base fields">Base fields</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-events-logs-reference.html#web-crawler-events-logs-reference-common-service" title="Service fields">Service fields</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-events-logs-reference.html#web-crawler-events-logs-reference-common-process" title="Process fields">Process fields</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-events-logs-reference.html#web-crawler-events-logs-reference-common-host" title="Host fields">Host fields</a>
</li>
</ul>
</div>
<p>Then, the remainder of the document describes different types of web crawler events:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<p><a class="xref" href="web-crawler-events-logs-reference.html#web-crawler-events-logs-reference-crawl" title="Crawl lifecycle events">Crawl lifecycle events</a></p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="web-crawler-events-logs-reference.html#web-crawler-events-logs-reference-crawl-start" title="Crawl start events">Crawl start events</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-events-logs-reference.html#web-crawler-events-logs-reference-crawl-seed" title="Crawl seed events">Crawl seed events</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-events-logs-reference.html#web-crawler-events-logs-reference-crawl-end" title="Crawl end events">Crawl end events</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-events-logs-reference.html#web-crawler-events-logs-reference-crawl-status" title="Crawl status events">Crawl status events</a>
</li>
</ul>
</div>
</li>
<li class="listitem">
<p><a class="xref" href="web-crawler-events-logs-reference.html#web-crawler-events-logs-reference-url" title="URL lifecycle events">URL lifecycle events</a></p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="web-crawler-events-logs-reference.html#web-crawler-events-logs-reference-url-common" title="Fields common to all URL lifecycle events">Fields common to all URL lifecycle events</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-events-logs-reference.html#web-crawler-events-logs-reference-url-seed" title="URL seed events">URL seed events</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-events-logs-reference.html#web-crawler-events-logs-reference-url-fetch" title="URL fetch events">URL fetch events</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-events-logs-reference.html#web-crawler-events-logs-reference-url-discover" title="URL discover events">URL discover events</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-events-logs-reference.html#web-crawler-events-logs-reference-url-extracted" title="URL extracted events">URL extracted events</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-events-logs-reference.html#web-crawler-events-logs-reference-url-output" title="URL output events">URL output events</a>
</li>
</ul>
</div>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-events-logs-reference.html#web-crawler-events-logs-reference-ingest" title="Content ingestion events">Content ingestion events</a>
</li>
</ul>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-events-logs-reference-common"></a>Fields common to all web crawler events<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.1/app-search-docs/guides/web-crawler-events-logs-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>All web crawler events include the following common fields.</p>
<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="web-crawler-events-logs-reference-common-crawler-specific"></a>Crawler-specific fields<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.1/app-search-docs/guides/web-crawler-events-logs-reference.asciidoc">edit</a></h4>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">crawler.crawl.id</code>
</span>
</dt>
<dd>
A unique ID of a specific crawl.
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="web-crawler-events-logs-reference-common-base"></a>Base fields<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.1/app-search-docs/guides/web-crawler-events-logs-reference.asciidoc">edit</a></h4>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">@timestamp</code>
</span>
</dt>
<dd>
A UTC timestamp of the event.
</dd>
<dt>
<span class="term">
<code class="literal">event.id</code>
</span>
</dt>
<dd>
A unique identifier of the event.
</dd>
<dt>
<span class="term">
<code class="literal">event.action</code>
</span>
</dt>
<dd>
The type of event.
See the sections that follow.
</dd>
<dt>
<span class="term">
<code class="literal">message</code>
</span>
</dt>
<dd>
A textual description of the event (useful for displaying in a UI for human consumption).
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="web-crawler-events-logs-reference-common-service"></a>Service fields<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.1/app-search-docs/guides/web-crawler-events-logs-reference.asciidoc">edit</a></h4>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">service.ephemeral_id</code>
</span>
</dt>
<dd>
A unique identifier of the crawler process generating the ID (changes every time a process is restarted).
</dd>
<dt>
<span class="term">
<code class="literal">service.type</code>
</span>
</dt>
<dd>
All events will have this set to <code class="literal">crawler</code>.
</dd>
<dt>
<span class="term">
<code class="literal">service.version</code>
</span>
</dt>
<dd>
Current version of the Enterprise Search product.
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="web-crawler-events-logs-reference-common-process"></a>Process fields<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.1/app-search-docs/guides/web-crawler-events-logs-reference.asciidoc">edit</a></h4>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">process.pid</code>
</span>
</dt>
<dd>
The PID of the crawler instance.
</dd>
<dt>
<span class="term">
<code class="literal">process.thread.id</code>
</span>
</dt>
<dd>
The id of the thread logging the event.
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="web-crawler-events-logs-reference-common-host"></a>Host fields<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.1/app-search-docs/guides/web-crawler-events-logs-reference.asciidoc">edit</a></h4>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">host.name</code>
</span>
</dt>
<dd>
The host name where the crawler instance is deployed.
</dd>
</dl>
</div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-events-logs-reference-crawl"></a>Crawl lifecycle events<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.1/app-search-docs/guides/web-crawler-events-logs-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>Each crawl lifecycle event records important checkpoints within the lifecycle of a specific crawl, for example: start, seed, end.
Most of the event information is captured in the <code class="literal">message</code> field, along with the other common fields described above.
The fields below provide additional details.</p>
<p>Each crawl lifecycle event has one of the following values for <code class="literal">event.action</code>:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">crawl-start</code>
</span>
</dt>
<dd>
Emitted when a crawl is started.
Includes crawl configuration.
</dd>
<dt>
<span class="term">
<code class="literal">crawl-seed</code>
</span>
</dt>
<dd>
Emitted every time a crawl is seeded with a set of URLs from the outside.
Includes the list of URLs submitted to the crawler.
</dd>
<dt>
<span class="term">
<code class="literal">crawl-end</code>
</span>
</dt>
<dd>
Emitted when a crawl is ended for any reason (finished, canceled, etc).
</dd>
<dt>
<span class="term">
<code class="literal">crawl-status</code>
</span>
</dt>
<dd>
Periodic events with a snapshot of crawler status metrics used for monitoring an active crawl over time.
</dd>
</dl>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="web-crawler-events-logs-reference-crawl-start"></a>Crawl start events<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.1/app-search-docs/guides/web-crawler-events-logs-reference.asciidoc">edit</a></h4>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">event.kind</code>
</span>
</dt>
<dd>
Set to <code class="literal">event</code>.
</dd>
<dt>
<span class="term">
<code class="literal">event.type</code>
</span>
</dt>
<dd>
Set to <code class="literal">start</code>.
</dd>
<dt>
<span class="term">
<code class="literal">event.action</code>
</span>
</dt>
<dd>
Set to <code class="literal">crawl-start</code>.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.crawl.config</code>
</span>
</dt>
<dd>
A serialized version of the crawl config.
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="web-crawler-events-logs-reference-crawl-seed"></a>Crawl seed events<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.1/app-search-docs/guides/web-crawler-events-logs-reference.asciidoc">edit</a></h4>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">event.kind</code>
</span>
</dt>
<dd>
Set to <code class="literal">event</code>.
</dd>
<dt>
<span class="term">
<code class="literal">event.type</code>
</span>
</dt>
<dd>
Set to <code class="literal">change</code>.
</dd>
<dt>
<span class="term">
<code class="literal">event.action</code>
</span>
</dt>
<dd>
Set to <code class="literal">crawl-seed</code>.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.crawl.seed_urls</code>
</span>
</dt>
<dd>
A list of URLs used to seed a crawl.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.url.type</code>
</span>
</dt>
<dd>
<p>
A type of the URLs being added:
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">content</code> for generic content URLs.
</li>
<li class="listitem">
<code class="literal">sitemap</code> for sitemap and sitemap-index URLs.
</li>
<li class="listitem">
<code class="literal">feed</code> for RSS/ATOM feeds.
</li>
</ul>
</div>
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="web-crawler-events-logs-reference-crawl-end"></a>Crawl end events<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.1/app-search-docs/guides/web-crawler-events-logs-reference.asciidoc">edit</a></h4>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">event.kind</code>
</span>
</dt>
<dd>
Set to <code class="literal">event</code>.
</dd>
<dt>
<span class="term">
<code class="literal">event.type</code>
</span>
</dt>
<dd>
Set to <code class="literal">end</code>.
</dd>
<dt>
<span class="term">
<code class="literal">event.action</code>
</span>
</dt>
<dd>
Set to <code class="literal">crawl-end</code>.
</dd>
<dt>
<span class="term">
<code class="literal">event.outcome</code>
</span>
</dt>
<dd>
Set to <code class="literal">success</code> or <code class="literal">failure</code> depending on how a crawl ended (canceled crawls will be considered failed, etc).
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="web-crawler-events-logs-reference-crawl-status"></a>Crawl status events<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.1/app-search-docs/guides/web-crawler-events-logs-reference.asciidoc">edit</a></h4>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">event.kind</code>
</span>
</dt>
<dd>
Set to <code class="literal">metric</code>.
</dd>
<dt>
<span class="term">
<code class="literal">event.type</code>
</span>
</dt>
<dd>
Set to <code class="literal">info</code>.
</dd>
<dt>
<span class="term">
<code class="literal">event.action</code>
</span>
</dt>
<dd>
Set to <code class="literal">crawl-status</code>.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.status.*</code>
</span>
</dt>
<dd>
A set of metrics describing the global state of a crawl and crawl-specific stats that may be useful to understand the state of a crawl over time.
</dd>
</dl>
</div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-events-logs-reference-url"></a>URL lifecycle events<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.1/app-search-docs/guides/web-crawler-events-logs-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>Each URL lifecycle event is scoped to a particular URL within a specific crawl.
Each event describes what happened to the URL during the crawl, for example: how and when did the crawler discover it?, why did the crawler skip it?
These events have enough details to allow a human operator to understand exactly how the system discovered a specific URL, what decisions have been made about it, and what was the result of processing the URL.</p>
<p>Each URL lifecycle event has one of the following values for <code class="literal">event.action</code>:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">url-seed</code>
</span>
</dt>
<dd>
URL submitted to the crawl backlog for processing (from a seed list, from within the crawl, via an API, etc).
</dd>
<dt>
<span class="term">
<code class="literal">url-fetch</code>
</span>
</dt>
<dd>
URL fetch attempt including timing information, server response headers, HTTP code, etc.
</dd>
<dt>
<span class="term">
<code class="literal">url-discover</code>
</span>
</dt>
<dd>
URL discovery events.
Each time the crawler discovers a URL on a page and makes a descision about it, the URL and the decision are logged.
</dd>
<dt>
<span class="term">
<code class="literal">url-extracted</code>
</span>
</dt>
<dd>
Events logged when we finish content extraction from a URL (maybe with some basic metadata extracted from the page).
</dd>
<dt>
<span class="term">
<code class="literal">url-output</code>
</span>
</dt>
<dd>
An event marking the end of URL processing.
</dd>
</dl>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="web-crawler-events-logs-reference-url-common"></a>Fields common to all URL lifecycle events<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.1/app-search-docs/guides/web-crawler-events-logs-reference.asciidoc">edit</a></h4>
</div></div></div>
<p>All URL lifecycle events include the following common fields:</p>
<p><span class="strong strong"><strong>Identification fields:</strong></span></p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">crawler.url.hash</code>
</span>
</dt>
<dd>
A unique identifier (hash) for the URL as it is handled by the crawler.
All events for the same URL within a single crawl share the same hash.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.url.source_hash</code>
</span>
</dt>
<dd>
A unique identifier of the URL that was used to discover this URL (only used for cases when a URL was discovered during a crawl and not submitted as a seed URL).
</dd>
</dl>
</div>
<p><span class="strong strong"><strong>URL details:</strong></span></p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">url.full</code>
</span>
</dt>
<dd>
The full URL string.
</dd>
<dt>
<span class="term">
<code class="literal">url.scheme</code>
</span>
</dt>
<dd>
Scheme portion of the URL.
</dd>
<dt>
<span class="term">
<code class="literal">url.domain</code>
</span>
</dt>
<dd>
Domain portion of the URL.
</dd>
<dt>
<span class="term">
<code class="literal">url.port</code>
</span>
</dt>
<dd>
Port of the URL.
</dd>
<dt>
<span class="term">
<code class="literal">url.path</code>
</span>
</dt>
<dd>
Path of the URL.
</dd>
<dt>
<span class="term">
<code class="literal">url.query</code>
</span>
</dt>
<dd>
URL query string.
Included when available.
</dd>
<dt>
<span class="term">
<code class="literal">url.fragment</code>
</span>
</dt>
<dd>
URL fragment.
Included when available.
</dd>
<dt>
<span class="term">
<code class="literal">url.username</code>
</span>
</dt>
<dd>
Username portion of the URL.
Included when available.
</dd>
<dt>
<span class="term">
<code class="literal">url.password</code>
</span>
</dt>
<dd>
Password portion of the URL.
Included when available.
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="web-crawler-events-logs-reference-url-seed"></a>URL seed events<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.1/app-search-docs/guides/web-crawler-events-logs-reference.asciidoc">edit</a></h4>
</div></div></div>
<p>These are small events used to track the flow of URLs into the crawler system and are primarily focused on tracking how a specific URL got into the backlog.</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">event.kind</code>
</span>
</dt>
<dd>
Set to <code class="literal">event</code>.
</dd>
<dt>
<span class="term">
<code class="literal">event.type</code>
</span>
</dt>
<dd>
Set to <code class="literal">start</code>.
</dd>
<dt>
<span class="term">
<code class="literal">event.action</code>
</span>
</dt>
<dd>
Set to <code class="literal">url-seed</code>.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.url.type</code>
</span>
</dt>
<dd>
<p>
A type of the URL being added:
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">content</code> for generic content URLs.
</li>
<li class="listitem">
<code class="literal">sitemap</code> for sitemap and sitemap-index URLs.
</li>
<li class="listitem">
<code class="literal">feed</code> for RSS/ATOM feeds.
</li>
</ul>
</div>
</dd>
<dt>
<span class="term">
<code class="literal">crawler.url.source_type</code>
</span>
</dt>
<dd>
<p>
A name of the source used for seeding the crawl:
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">seed-list</code> for seed-list URLs submitted as a part of the crawl configuration.
</li>
<li class="listitem">
<code class="literal">organic</code> for URLs discovered during a crawl by following organic links.
</li>
<li class="listitem">
<code class="literal">redirect</code> for pages discovered by following a redirect.
</li>
<li class="listitem">
<code class="literal">canonical-url</code> for pages discovered via the canonical URL meta tag.
</li>
</ul>
</div>
</dd>
<dt>
<span class="term">
<code class="literal">crawler.url.source_url.hash</code>
</span>
</dt>
<dd>
Set to the hash of the URL the crawler used to discover this page (only for URLs discovered during a crawl and not for entry points).
</dd>
<dt>
<span class="term">
<code class="literal">crawler.url.crawl_depth</code>
</span>
</dt>
<dd>
A positive number, indicating the number of steps the crawler had to take from our seed URLs set to reach this specific page.
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="web-crawler-events-logs-reference-url-fetch"></a>URL fetch events<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.1/app-search-docs/guides/web-crawler-events-logs-reference.asciidoc">edit</a></h4>
</div></div></div>
<p>These are the primary events that will be used for troubleshooting networking layer issues with a crawl.
They therefore aim to provide enough insight into what happened during a fetch attempt and what were the results.</p>
<p>These events represent a single HTTP request.
If the crawler followed redirects, it logs a separate record for each event including information about the redirect response to help with redirect chain troubleshooting.</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">event.kind</code>
</span>
</dt>
<dd>
Set to <code class="literal">event</code>.
</dd>
<dt>
<span class="term">
<code class="literal">event.type</code>
</span>
</dt>
<dd>
Set to <code class="literal">access</code>.
</dd>
<dt>
<span class="term">
<code class="literal">event.action</code>
</span>
</dt>
<dd>
Set to <code class="literal">url-fetch</code>.
</dd>
</dl>
</div>
<p><span class="strong strong"><strong>Event timing and outcome details:</strong></span></p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">event.start</code>
</span>
</dt>
<dd>
The start of the HTTP request.
</dd>
<dt>
<span class="term">
<code class="literal">event.end</code>
</span>
</dt>
<dd>
The end of the HTTP request.
</dd>
<dt>
<span class="term">
<code class="literal">event.duration</code>
</span>
</dt>
<dd>
Response timing for the HTTP request (total time it took to get the full response).
</dd>
<dt>
<span class="term">
<code class="literal">event.outcome</code>
</span>
</dt>
<dd>
<p>
An ECS categorization field.
Denotes whether the event represents a success or a failure from the perspective of the crawler:
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">failure</code> - for all 3xx, 4xx and 5xx responses.
</li>
<li class="listitem">
<code class="literal">success</code> - for all 2xx responses.
</li>
<li class="listitem">
<code class="literal">unknown</code> - for network timeouts.
</li>
</ul>
</div>
</dd>
</dl>
</div>
<p><span class="strong strong"><strong>HTTP request details:</strong></span></p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">http.request.method</code>
</span>
</dt>
<dd>
The method of the request.
</dd>
</dl>
</div>
<p><span class="strong strong"><strong>HTTP response details:</strong></span></p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">http.response.body.bytes</code>
</span>
</dt>
<dd>
The size of the response body in bytes (for successful responses only).
</dd>
<dt>
<span class="term">
<code class="literal">http.response.status_code</code>
</span>
</dt>
<dd>
A string status code.
</dd>
</dl>
</div>
<p><span class="strong strong"><strong>HTTP redirect details:</strong></span></p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">crawler.url.redirect.location</code>
</span>
</dt>
<dd>
A <code class="literal">Location</code> header content for redirect responses.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.url.redirect.count</code>
</span>
</dt>
<dd>
Number of redirects followed so far in a redirect chain (starts with 1 on the first redirect and is increased on each subsequent redirect until a non-redirect response is received or the maximum number of redirects is reached).
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="web-crawler-events-logs-reference-url-discover"></a>URL discover events<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.1/app-search-docs/guides/web-crawler-events-logs-reference.asciidoc">edit</a></h4>
</div></div></div>
<p>These are small events used to troubleshoot URL discovery within the crawler.
Each time the crawler sees a new URL (extracted from a page, from a sitemap or from following a redirect), it logs information about it along with the decision on what will happen to the newly discovered link.</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">event.kind</code>
</span>
</dt>
<dd>
Set to <code class="literal">event</code>.
</dd>
<dt>
<span class="term">
<code class="literal">event.action</code>
</span>
</dt>
<dd>
Set to <code class="literal">url-discover</code>.
</dd>
<dt>
<span class="term">
<code class="literal">event.type</code>
</span>
</dt>
<dd>
<p>
Depending on the decision regarding the URL, set to one of:
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">allowed</code> if the URL will be added to the backlog for future crawling.
</li>
<li class="listitem">
<code class="literal">denied</code> if the URL will not be followed (the <code class="literal">message</code> field will have a human-readable explanation of why the crawler decided not to follow it).
</li>
</ul>
</div>
</dd>
<dt>
<span class="term">
<code class="literal">crawler.url.source_type</code>
</span>
</dt>
<dd>
<p>
A type of the source used for discovering the link:
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">organic</code> for URLs discovered during a crawl by following organic links.
</li>
<li class="listitem">
<code class="literal">redirect</code> for pages discovered by following a redirect.
</li>
</ul>
</div>
</dd>
<dt>
<span class="term">
<code class="literal">crawler.url.source_url.hash</code>
</span>
</dt>
<dd>
Set to the hash of the URL the crawler used to discover this page (for URLs discovered during the crawl and not for entry points).
</dd>
<dt>
<span class="term">
<code class="literal">crawler.url.crawl_depth</code>
</span>
</dt>
<dd>
A positive number, indicating the number of steps the crawler had to take from our seed URLs set to reach this specific page.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.url.deny_reason</code>
</span>
</dt>
<dd>
<p>
A field with a code explaining the reason for skipping a URL during a crawl:
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">link_too_deep</code> when we hit a crawl depth limit.
</li>
<li class="listitem">
<code class="literal">link_too_long</code> when we hit a URL length limit.
</li>
<li class="listitem">
<code class="literal">link_with_too_many_params</code> when we hit a limit on the number of URL parameters allowed.
</li>
<li class="listitem">
<code class="literal">link_with_too_many_segments</code> when we hit a limit on the number of URL segments allowed.
</li>
<li class="listitem">
<code class="literal">queue_full</code> when we hit a backlog size limit.
</li>
<li class="listitem">
<code class="literal">sitemap_denied</code> when a URL is prohibited from crawling by a sitemap rule.
</li>
<li class="listitem">
<code class="literal">domain_filter_denied</code> for prohibited cross-domain links.
</li>
<li class="listitem">
<code class="literal">page_already_visited</code> for crawl-scoped URL de-duplication events.
</li>
<li class="listitem">
<code class="literal">incorrect_protocol</code> for non-HTTP links and non-HTTPS links in HTTPS-enforced mode.
</li>
</ul>
</div>
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="web-crawler-events-logs-reference-url-extracted"></a>URL extracted events<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.1/app-search-docs/guides/web-crawler-events-logs-reference.asciidoc">edit</a></h4>
</div></div></div>
<p>These events are focused on the extraction portion of the crawler process and are logged to help an operator troubleshoot the process of content extraction for the pages on their domains.
The primary focus here is capturing the details of the extraction process.</p>
<p>Each event represents a single extractor handling a single piece of content.</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">event.kind</code>
</span>
</dt>
<dd>
Set to <code class="literal">event</code>.
</dd>
<dt>
<span class="term">
<code class="literal">event.action</code>
</span>
</dt>
<dd>
Set to <code class="literal">url-extracted</code>.
</dd>
<dt>
<span class="term">
<code class="literal">event.module</code>
</span>
</dt>
<dd>
The name of the extractor generating the event (e.g. <code class="literal">html</code>).
</dd>
<dt>
<span class="term">
<code class="literal">event.type</code>
</span>
</dt>
<dd>
<p>
Depending on the decision regarding the URL, set to one of:
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">allowed</code> if the URL has been allowed to be indexed.
</li>
<li class="listitem">
<code class="literal">denied</code> if the URL has not been indexed because of a crawl rule, a <code class="literal">robots.txt</code> rule, etc (the <code class="literal">message</code> field will have a human-readable explanation of what happened).
</li>
</ul>
</div>
</dd>
</dl>
</div>
<p><span class="strong strong"><strong>Event timing and outcome details:</strong></span></p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">event.start</code>
</span>
</dt>
<dd>
The start of the extraction process.
</dd>
<dt>
<span class="term">
<code class="literal">event.end</code>
</span>
</dt>
<dd>
The end of the extraction process.
</dd>
<dt>
<span class="term">
<code class="literal">event.duration</code>
</span>
</dt>
<dd>
End-to-end timing for the extraction process (total time it took to get the data extracted).
</dd>
<dt>
<span class="term">
<code class="literal">event.outcome</code>
</span>
</dt>
<dd>
<p>
An ECS categorization field.
Denotes whether the event represents a success or a failure from the perspective of the crawler:
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">failure</code> if extraction process failed and we are going to drop the content.
</li>
<li class="listitem">
<code class="literal">success</code> if extraction process succeeded (or failed in a graceful manner).
</li>
</ul>
</div>
</dd>
</dl>
</div>
<p><span class="strong strong"><strong>Extraction result details:</strong></span></p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">crawler.extraction.content_type</code>
</span>
</dt>
<dd>
Content type for the page.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.extraction.content_size.bytes</code>
</span>
</dt>
<dd>
The size of the page.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.extraction.fields_extracted</code>
</span>
</dt>
<dd>
The list of fields extracted.
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="web-crawler-events-logs-reference-url-output"></a>URL output events<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.1/app-search-docs/guides/web-crawler-events-logs-reference.asciidoc">edit</a></h4>
</div></div></div>
<p>These events are designed to capture the results of ingestion of a single piece of content into an external system (file, App Search, etc).
The main goal here is to capture any data needed to tie a URL fetched and processed by the crawler to the changes performed in the external system as a result of the crawl.</p>
<p>Each event represents a single output module handling a single piece of content.</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">event.kind</code>
</span>
</dt>
<dd>
Set to <code class="literal">event</code>.
</dd>
<dt>
<span class="term">
<code class="literal">event.type</code>
</span>
</dt>
<dd>
Set to <code class="literal">end</code>.
</dd>
<dt>
<span class="term">
<code class="literal">event.action</code>
</span>
</dt>
<dd>
Set to <code class="literal">url-output</code>.
</dd>
<dt>
<span class="term">
<code class="literal">event.module</code>
</span>
</dt>
<dd>
The name of the output module generating the event (e.g. <code class="literal">file</code>, <code class="literal">app-search</code>).
</dd>
</dl>
</div>
<p><span class="strong strong"><strong>Event timing and outcome details:</strong></span></p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">event.start</code>
</span>
</dt>
<dd>
The start of the output ingestion process.
</dd>
<dt>
<span class="term">
<code class="literal">event.end</code>
</span>
</dt>
<dd>
The end of the output ingestion process.
</dd>
<dt>
<span class="term">
<code class="literal">event.duration</code>
</span>
</dt>
<dd>
End-to-end timing for the output ingestion process (total time it took to get the data processed by the module).
</dd>
<dt>
<span class="term">
<code class="literal">event.outcome</code>
</span>
</dt>
<dd>
<p>
An ECS categorization field.
Denotes whether the event represents a success or a failure from the perspective of the crawler:
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">failure</code> if output ingestion process failed and we are going to drop the content.
</li>
<li class="listitem">
<code class="literal">success</code> if output ingestion process succeeded (or failed in a graceful manner).
</li>
<li class="listitem">
<code class="literal">unknown</code> for cases specific to an output module.
</li>
</ul>
</div>
</dd>
</dl>
</div>
<p><span class="strong strong"><strong>Output ingestion results (<code class="literal">file</code> module):</strong></span></p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">crawler.output.file.directory</code>
</span>
</dt>
<dd>
The directory where the event has been logged.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.output.file.name</code>
</span>
</dt>
<dd>
The name of the file where the event has been logged (base name without the directory).
</dd>
</dl>
</div>
<p><span class="strong strong"><strong>Output ingestion results (<code class="literal">app-search</code> module):</strong></span></p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">crawler.output.app-search.engine.id</code>
</span>
</dt>
<dd>
The id of the engine used to ingest the content.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.output.app-search.engine.name</code>
</span>
</dt>
<dd>
The name of the engine used to ingest the content.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.output.app-search.document_id</code>
</span>
</dt>
<dd>
The id of the document within the engine.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.output.app-search.content_hash</code>
</span>
</dt>
<dd>
The content hash used for de-duplication purposes.
</dd>
</dl>
</div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-events-logs-reference-ingest"></a>Content ingestion events<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.1/app-search-docs/guides/web-crawler-events-logs-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>A special kind of event used to troubleshoot the ingestion process.
These events are used only by complex output modules and, potentially, only enabled in debug mode or by using a special crawl config option.
The goal of these events is to explain the ingestion process results in more details than could be captured by a URL output event.</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">event.kind</code>
</span>
</dt>
<dd>
Set to <code class="literal">event</code>.
</dd>
<dt>
<span class="term">
<code class="literal">event.type</code>
</span>
</dt>
<dd>
Set to <code class="literal">info</code>.
</dd>
<dt>
<span class="term">
<code class="literal">event.action</code>
</span>
</dt>
<dd>
Set to <code class="literal">ingest-progress</code>.
</dd>
<dt>
<span class="term">
<code class="literal">event.module</code>
</span>
</dt>
<dd>
The name of the output module generating the event (e.g. <code class="literal">file</code>, <code class="literal">app-search</code>).
</dd>
<dt>
<span class="term">
<code class="literal">message</code>
</span>
</dt>
<dd>
<p>
Details on what is happening with the extraction process.
</p>
<p>App Search logs URL-scoped events that explain how a specific piece of content from the crawler got ingested into the external system.
These are important for troubleshooting cases when the crawler discovers and crawls a URL, but due to App Search de-duplication logic the content does not get ingested, etc.</p>
</dd>
<dt>
<span class="term">
<code class="literal">ingest-progress</code>
</span>
</dt>
<dd>
An event logged by an output module to help an operator troubleshoot the ingestion process.
These are pretty generic events using the message field to explain what is happening.
</dd>
</dl>
</div>
<p><span class="strong strong"><strong>URL identification fields:</strong></span></p>
<p>These are used to correlate an ingestion event to the rest of the events generated by the crawler for a specific page:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">crawler.url.hash</code>
</span>
</dt>
<dd>
A unique identifier for the URL as it is handled by the crawler, all events for the same URL within a single crawl share the same hash (since it is calculated as SHA1 hash of the URL itself).
</dd>
<dt>
<span class="term">
<code class="literal">url.full</code>
</span>
</dt>
<dd>
The full URL string.
</dd>
<dt>
<span class="term">
<code class="literal">url.scheme</code>
</span>
</dt>
<dd>
Scheme portion of the URL.
</dd>
<dt>
<span class="term">
<code class="literal">url.domain</code>
</span>
</dt>
<dd>
Domain portion of the URL.
</dd>
<dt>
<span class="term">
<code class="literal">url.port</code>
</span>
</dt>
<dd>
Port of the URL.
</dd>
<dt>
<span class="term">
<code class="literal">url.path</code>
</span>
</dt>
<dd>
Path of the URL.
</dd>
<dt>
<span class="term">
<code class="literal">url.query</code>
</span>
</dt>
<dd>
URL query string.
Included when available.
</dd>
<dt>
<span class="term">
<code class="literal">url.fragment</code>
</span>
</dt>
<dd>
URL fragment.
Included when available.
</dd>
<dt>
<span class="term">
<code class="literal">url.username</code>
</span>
</dt>
<dd>
Username portion of the URL.
Included when available.
</dd>
<dt>
<span class="term">
<code class="literal">url.password</code>
</span>
</dt>
<dd>
Password portion of the URL.
Included when available.
</dd>
</dl>
</div>
</div>

</div>
<div class="navfooter">
<span class="prev">
<a href="web-crawler-reference.html">« Web crawler reference</a>
</span>
<span class="next">
<a href="api-reference.html">API Reference »</a>
</span>
</div>
</div>
</body>
</html>
