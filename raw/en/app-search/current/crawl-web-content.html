<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Crawl web content | Elastic App Search Documentation [7.11] | Elastic</title>
<link rel="home" href="index.html" title="Elastic App Search Documentation [7.11]"/>
<link rel="up" href="guides.html" title="Guides"/>
<link rel="prev" href="analytics-tags-guide.html" title="Analytics Tags Guide"/>
<link rel="next" href="curations-guide.html" title="Curations Guide"/>
<meta name="DC.type" content="Learn/Docs/App Search/Guide/7.11"/>
<meta name="DC.subject" content="App Search"/>
<meta name="DC.identifier" content="7.11"/>
</head>
<body>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="index.html">Elastic App Search Documentation [7.11]</a></span>
»
<span class="breadcrumb-link"><a href="guides.html">Guides</a></span>
»
<span class="breadcrumb-node">Crawl web content</span>
</div>
<div class="navheader">
<span class="prev">
<a href="analytics-tags-guide.html">« Analytics Tags Guide</a>
</span>
<span class="next">
<a href="curations-guide.html">Curations Guide »</a>
</span>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="crawl-web-content"></a>Crawl web content<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h2>
</div></div></div>
<div class="caution admon">
<div class="icon"></div>
<div class="admon_content">
<p>The Elastic Enterprise Search web crawler is a <span class="strong strong"><strong>beta</strong></span> feature.
Beta features are subject to change and are not covered by the support SLA of general release (GA) features.
Elastic plans to promote this feature to GA in a future release.</p>
</div>
</div>
<p>Complete the following steps to crawl your web content using the Enterprise Search web crawler.</p>
<p><span class="strong strong"><strong>1. Identify your web content and create engines:</strong></span></p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-identify-web-content" title="Identify web content">Identify web content</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-create-engine" title="Create engine">Create engine</a>
</li>
</ul>
</div>
<p><span class="strong strong"><strong>2. For each engine, complete the first crawl cycle:</strong></span></p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<p><a class="xref" href="crawl-web-content.html#crawl-web-content-manage-crawl" title="Manage crawl">Manage crawl</a></p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-manage-domains" title="Manage domains">Manage domains</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-manage-entry-points" title="Manage entry points">Manage entry points</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-manage-crawl-rules" title="Manage crawl rules">Manage crawl rules</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-embed-web-crawler-instructions" title="Embed web crawler instructions within content">Embed web crawler instructions within content</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-start-crawl" title="Start crawl">Start crawl</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-cancel-crawl" title="Cancel crawl">Cancel crawl</a>
</li>
</ul>
</div>
</li>
<li class="listitem">
<p><a class="xref" href="crawl-web-content.html#crawl-web-content-monitor-crawl" title="Monitor crawl">Monitor crawl</a></p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-view-crawl-status" title="View crawl status">View crawl status</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-view-crawl-request-id" title="View crawl request ID">View crawl request ID</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-view-web-crawler-events-logs" title="View web crawler events logs">View web crawler events logs</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-view-web-crawler-events-by-crawl-id-and-url" title="View web crawler events by crawl ID and URL">View web crawler events by crawl ID and URL</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-view-web-crawler-system-logs" title="View web crawler system logs">View web crawler system logs</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-view-indexed-documents" title="View indexed documents">View indexed documents</a>
</li>
</ul>
</div>
</li>
<li class="listitem">
<p><a class="xref" href="crawl-web-content.html#crawl-web-content-troubleshoot-crawl" title="Troubleshoot crawl">Troubleshoot crawl</a></p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-troubleshoot-crawl-stability" title="Troubleshoot crawl stability">Troubleshoot crawl stability</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-troubleshoot-content-discovery" title="Troubleshoot content discovery">Troubleshoot content discovery</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-troubleshoot-content-extraction-and-indexing" title="Troubleshoot content extraction and indexing">Troubleshoot content extraction and indexing</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-provide-feedback" title="Provide feedback">Provide feedback</a>
</li>
</ul>
</div>
</li>
</ul>
</div>
<p><span class="strong strong"><strong>3. Re-crawl your web content and optionally schedule crawls:</strong></span></p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-re-crawl" title="Re-crawl web content">Re-crawl web content</a>
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-schedule-crawls" title="Schedule crawls">Schedule crawls</a>
</li>
</ul>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="crawl-web-content-identify-web-content"></a>Identify web content<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h3>
</div></div></div>
<p>Before crawling your web content, you must inventory your domains and decide which you&#8217;d like to crawl and where you&#8217;d like to store the crawled documents.
Consider an organization managing the following web content:</p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Content type</th>
<th align="left" valign="top">URL</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p>Website</p></td>
<td align="left" valign="top"><p><code class="literal">https://example.com</code></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Blog</p></td>
<td align="left" valign="top"><p><code class="literal">https://example.com/blog</code></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Ecommerce application</p></td>
<td align="left" valign="top"><p><code class="literal">https://shop.example.com</code></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Ecommerce administrative dashboard</p></td>
<td align="left" valign="top"><p><code class="literal">https://shop.example.com/admin</code></p></td>
</tr>
</tbody>
</table>
</div>
<p>This organization may decide to index their website and blog using the web crawler, while using the <a class="xref" href="documents.html" title="Documents API">Documents API</a> to index their ecommerce data.</p>
<p>Complete this exercise with your own content to determine which domains you&#8217;d like to crawl.
If you haven&#8217;t already, read the <a class="xref" href="web-crawler-faq.html" title="Web crawler (beta) FAQ">Web crawler (beta) FAQ</a> to evaluate the crawler&#8217;s capabilities and limitations.</p>
<p>After choosing domains to crawl, decide where you will store the resulting search documents.
Consider another organization with the following web content.</p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Content type</th>
<th align="left" valign="top">URL</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p>Website</p></td>
<td align="left" valign="top"><p><code class="literal">https://example.com</code></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Blog</p></td>
<td align="left" valign="top"><p><code class="literal">https://blog.example.com</code></p></td>
</tr>
</tbody>
</table>
</div>
<p>Although their website and blog are separate domains, they may choose to index them into a single engine.</p>
<p>Again, complete this exercise with your own content.
Choose one engine per search experience.
Each engine has its own crawl configuration, and is limited to a single active crawl.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="crawl-web-content-create-engine"></a>Create engine<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h3>
</div></div></div>
<p>After reviewing <a class="xref" href="crawl-web-content.html#crawl-web-content-identify-web-content" title="Identify web content">Identify web content</a>, create one or more new engines for your content.
See <a class="xref" href="getting-started.html#getting-started-with-app-search-engine" title="1 - Create an Engine">Create an engine</a> for an explanation of the process.</p>
<p>The following sections of this document describe a crawl cycle composed of the following steps: <em>manage</em>, <em>monitor</em>, <em>troubleshoot</em>.
Repeat this cycle for each new engine.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="crawl-web-content-manage-crawl"></a>Manage crawl<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h3>
</div></div></div>
<p>A crawl is the process by which the web crawler discovers, extracts, and indexes web content into an engine.
See <a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl" title="Crawl">Crawl</a> in the web crawler reference for a detailed explanation of a crawl.</p>
<p>Primarily, you manage each crawl in the App Search dashboard.
There, you manage domains, entry points, and crawl rules; and start and cancel the active crawl.
However, you can also manage a crawl by embedding instructions within your content, such as canonical URL link tags, robots meta tags, and nofollow links.
You can also start and cancel a crawl using the App Search API.</p>
<p>The following sections cover these topics.</p>
<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-manage-domains"></a>Manage domains<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>A domain is a website or property you&#8217;d like to crawl.
You must associate one or more domains to a crawl.
See <a class="xref" href="web-crawler-reference.html#web-crawler-reference-domain" title="Domain">Domain</a> in the web crawler reference for a detailed explanation of a domain.</p>
<p>Manage the domains for a crawl through the web crawler dashboard.
From the engine menu, choose <span class="strong strong"><strong>Web Crawler</strong></span>.</p>
<div class="imageblock">
<div class="content">
<img src="images/app-search/web-crawler-navigation.png" alt="web crawler navigation">
</div>
</div>
<p>Add your first domain on the getting started screen.</p>
<div class="imageblock">
<div class="content">
<img src="images/app-search/add-first-domain.png" alt="add first domain">
</div>
</div>
<p>From there, you can view, add, manage, and delete domains using the web crawler dashboard.</p>
<div class="imageblock">
<div class="content">
<img src="images/app-search/web-crawler-dashboard-domains.png" alt="web crawler dashboard domains">
</div>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-manage-entry-points"></a>Manage entry points<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>Each domain must have one or more entry points.
These are paths from which the crawler will start each crawl.
See <a class="xref" href="web-crawler-reference.html#web-crawler-reference-entry-point" title="Entry point">Entry point</a> in the web crawler reference for a detailed explanation of an entry point.</p>
<p>Manage the entry points for a domain through the domain dashboard.
From the engine menu, choose <span class="strong strong"><strong>Web Crawler</strong></span>.
Choose <span class="strong strong"><strong>Manage</strong></span> next to the domain you&#8217;d like to manage.
Then locate the <em>Entry Points</em> section of the dashboard.</p>
<div class="imageblock">
<div class="content">
<img src="images/app-search/domains-dashboard-entry-points.png" alt="domains dashboard entry points">
</div>
</div>
<p>From here, you can view, add, edit, and delete entry points.</p>
<p>The dashboard adds a default entry point of <code class="literal">/</code> to each domain.
You can delete this entry point, but each domain must have at least one entry point.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-manage-crawl-rules"></a>Manage crawl rules<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>Each domain must also have one or more crawl rules.
These rules instruct the crawler which pages to crawl within the domain.
See <a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl-rule" title="Crawl rule">Crawl rule</a> in the web crawler reference for a detailed explanation of a crawl rule.</p>
<p>Manage the crawl rules for a domain through the domain dashboard.
From the engine menu, choose <span class="strong strong"><strong>Web Crawler</strong></span>.
Choose <span class="strong strong"><strong>Manage</strong></span> next to the domain you&#8217;d like to manage.
Then locate the <em>Crawl Rules</em> section of the dashboard.</p>
<div class="imageblock">
<div class="content">
<img src="images/app-search/domains-dashboard-crawl-rules.png" alt="domains dashboard crawl rules">
</div>
</div>
<p>From here, you can view, add, edit, delete, and re-order crawl rules.</p>
<p>The dashboard adds a default crawl rule to allow all paths.
You cannot delete this crawl rule, but you can insert more restrictive rules in front of this rule.
See <a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl-rule" title="Crawl rule">Crawl rule</a> for explanations of crawl rule logic and the effects of crawl rule order.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-embed-web-crawler-instructions"></a>Embed web crawler instructions within content<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>You can also embed instructions for the web crawler within your HTML content.
These instructions are specific HTML tags, attributes, and values that affect the web crawler&#8217;s behavior.</p>
<p>The Enterprise Search web crawler recognizes the following embedded instructions, each of which is described further in the web crawler reference:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-canonical-url-link-tag" title="Canonical URL link tag">Canonical URL link tag</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-robots-meta-tags" title="Robots meta tags">Robots meta tags</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-nofollow-link" title="Nofollow link">Nofollow link</a>
</li>
</ul>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-start-crawl"></a>Start crawl<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>Start a crawl from the web crawler or domain dashboard, or using the App Search API.</p>
<p>To use a dashboard, navigate to <span class="strong strong"><strong>Web Crawler</strong></span>, then optionally choose a domain to manage.
Choose the <span class="strong strong"><strong>Start a Crawl</strong></span> button at the top of the dashboard.</p>
<div class="imageblock">
<div class="content">
<img src="images/app-search/start-crawl-button-default.png" alt="start crawl button default">
</div>
</div>
<p>Each engine may have only one active crawl.
The start button changes state to reflect a crawl is in progress.</p>
<div class="imageblock">
<div class="content">
<img src="images/app-search/start-crawl-button-crawling.png" alt="start crawl button crawling">
</div>
</div>
<p>To start a crawl programatically, refer to the following API reference:</p>
<p><a class="xref" href="web-crawler-api-reference.html#web-crawler-apis-post-crawler-crawl-requests" title="Create a new crawl request">Create a new crawl request</a></p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-cancel-crawl"></a>Cancel crawl<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>Cancel an active crawl from the web crawler or domain dashboard, or using the App Search API.</p>
<p>To use a dashboard, navigate to <span class="strong strong"><strong>Web Crawler</strong></span>, then optionally choose a domain to manage.
Expand the <span class="strong strong"><strong>Crawling&#8230;&#8203;</strong></span> button at the top of the dashboard.
Choose <span class="strong strong"><strong>Cancel Crawl</strong></span>.</p>
<div class="imageblock">
<div class="content">
<img src="images/app-search/cancel-crawl-button.png" alt="cancel crawl button">
</div>
</div>
<p>To cancel a crawl programatically, refer to the following API reference:</p>
<p><a class="xref" href="web-crawler-api-reference.html#web-crawler-apis-post-crawler-crawl-requests-active-cancel" title="Cancel an active crawl">Cancel an active crawl</a></p>
<p>Due to a known issue in Enterprise Search 7.11.0, you cannot cancel a crawl while the crawl is running its <a class="xref" href="web-crawler-reference.html#web-crawler-reference-content-deletion" title="Content deletion">Content deletion</a> phase.</p>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="crawl-web-content-monitor-crawl"></a>Monitor crawl<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h3>
</div></div></div>
<p>You can monitor a crawl while it is running or audit the crawl after it has completed.</p>
<p>Monitoring includes viewing the crawl status, crawl request ID, web crawler event logs (optionally filtered by the crawl ID and a specific URL), web crawler system logs, and documents indexed by the crawl.</p>
<p>The following sections cover these topics.</p>
<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-view-crawl-status"></a>View crawl status<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>Each crawl has a status, which quickly communicates its state.
See <a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl-status" title="Crawl status">Crawl status</a> in the web crawler reference for a description of each crawl status.</p>
<p>View the status of a crawl within the web crawler dashboard or using the App Search API.</p>
<p>To use the dashboard, navigate to <span class="strong strong"><strong>Web Crawler</strong></span> and locate the <em>Recent crawl requests</em> section.</p>
<div class="imageblock">
<div class="content">
<img src="images/app-search/domains-dashboard-recent-crawl-requests.png" alt="domains dashboard recent crawl requests">
</div>
</div>
<p>Refer to the <em>Status</em> column for the status of each recent crawl.</p>
<p>To get a crawl status programatically, refer to the following API references:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="web-crawler-api-reference.html#web-crawler-apis-get-crawler-crawl-requests-active" title="Get current active crawl request">Get current active crawl request</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-api-reference.html#web-crawler-apis-get-crawler-crawl-requests" title="List crawl requests">List crawl requests</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-api-reference.html#web-crawler-apis-get-crawler-crawl-requests-id" title="View details for a crawl request">View details for a crawl request</a>
</li>
</ul>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-view-crawl-request-id"></a>View crawl request ID<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>Each crawl has an associated <em>crawl request</em>, which is identified by a unique ID in the following format: <code class="literal">60106315beae67d49a8e787d</code>.
Use a crawl request ID to filter the <a class="xref" href="crawl-web-content.html#crawl-web-content-view-web-crawler-events-logs" title="View web crawler events logs">web crawler events logs</a> to a specific crawl.</p>
<p>View the request ID of a crawl within the web crawler dashboard or using the App Search API.</p>
<p>To use the dashboard, navigate to <span class="strong strong"><strong>Web Crawler</strong></span> and locate the <em>Recent crawl requests</em> section.</p>
<div class="imageblock">
<div class="content">
<img src="images/app-search/domains-dashboard-recent-crawl-requests.png" alt="domains dashboard recent crawl requests">
</div>
</div>
<p>Refer to the <em>Request ID</em> column for the request ID of each recent crawl.</p>
<p>To get a crawl request ID programatically, refer to the following API references:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="web-crawler-api-reference.html#web-crawler-apis-get-crawler-crawl-requests-active" title="Get current active crawl request">Get current active crawl request</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-api-reference.html#web-crawler-apis-get-crawler-crawl-requests" title="List crawl requests">List crawl requests</a>
</li>
</ul>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-view-web-crawler-events-logs"></a>View web crawler events logs<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>The Enterprise Search web crawler records detailed structured events logs for each crawl.
The crawler indexes these logs into Elasticsearch, and you can view the logs using <a href="/guide/en/kibana/current/index.html" class="ulink" target="_blank" rel="noopener">Kibana</a>.</p>
<p>See <a class="xref" href="view-web-crawler-events-logs.html" title="View web crawler events logs">View web crawler events logs</a> for a step by step process to view the web crawler events logs in Kibana.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-view-web-crawler-events-by-crawl-id-and-url"></a>View web crawler events by crawl ID and URL<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>To monitor a specific crawl or a specific domain, you must filter the <a class="xref" href="crawl-web-content.html#crawl-web-content-view-web-crawler-events-logs" title="View web crawler events logs">web crawler events logs</a> within Kibana.</p>
<p>To view the events for a specific crawl, first <a class="xref" href="crawl-web-content.html#crawl-web-content-view-crawl-request-id" title="View crawl request ID">get the crawl&#8217;s request ID</a>.
Then filter within Kibana on the <code class="literal">crawler.crawl.id</code> field.</p>
<p>You can filter further to narrow your results to a specific URL.
Use the following fields:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
The full URL: <code class="literal">url.full</code>
</li>
<li class="listitem">
Required components of the URL: <code class="literal">url.scheme</code>, <code class="literal">url.domain</code>, <code class="literal">url.port</code>, <code class="literal">url.path</code>
</li>
<li class="listitem">
Optional components of the URL: <code class="literal">url.query</code>, <code class="literal">url.fragment</code>, <code class="literal">url.username</code>, <code class="literal">url.password</code>
</li>
</ul>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-view-web-crawler-system-logs"></a>View web crawler system logs<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>If you are managing your own Enterprise Search deployment, you can also view the web crawler system logs.</p>
<p>View these logs on disk in the <code class="literal">crawler.log</code> file.</p>
<p>The events in these logs are less verbose then the web crawler events logs, but they can help solve web crawler issues.
Each event has a crawl request ID, which allows you to analyze the logs for a specific crawl.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-view-indexed-documents"></a>View indexed documents<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>The web crawler extracts the content from each web page, transforming it into a search document.
It indexes these documents within the engine associated with the crawl.
See <a class="xref" href="web-crawler-reference.html#web-crawler-reference-content-extraction-and-indexing" title="Content extraction and indexing">Content extraction and indexing</a> for more details on this process, and see <a class="xref" href="web-crawler-reference.html#web-crawler-reference-web-crawler-schema" title="Web crawler schema">Web crawler schema</a> for more details on the structure of each search document.</p>
<p>View the indexed documents using the Documents or Query Tester views within the App Search dashboard, or use the search API.</p>
<p><span class="strong strong"><strong>To find a specific document</strong></span>, wrap the document&#8217;s URL in quotes, and use that as your search query.
For example: <code class="literal">"https://example.com/some/page.html"</code>.
If the document is present in the engine, it should be a top result (or only result).</p>
<p>To access the documents dashboard, choose <span class="strong strong"><strong>Documents</strong></span> from the engine menu.</p>
<div class="imageblock">
<div class="content">
<img src="images/app-search/documents-dashboard.png" alt="documents dashboard">
</div>
</div>
<p>To access the query tester, choose <span class="strong strong"><strong>Query Tester</strong></span> from the engine menu.</p>
<div class="imageblock">
<div class="content">
<img src="images/app-search/query-tester-dashboard.png" alt="query tester dashboard">
</div>
</div>
<p>To use the search API, refer to the following API reference:</p>
<p><a class="xref" href="search.html" title="Search API">Search API</a></p>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="crawl-web-content-troubleshoot-crawl"></a>Troubleshoot crawl<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h3>
</div></div></div>
<p>A crawl may not behave as expected or discover and index the documents you expected.
The web crawler faces many challenges while it crawls, including:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Network issues: lost packets, timeouts, DNS issues
</li>
<li class="listitem">
Resource contention: memory usage, CPU cycles
</li>
<li class="listitem">
Parsing problems: broken HTML
</li>
<li class="listitem">
HTTP protocol issues: broken HTTP servers, incorrect HTTP status codes
</li>
</ul>
</div>
<p>For a detailed look at crawl issues, see <a href="/elasticon/archive/2020/global/sprinting-to-a-crawl-building-an-effective-web-crawler" class="ulink" target="_blank" rel="noopener">Sprinting to a crawl: Building an effective web crawler</a>.</p>
<p>However, these issues generally fall into three categories: <em>crawl stability</em>, <em>content discovery</em>, and <em>content extraction and indexing</em>.
Use the following sections to guide your troubleshooting:</p>
<p>See <a class="xref" href="crawl-web-content.html#crawl-web-content-troubleshoot-crawl-stability" title="Troubleshoot crawl stability">Troubleshoot crawl stability</a> if:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
You&#8217;re not sure where to start (resolve stability issues first)
</li>
<li class="listitem">
No documents in the engine
</li>
<li class="listitem">
Many documents missing or outdated
</li>
<li class="listitem">
Crawl fails
</li>
<li class="listitem">
Crawl runs for the maximum duration (defaults to 24 hours)
</li>
</ul>
</div>
<p>See <a class="xref" href="crawl-web-content.html#crawl-web-content-troubleshoot-content-discovery" title="Troubleshoot content discovery">Troubleshoot content discovery</a> if:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Specific documents missing or outdated
</li>
</ul>
</div>
<p>See <a class="xref" href="crawl-web-content.html#crawl-web-content-troubleshoot-content-extraction-and-indexing" title="Troubleshoot content extraction and indexing">Troubleshoot content extraction and indexing</a> if:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Specific documents missing or outdated
</li>
<li class="listitem">
Incorrect content within documents
</li>
<li class="listitem">
Content missing from documents
</li>
</ul>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-troubleshoot-crawl-stability"></a>Troubleshoot crawl stability<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>Crawl stability issues prevent the crawler from discovering, extracting, and indexing your content.
It is therefore critical you address these issues first.</p>
<p>Use the following techniques to troubleshoot crawl stability issues.</p>
<p><span class="strong strong"><strong>Analyze web crawler events logs for the most recent crawl:</strong></span></p>
<p>First:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-view-crawl-request-id" title="View crawl request ID">Find the crawl request ID</a> for the most recent crawl.
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-view-web-crawler-events-by-crawl-id-and-url" title="View web crawler events by crawl ID and URL">Filter the web crawler events logs</a> by that ID.
</li>
</ol>
</div>
<p>Then:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Order the events by timestamp, oldest first.
</li>
<li class="listitem">
Locate the <code class="literal">crawl-end</code> event and preceding events.
These events communicate what happened before the crawl failed.
</li>
</ul>
</div>
<p><span class="strong strong"><strong>Analyze web crawler system logs:</strong></span></p>
<p>These logs may contain additional information about your crawl.</p>
<p>See <a class="xref" href="crawl-web-content.html#crawl-web-content-view-web-crawler-system-logs" title="View web crawler system logs">View web crawler system logs</a>.</p>
<p><span class="strong strong"><strong>Modify the web crawler configuration:</strong></span></p>
<p>As a last resort, operators can modify the web crawler configuration, including resource limits.</p>
<p>See <a class="xref" href="web-crawler-reference.html#web-crawler-reference-web-crawler-configuration-settings" title="Web crawler configuration settings">Web crawler configuration settings</a> in the web crawler reference.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-troubleshoot-content-discovery"></a>Troubleshoot content discovery<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>After your crawls are stable, you may find the crawler is not discovering your content as expected.
It&#8217;s helpful to understand how the web crawler discovers content.
See <a class="xref" href="web-crawler-reference.html#web-crawler-reference-content-discovery" title="Content discovery">Content discovery</a> in the web crawler reference.</p>
<p>Use the following techniques to troubleshoot content discovery issues.</p>
<p><span class="strong strong"><strong>Confirm the most recent crawl completed successfully:</strong></span></p>
<p>View the status of the most recent crawl to confirm it completed successfully.
See <a class="xref" href="crawl-web-content.html#crawl-web-content-view-crawl-status" title="View crawl status">View crawl status</a>.</p>
<p>If the crawl failed, look for signs of crawl stability issues.
See <a class="xref" href="crawl-web-content.html#crawl-web-content-troubleshoot-crawl-stability" title="Troubleshoot crawl stability">Troubleshoot crawl stability</a>.</p>
<p><span class="strong strong"><strong>View indexed documents to confirm missing pages:</strong></span></p>
<p>Identify which pages are missing from your engine, or focus on specific pages.
See <a class="xref" href="crawl-web-content.html#crawl-web-content-view-indexed-documents" title="View indexed documents">View indexed documents</a> for instructions to view all documents and specific documents.</p>
<p><span class="strong strong"><strong>Analyze web crawler events logs for the most recent crawl:</strong></span></p>
<p>First:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-view-crawl-request-id" title="View crawl request ID">Find the crawl request ID</a> for the most recent crawl.
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-view-web-crawler-events-by-crawl-id-and-url" title="View web crawler events by crawl ID and URL">Filter the web crawler events logs</a>by that ID.
</li>
<li class="listitem">
Find the URL of a specific document missing from the engine.
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-view-web-crawler-events-by-crawl-id-and-url" title="View web crawler events by crawl ID and URL">Filter the web crawler events logs</a> by that URL.
</li>
</ol>
</div>
<p>Then:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Locate <code class="literal">url-discover</code> events to confirm the crawler has seen links to your page.
The <code class="literal">outcome</code> and <code class="literal">message</code> fields may explain why the web crawler did not crawl the page.
</li>
<li class="listitem">
If <code class="literal">url-discover</code> events indicate discovery was successful, locate <code class="literal">url-fetch</code> events to analyze the fetching phase of the crawl.
</li>
</ul>
</div>
<p><span class="strong strong"><strong>Analyze web crawler system logs:</strong></span></p>
<p>These may contain additional information about specific pages.</p>
<p>See <a class="xref" href="crawl-web-content.html#crawl-web-content-view-web-crawler-system-logs" title="View web crawler system logs">View web crawler system logs</a>.</p>
<p><span class="strong strong"><strong>Address specific content discovery problems:</strong></span></p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Problem</th>
<th align="left" valign="top">Description</th>
<th align="left" valign="top">Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p>External domain</p></td>
<td align="left" valign="top"><p>The web crawler does not follow links that go outside the <a class="xref" href="web-crawler-reference.html#web-crawler-reference-domain" title="Domain">domains</a> configured for each crawl.</p></td>
<td align="left" valign="top"><p><a class="xref" href="crawl-web-content.html#crawl-web-content-manage-domains" title="Manage domains">Manage domains</a> for your crawl to add any missing domains.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Disallowed path</p></td>
<td align="left" valign="top"><p>The web crawler does not follow links whose paths are disallowed by the <a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl-rule" title="Crawl rule">crawl rules</a> for each domain.</p></td>
<td align="left" valign="top"><p><a class="xref" href="crawl-web-content.html#crawl-web-content-manage-crawl-rules" title="Manage crawl rules">Manage crawl rules</a> for each domain to ensure paths are allowed.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>No incoming links</p></td>
<td align="left" valign="top"><p>The web crawler cannot find pages that have no incoming links, unless you provide the path as an entry point. See <a class="xref" href="web-crawler-reference.html#web-crawler-reference-content-discovery" title="Content discovery">Content discovery</a> for an explanation of how the web crawler discovers content.</p></td>
<td align="left" valign="top"><p>Add links to the content from other content that the web crawler has already discovered, or explicitly <a class="xref" href="crawl-web-content.html#crawl-web-content-manage-entry-points" title="Manage entry points">add the path as an entry point</a>.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Nofollow links</p></td>
<td align="left" valign="top"><p>The web crawler does not follow <a class="xref" href="web-crawler-reference.html#web-crawler-reference-nofollow-link" title="Nofollow link">nofollow links</a>.</p></td>
<td align="left" valign="top"><p>Remove the nofollow link to allow content discovery.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">nofollow</code> robots meta tag</p></td>
<td align="left" valign="top"><p>If a page contains a <code class="literal">nofollow</code> <a class="xref" href="web-crawler-reference.html#web-crawler-reference-robots-meta-tags" title="Robots meta tags">robots meta tag</a>, the web crawler will not follows links from that page.</p></td>
<td align="left" valign="top"><p>Remove the meta tag from your page.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Redirect loop</p></td>
<td align="left" valign="top"><p>The web crawler cannot discover and index content if redirects result in an infinite loop.</p></td>
<td align="left" valign="top"><p>Ensure all HTTP redirects eventually lead to a <code class="literal">2xx</code>, <code class="literal">4xx</code>, or <code class="literal">5xx</code> response.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>HTTP errors</p></td>
<td align="left" valign="top"><p>The web crawler cannot discover and index content if it cannot fetch HTML pages from a domain.
The web crawler will not index pages that respond with a <code class="literal">4xx</code> or <code class="literal">5xx</code> response code.</p></td>
<td align="left" valign="top"><p>Fix HTTP server errors.
Ensure correct HTTP response codes.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>HTML errors</p></td>
<td align="left" valign="top"><p>The web crawler cannot parse extremely broken HTML pages.
In that case, the web crawler cannot index the page, and cannot discover links coming from that page.</p></td>
<td align="left" valign="top"><p>Use the <a href="https://validator.w3.org/" class="ulink" target="_blank" rel="noopener">W3C markup validation service</a> to identify and resolve HTML errors in your content.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Security</p></td>
<td align="left" valign="top"><p>The web crawler cannot access content requiring authentication or authorization.</p></td>
<td align="left" valign="top"><p>Remove the security to allow access to the web crawler.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Non-HTML content</p></td>
<td align="left" valign="top"><p>The web crawler does not extract and index non-HTML content (e.g. JavaScript, PDF).</p></td>
<td align="left" valign="top"><p>Publish your content in HTML format.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Non-HTTP protocol</p></td>
<td align="left" valign="top"><p>The web crawler recognizes only the HTTP and HTTPS protocols.</p></td>
<td align="left" valign="top"><p>Publish your content at URLs using HTTP or HTTPS protocols.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Invalid SSL certificate</p></td>
<td align="left" valign="top"><p>The web crawler will not crawl HTTPS pages with invalid certificates.</p></td>
<td align="left" valign="top"><p>Replace invalid certificates with valid certificates.</p></td>
</tr>
</tbody>
</table>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-troubleshoot-content-extraction-and-indexing"></a>Troubleshoot content extraction and indexing<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>The web crawler may be discovering your content but not extracting and indexing it as expected.
It&#8217;s helpful to understand how the web crawler extracts and indexes content.
See <a class="xref" href="web-crawler-reference.html#web-crawler-reference-content-extraction-and-indexing" title="Content extraction and indexing">Content extraction and indexing</a> in the web crawler reference.</p>
<p>Use the following techniques to troubleshoot content discovery issues.</p>
<p><span class="strong strong"><strong>Confirm the most recent crawl completed successfully:</strong></span></p>
<p>View the status of the most recent crawl to confirm it completed successfully.
See <a class="xref" href="crawl-web-content.html#crawl-web-content-view-crawl-status" title="View crawl status">View crawl status</a>.</p>
<p>If the crawl failed, look for signs of crawl stability issues.
See <a class="xref" href="crawl-web-content.html#crawl-web-content-troubleshoot-crawl-stability" title="Troubleshoot crawl stability">Troubleshoot crawl stability</a>.</p>
<p><span class="strong strong"><strong>View indexed documents to confirm missing pages:</strong></span></p>
<p>Identify which pages are missing from your engine, or focus on specific pages.
See <a class="xref" href="crawl-web-content.html#crawl-web-content-view-indexed-documents" title="View indexed documents">View indexed documents</a> for instructions to view all documents and specific documents.</p>
<p>If documents are missing from the engine, look for signs of content discovery issues.
See <a class="xref" href="crawl-web-content.html#crawl-web-content-troubleshoot-content-discovery" title="Troubleshoot content discovery">Troubleshoot content discovery</a>.</p>
<p><span class="strong strong"><strong>Analyze web crawler events logs for the most recent crawl:</strong></span></p>
<p>First:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-view-crawl-request-id" title="View crawl request ID">Find the crawl request ID</a> for the most recent crawl.
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-view-web-crawler-events-by-crawl-id-and-url" title="View web crawler events by crawl ID and URL">Filter the web crawler events logs</a>by that ID.
</li>
<li class="listitem">
Find the URL of a specific document missing from the engine.
</li>
<li class="listitem">
<a class="xref" href="crawl-web-content.html#crawl-web-content-view-web-crawler-events-by-crawl-id-and-url" title="View web crawler events by crawl ID and URL">Filter the web crawler events logs</a> by that URL.
</li>
</ol>
</div>
<p>Then:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Locate <code class="literal">url-extracted</code> events to confirm the crawler was able to extract content from your page.
The <code class="literal">outcome</code> and <code class="literal">message</code> fields may explain why the web crawler could not extract and index the content.
</li>
<li class="listitem">
If <code class="literal">url-extracted</code> events indicate extraction was successful, locate <code class="literal">url-output</code> events to confirm the web crawler attempted ingestion of the page&#8217;s content.
</li>
</ul>
</div>
<p><span class="strong strong"><strong>Analyze web crawler system logs:</strong></span></p>
<p>These may contain additional information about specific pages.</p>
<p>See <a class="xref" href="crawl-web-content.html#crawl-web-content-view-web-crawler-system-logs" title="View web crawler system logs">View web crawler system logs</a>.</p>
<p><span class="strong strong"><strong>Address specific content extraction and indexing problems:</strong></span></p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Problem</th>
<th align="left" valign="top">Description</th>
<th align="left" valign="top">Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p>Duplicate content</p></td>
<td align="left" valign="top"><p>If your website contains pages with duplicate content, those pages are stored as a single document within your engine.
The document&#8217;s <code class="literal">additional_urls</code> field indicates the URLs that contain the same content.</p></td>
<td align="left" valign="top"><p>Use a <a class="xref" href="web-crawler-reference.html#web-crawler-reference-canonical-url-link-tag" title="Canonical URL link tag">canonical URL link tag</a> within any document containing duplicate content.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Non-HTML content</p></td>
<td align="left" valign="top"><p>The web crawler does not extract and index non-HTML content (e.g. JavaScript, PDF).</p></td>
<td align="left" valign="top"><p>Publish your content in HTML format.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">noindex</code> robots meta tag</p></td>
<td align="left" valign="top"><p>The web crawler will not index pages that include a <code class="literal">noindex</code> <a class="xref" href="web-crawler-reference.html#web-crawler-reference-robots-meta-tags" title="Robots meta tags">robots meta tag</a>.</p></td>
<td align="left" valign="top"><p>Remove the meta tag from your page.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Page too large</p></td>
<td align="left" valign="top"><p>The web crawler cannot parse extremely large HTML pages.</p></td>
<td align="left" valign="top"><p>Reduce the size of your page.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Broken HTML</p></td>
<td align="left" valign="top"><p>The web crawler cannot parse extremely broken HTML pages.</p></td>
<td align="left" valign="top"><p>Use the <a href="https://validator.w3.org/" class="ulink" target="_blank" rel="noopener">W3C markup validation service</a> to identify and resolve HTML errors in your content.</p></td>
</tr>
</tbody>
</table>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="crawl-web-content-provide-feedback"></a>Provide feedback<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h4>
</div></div></div>
<p>After troubleshooting your crawl, we&#8217;d love to know what worked, what didn&#8217;t, and what we can improve.</p>
<p>Please <a class="xref" href="web-crawler.html#web-crawler-feedback" title="Web crawler feedback">send us your feedback</a>.</p>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="crawl-web-content-re-crawl"></a>Re-crawl web content<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h3>
</div></div></div>
<p>For each engine, repeat the <em>manage-monitor-troubleshoot</em> cycle until the web crawler is discovering and indexing your documents as expected.</p>
<p>From there, you move into the next cycle: update your web content, re-crawl your web content, (repeat).</p>
<p>At this point, you may want to move beyond manual crawls and <a class="xref" href="crawl-web-content.html#crawl-web-content-schedule-crawls" title="Schedule crawls">schedule crawls</a> instead.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="crawl-web-content-schedule-crawls"></a>Schedule crawls<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/crawl-web-content.asciidoc">edit</a></h3>
</div></div></div>
<p>You may want to trigger crawls programatically.
Use this technique to crawl in response to an event, such as pushing updated web content.
Or, crawl according to a schedule.</p>
<p>To trigger crawls programatically, refer to the following API reference:</p>
<p><a class="xref" href="web-crawler-api-reference.html#web-crawler-apis-post-crawler-crawl-requests" title="Create a new crawl request">Create a new crawl request</a></p>
<p>Schedule your API calls using a job scheduler, like <a href="https://en.wikipedia.org/wiki/Cron" class="ulink" target="_blank" rel="noopener">cron</a>.
Or write your own application code to manage crawls.</p>
</div>

</div>
<div class="navfooter">
<span class="prev">
<a href="analytics-tags-guide.html">« Analytics Tags Guide</a>
</span>
<span class="next">
<a href="curations-guide.html">Curations Guide »</a>
</span>
</div>
</div>
</body>
</html>
