<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Web crawler (beta) reference | Elastic App Search Documentation [7.11] | Elastic</title>
<link rel="home" href="index.html" title="Elastic App Search Documentation [7.11]"/>
<link rel="up" href="guides.html" title="Guides"/>
<link rel="prev" href="web-crawler-faq.html" title="Web crawler (beta) FAQ"/>
<link rel="next" href="api-reference.html" title="API Reference"/>
<meta name="DC.type" content="Learn/Docs/App Search/Guide/7.11"/>
<meta name="DC.subject" content="App Search"/>
<meta name="DC.identifier" content="7.11"/>
</head>
<body>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="index.html">Elastic App Search Documentation [7.11]</a></span>
»
<span class="breadcrumb-link"><a href="guides.html">Guides</a></span>
»
<span class="breadcrumb-node">Web crawler (beta) reference</span>
</div>
<div class="navheader">
<span class="prev">
<a href="web-crawler-faq.html">« Web crawler (beta) FAQ</a>
</span>
<span class="next">
<a href="api-reference.html">API Reference »</a>
</span>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="web-crawler-reference"></a>Web crawler (beta) reference<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h2>
</div></div></div>
<div class="caution admon">
<div class="icon"></div>
<div class="admon_content">
<p>The Elastic Enterprise Search web crawler is a <span class="strong strong"><strong>beta</strong></span> feature.
Beta features are subject to change and are not covered by the support SLA of general release (GA) features.
Elastic plans to promote this feature to GA in a future release.</p>
</div>
</div>
<p>See <a class="xref" href="web-crawler.html" title="Web crawler (beta)">Web crawler (beta)</a> for a guided introduction to the Elastic Enterprise Search web crawler.</p>
<p>Refer to the following sections for terms, definitions, tables, and other detailed information.</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl" title="Crawl">Crawl</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-content-discovery" title="Content discovery">Content discovery</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-content-extraction-and-indexing" title="Content extraction and indexing">Content extraction and indexing</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-content-deletion" title="Content deletion">Content deletion</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-http-response-status-codes" title="HTTP response status codes">HTTP response status codes</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-domain" title="Domain">Domain</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-entry-point" title="Entry point">Entry point</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl-rule" title="Crawl rule">Crawl rule</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-canonical-url-link-tag" title="Canonical URL link tag">Canonical URL link tag</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-robots-meta-tags" title="Robots meta tags">Robots meta tags</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-nofollow-link" title="Nofollow link">Nofollow link</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl-status" title="Crawl status">Crawl status</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-web-crawler-schema" title="Web crawler schema">Web crawler schema</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-web-crawler-events-logs" title="Web crawler events logs">Web crawler events logs</a>
</li>
<li class="listitem">
<a class="xref" href="web-crawler-reference.html#web-crawler-reference-web-crawler-configuration-settings" title="Web crawler configuration settings">Web crawler configuration settings</a>
</li>
</ul>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-crawl"></a>Crawl<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>A <em>crawl</em> is a process, associated with an engine, by which the web crawler <a class="xref" href="web-crawler-reference.html#web-crawler-reference-content-discovery" title="Content discovery">discovers web content</a>, and <a class="xref" href="web-crawler-reference.html#web-crawler-reference-content-extraction-and-indexing" title="Content extraction and indexing">extracts and indexes that content</a> into the engine as search documents.</p>
<p>During a crawl, the web crawler stays within user-defined <a class="xref" href="web-crawler-reference.html#web-crawler-reference-domain" title="Domain">domains</a>, starting from specific <a class="xref" href="web-crawler-reference.html#web-crawler-reference-entry-point" title="Entry point">entry points</a>, and it discovers additional content according to <a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl-rule" title="Crawl rule">crawl rules</a>.
Operators and administrators <a class="xref" href="crawl-web-content.html#crawl-web-content-manage-crawl" title="Manage crawl">manage crawls</a> through the App Search dashboard.</p>
<p>Each crawl also respects various instructions embedded within the web content, such as <a class="xref" href="web-crawler-reference.html#web-crawler-reference-canonical-url-link-tag" title="Canonical URL link tag">canonical link tags</a>, <a class="xref" href="web-crawler-reference.html#web-crawler-reference-robots-meta-tags" title="Robots meta tags">robots meta tags</a>, and <a class="xref" href="web-crawler-reference.html#web-crawler-reference-nofollow-link" title="Nofollow link">nofollow links</a>.</p>
<p>Operators can <a class="xref" href="crawl-web-content.html#crawl-web-content-monitor-crawl" title="Monitor crawl">monitor crawls</a> using the ID and status of each crawl, and using the web crawler events logs.</p>
<p>Each engine may run one active crawl at a time, but multiple engines can run crawls concurrently.</p>
<p>Each crawl is stateless.
Crawl state is not persisted, and crawls cannot be resumed.</p>
<p>Each re-crawl is a full crawl.
Partial crawls are not supported at this time.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-content-discovery"></a>Content discovery<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>To begin each crawl, the web crawler populates a <em>crawl queue</em> with the <a class="xref" href="web-crawler-reference.html#web-crawler-reference-entry-point" title="Entry point">entry points</a> for the crawl.
It then begins fetching and parsing each page in the queue.
The crawler handles each page according to its <a class="xref" href="web-crawler-reference.html#web-crawler-reference-http-response-status-codes" title="HTTP response status codes">HTTP response status code</a>.</p>
<p>As the web crawler discovers additional URLs, it uses the <a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl-rule" title="Crawl rule">crawl rules</a> from the crawl configuration and instructions embedded within the content to determine which of those URLs it is allowed to crawl.
The crawler adds the allowed URLs to the crawl queue.</p>
<p>If a page is not linked from other pages, the web crawler will not discover it.
Refer to the web of pages in the following image.
The crawler will not discover the page outlined in red, unless an admin adds its URL as an entry point for the domain:</p>
<div class="imageblock">
<div class="content">
<img src="images/app-search/web-of-content.png" alt="web of content">
</div>
</div>
<p>The web crawler continues fetching and adding to the crawl queue until the queue is empty, the crawler hits a resource limit, or the crawl fails unexpectedly.</p>
<p>The crawler logs detailed events while it crawls, which allow monitoring and auditing of the content discovery process.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-content-extraction-and-indexing"></a>Content extraction and indexing<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p><em>Content extraction</em> is the process where the web crawler transforms an HTML document into a search document.
And <em>indexing</em> is the process where the crawler puts the search document into an App Search engine for searching.</p>
<p>During content extraction, the web crawler checks for duplicate content.
The crawler indexes duplicate web documents as a single search document.</p>
<p>The web crawler extracts each web document into a predefined <a class="xref" href="web-crawler-reference.html#web-crawler-reference-web-crawler-schema" title="Web crawler schema">schema</a> before indexing it as a search document.
At this time, the web crawler does not provide configuration or control over what content is extracted or how it is extracted.</p>
<p>During a crawl, the web crawler uses <a class="xref" href="web-crawler-reference.html#web-crawler-reference-http-response-status-codes" title="HTTP response status codes">HTTP response status codes</a> and <a class="xref" href="web-crawler-reference.html#web-crawler-reference-robots-meta-tags" title="Robots meta tags">robots meta tags</a> to determine which documents it is allowed to index.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-content-deletion"></a>Content deletion<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>The web crawler must also delete documents from an engine to keep its documents in sync with the corresponding web content.</p>
<p>During a crawl, the web crawler uses <a class="xref" href="web-crawler-reference.html#web-crawler-reference-http-response-status-codes" title="HTTP response status codes">HTTP response status codes</a> to determine which documents to delete.
However, this process cannot delete stale documents in the engine that are no longer linked to on the web.</p>
<p>Therefore, at the conclusion of each crawl, the web crawler begins an additional "purge" crawl, which fetches URLs that exist in the engine, but have not been seen on the web during recent crawls.
The crawler deletes from the engine all documents that respond with <code class="literal">4xx</code> and <code class="literal">3xx</code> responses during the phase.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-http-response-status-codes"></a>HTTP response status codes<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>The web crawler handles HTTP response status codes as follows:</p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Code</th>
<th align="left" valign="top">Description</th>
<th align="left" valign="top">Web crawler behavior</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p><code class="literal">2xx</code></p></td>
<td align="left" valign="top"><p>Success</p></td>
<td align="left" valign="top"><p>The web crawler extracts and de-duplicates the page&#8217;s content into a search document.
Then it indexes the document into the engine, replacing an existing document with the same content if present.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">3xx</code></p></td>
<td align="left" valign="top"><p>Redirection</p></td>
<td align="left" valign="top"><p>The web crawler follows all redirects recursively until it receives a <code class="literal">2xx</code>, <code class="literal">4xx</code>, or <code class="literal">5xx</code> response status code.
The crawler then handles that code as indicated in this table.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">4xx</code></p></td>
<td align="left" valign="top"><p>Client (permanent) error</p></td>
<td align="left" valign="top"><p>The web crawler assumes this error is permanent.
It therefore does not index the document.
Furthermore, if the document is present in the engine, the web crawler deletes the document.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">5xx</code></p></td>
<td align="left" valign="top"><p>Server (impermanent) error</p></td>
<td align="left" valign="top"><p>The web crawler optimistically assumes this error will resolve in the future.
Therefore, if this content is already present in the engine, the crawler retains the content but updates the timestamp and state to indicate it&#8217;s been re-crawled.</p></td>
</tr>
</tbody>
</table>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-domain"></a>Domain<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>A <em>domain</em> is a website or other internet realm that is the target of a <a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl" title="Crawl">crawl</a>.
Each domain belongs to a crawl, and each crawl has one or more domains.</p>
<p>See <a class="xref" href="crawl-web-content.html#crawl-web-content-manage-domains" title="Manage domains">Manage domains</a> to manage domains for a crawl.</p>
<p>A crawl always stays <em>within</em> its domains when <a class="xref" href="web-crawler-reference.html#web-crawler-reference-content-discovery" title="Content discovery">discovering content</a>.
It cannot discover and index content outside of its domains.</p>
<p>Each domain has a <em>domain URL</em> that identifies the domain using a protocol and hostname.
The domain URL must not include a path.</p>
<p>Each unique combination of protocol and hostname is a separate domain.
Each of the following is its own domain:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">http://example.com</code>
</li>
<li class="listitem">
<code class="literal">https://example.com</code>
</li>
<li class="listitem">
<code class="literal">http://www.example.com</code>
</li>
<li class="listitem">
<code class="literal">https://www.example.com</code>
</li>
<li class="listitem">
<code class="literal">http://shop.example.com</code>
</li>
<li class="listitem">
<code class="literal">https://shop.example.com</code>
</li>
</ul>
</div>
<p>Each domain has one or more <a class="xref" href="web-crawler-reference.html#web-crawler-reference-entry-point" title="Entry point">entry points</a>.</p>
<p>Each domain has one or more <a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl-rule" title="Crawl rule">crawl rules</a>.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-entry-point"></a>Entry point<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>An <em>entry point</em> is a path within a <a class="xref" href="web-crawler-reference.html#web-crawler-reference-domain" title="Domain">domain</a> that serves as a starting point for a crawl.
Each entry point belongs to a domain, and each domain has one or more entry points.</p>
<p>See <a class="xref" href="crawl-web-content.html#crawl-web-content-manage-entry-points" title="Manage entry points">Manage entry points</a> to manage entry points for a domain.</p>
<p>Use entry points to instruct the crawler to fetch URLs it would otherwise not discover.
For example, if you&#8217;d like to index a page that isn&#8217;t linked from other pages, add the page&#8217;s URL as an entry point.</p>
<p>Ensure the entry points you choose are also allowed by <a class="xref" href="web-crawler-reference.html#web-crawler-reference-crawl-rule" title="Crawl rule">crawl rules</a>.
The web crawler will not fetch entry points that are disallowed by crawl rules.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-crawl-rule"></a>Crawl rule<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>A <em>crawl rule</em> is a crawler instruction to <em>allow</em> or <em>disallow</em> specific paths within a <a class="xref" href="web-crawler-reference.html#web-crawler-reference-domain" title="Domain">domain</a>.
Each crawl rule belongs to a domain, and each domain has one or more crawl rules.</p>
<p>See <a class="xref" href="crawl-web-content.html#crawl-web-content-manage-crawl-rules" title="Manage crawl rules">Manage crawl rules</a> to manage crawl rules for a domain.</p>
<p>During <a class="xref" href="web-crawler-reference.html#web-crawler-reference-content-discovery" title="Content discovery">content discovery</a>, the web crawler discovers new URLs and must determine which it is allowed to follow.
Each URL has a <em>domain</em> (e.g. <code class="literal">https://example.com</code>) and a <em>path</em> (e.g. <code class="literal">/category/clothing</code> or <code class="literal">/c/Credit_Center</code>).</p>
<p>The web crawler looks up the crawl rules for the domain, and applies the path to the crawl rules to determine if the path is <em>allowed</em> or <em>disallowed</em>.
The crawler evaluates the crawl rules in order.
The <em>first</em> matching crawl rule determines the policy for the newly discovered URL.</p>
<p>Each crawl rule has a <em>path pattern</em>, a <em>rule</em>, and a <em>policy</em>.
To evaluate each rule, the web crawler compares a newly discovered <em>path</em> to the path pattern, using the logic represented by the rule, resulting in a policy.</p>
<p><span class="strong strong"><strong>Rules:</strong></span></p>
<p>The logic for each rule is as follows:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
Begins with
</span>
</dt>
<dd>
<p>
The <em>path pattern</em> is a literal string (no metacharacters).
</p>
<p>The rule matches when the <em>path pattern</em> matches the <span class="strong strong"><strong>beginning</strong></span> of the <em>path</em> (which always begins with <code class="literal">/</code>).</p>
<p>If using this rule, begin your <em>path pattern</em> with <code class="literal">/</code>.</p>
</dd>
<dt>
<span class="term">
Ends with
</span>
</dt>
<dd>
<p>
The <em>path pattern</em> is a literal string (no metacharacters).
</p>
<p>The rule matches when the <em>path pattern</em> matches the <span class="strong strong"><strong>end</strong></span> of the <em>path</em>.</p>
</dd>
<dt>
<span class="term">
Contains
</span>
</dt>
<dd>
<p>
The <em>path pattern</em> is a literal string (no metacharacters).
</p>
<p>The rule matches when the <em>path pattern</em> matches anywhere <span class="strong strong"><strong>within</strong></span> the <em>path</em>.</p>
</dd>
<dt>
<span class="term">
Regex
</span>
</dt>
<dd>
<p>
The <em>path pattern</em> is a regular expression compatible with the Ruby language regular expression engine.
In addition to literal characters, the path pattern may include
<a href="https://ruby-doc.org/core-2.5.1/Regexp.html#class-Regexp-label-Metacharacters+and+Escapes" class="ulink" target="_blank" rel="noopener">metacharacters</a>, <a href="https://ruby-doc.org/core-2.5.1/Regexp.html#class-Regexp-label-Character+Classes" class="ulink" target="_blank" rel="noopener">character classes</a>, and <a href="https://ruby-doc.org/core-2.5.1/Regexp.html#class-Regexp-label-Repetition" class="ulink" target="_blank" rel="noopener">repetitions</a>.
You can test Ruby regular expressions using <a href="https://rubular.com" class="ulink" target="_blank" rel="noopener">Rubular</a>.
</p>
<p>The rule matches when the <em>path pattern</em> matches the <span class="strong strong"><strong>beginning</strong></span> of the <em>path</em> (which always begins with <code class="literal">/</code>).</p>
<p>If using this rule, begin your <em>path pattern</em> with <code class="literal">\/</code> or a metacharacter or character class that matches <code class="literal">/</code>.</p>
</dd>
</dl>
</div>
<p><span class="strong strong"><strong>Comparison examples:</strong></span></p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
<col class="col_4"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">URL path</th>
<th align="left" valign="top">Rule</th>
<th align="left" valign="top">Path pattern</th>
<th align="left" valign="top">Match?</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p><code class="literal">/foo/bar</code></p></td>
<td align="left" valign="top"><p><em>Begins with</em></p></td>
<td align="left" valign="top"><p><code class="literal">/foo</code></p></td>
<td align="left" valign="top"><p>YES</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/bar/foo</code></p></td>
<td align="left" valign="top"><p><em>Begins with</em></p></td>
<td align="left" valign="top"><p><code class="literal">/foo</code></p></td>
<td align="left" valign="top"><p>NO</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/foo/bar</code></p></td>
<td align="left" valign="top"><p><em>Begins with</em></p></td>
<td align="left" valign="top"><p><code class="literal">foo</code></p></td>
<td align="left" valign="top"><p>NO</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/blog/posts/hello-world</code></p></td>
<td align="left" valign="top"><p><em>Ends</em></p></td>
<td align="left" valign="top"><p><code class="literal">world</code></p></td>
<td align="left" valign="top"><p>YES</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/blog/world-hello</code></p></td>
<td align="left" valign="top"><p><em>Ends</em></p></td>
<td align="left" valign="top"><p><code class="literal">world</code></p></td>
<td align="left" valign="top"><p>NO</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/fruits/bananas</code></p></td>
<td align="left" valign="top"><p><em>Contains</em></p></td>
<td align="left" valign="top"><p><code class="literal">banana</code></p></td>
<td align="left" valign="top"><p>YES</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/fruits/apples</code></p></td>
<td align="left" valign="top"><p><em>Contains</em></p></td>
<td align="left" valign="top"><p><code class="literal">banana</code></p></td>
<td align="left" valign="top"><p>NO</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/2020</code></p></td>
<td align="left" valign="top"><p><em>Regex</em></p></td>
<td align="left" valign="top"><p><code class="literal">\/[0-9]{3,5}</code></p></td>
<td align="left" valign="top"><p>YES</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/20</code></p></td>
<td align="left" valign="top"><p><em>Regex</em></p></td>
<td align="left" valign="top"><p><code class="literal">\/[0-9]{3,5}</code></p></td>
<td align="left" valign="top"><p>NO</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/2020</code></p></td>
<td align="left" valign="top"><p><em>Regex</em></p></td>
<td align="left" valign="top"><p><code class="literal">[0-9]{3,5}</code></p></td>
<td align="left" valign="top"><p>NO</p></td>
</tr>
</tbody>
</table>
</div>
<p><span class="strong strong"><strong>Order:</strong></span></p>
<p>The <em>first</em> crawl rule to match determines the policy for the URL.
Therefore, the order of the crawl rules is significant.</p>
<p>The following table demonstrates how crawl rule order affects the resulting policy for a path:</p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Path</th>
<th align="left" valign="top">Crawl rules</th>
<th align="left" valign="top">Resulting policy</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p><code class="literal">/blog/2021/foo-bar-baz</code></p></td>
<td align="left" valign="top"><p>1. <code class="literal">Disallow</code> if <code class="literal">Begins with</code> <code class="literal">/blog</code></p>
<p>2. <code class="literal">Allow</code> if <code class="literal">Regex</code> <code class="literal">.*</code></p></td>
<td align="left" valign="top"><p>DISALLOW</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/blog/2021/foo-bar-baz</code></p></td>
<td align="left" valign="top"><p>1. <code class="literal">Allow</code> if <code class="literal">Regex</code> <code class="literal">.*</code></p>
<p>2. <code class="literal">Disallow</code> if <code class="literal">Begins with</code> <code class="literal">/blog</code></p></td>
<td align="left" valign="top"><p>ALLOW</p></td>
</tr>
</tbody>
</table>
</div>
<p><span class="strong strong"><strong>Restricting paths:</strong></span></p>
<p>The domain dashboard adds a default crawl rule to each domain: <code class="literal">Allow</code> if <code class="literal">Regex</code> <code class="literal">.*</code>.
You cannot delete or re-order this rule through the dashboard.</p>
<p>This rule is permissive, allowing all paths within the domain.
To restrict paths, use either of the following techniques:</p>
<p>Add rules that disallow specific paths (e.g. disallow the blog):</p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Policy</th>
<th align="left" valign="top">Rule</th>
<th align="left" valign="top">Path pattern</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p><code class="literal">Disallow</code></p></td>
<td align="left" valign="top"><p><code class="literal">Begins with</code></p></td>
<td align="left" valign="top"><p><code class="literal">/blog</code></p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">Allow</code></p></td>
<td align="left" valign="top"><p><code class="literal">Regex</code></p></td>
<td align="left" valign="top"><p><code class="literal">.*</code></p></td>
</tr>
</tbody>
</table>
</div>
<p>Or, add rules that allow specific paths and disallow all others (e.g. allow <em>only</em> the blog):</p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Policy</th>
<th align="left" valign="top">Rule</th>
<th align="left" valign="top">Path pattern</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p><code class="literal">Allow</code></p></td>
<td align="left" valign="top"><p><code class="literal">Begins with</code></p></td>
<td align="left" valign="top"><p><code class="literal">/blog</code></p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">Disallow</code></p></td>
<td align="left" valign="top"><p><code class="literal">Regex</code></p></td>
<td align="left" valign="top"><p><code class="literal">.*</code></p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">Allow</code></p></td>
<td align="left" valign="top"><p><code class="literal">Regex</code></p></td>
<td align="left" valign="top"><p><code class="literal">.*</code></p></td>
</tr>
</tbody>
</table>
</div>
<p>When you restrict a crawl to specific paths, be sure to add <a class="xref" href="web-crawler-reference.html#web-crawler-reference-entry-point" title="Entry point">entry points</a> that allow the crawler to discover those paths.
For example, if your crawl rules restrict the crawler to <code class="literal">/blog</code>, add <code class="literal">/blog</code> as an entry point.</p>
<p>If you leave only the default entry point of <code class="literal">/</code>, the crawl will end immediately, since <code class="literal">/</code> is disallowed.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-canonical-url-link-tag"></a>Canonical URL link tag<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>A <em>canonical URL link tag</em> is an HTML element you can embed within pages that contain duplicate content.
The canonical URL link tag indicates the URL of the canonical page with the same content.</p>
<p>Template:</p>
<div class="pre_wrapper lang-html">
<pre class="programlisting prettyprint lang-html">&lt;link rel="canonical" href="{CANONICAL_URL}"&gt;</pre>
</div>
<p>Example:</p>
<div class="pre_wrapper lang-html">
<pre class="programlisting prettyprint lang-html">&lt;link rel="canonical" href="https://example.com/categories/dresses/starlet-red-medium"&gt;</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-robots-meta-tags"></a>Robots meta tags<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p><em>Robots meta tags</em> are HTML elements you can embed within pages to prevent the crawler from following links or indexing content.</p>
<p>Template:</p>
<div class="pre_wrapper lang-html">
<pre class="programlisting prettyprint lang-html">&lt;meta name="robots" content="{DIRECTIVES}"&gt;</pre>
</div>
<p>Supported directives:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">noindex</code>
</span>
</dt>
<dd>
The web crawler will not index the page.
</dd>
<dt>
<span class="term">
<code class="literal">nofollow</code>
</span>
</dt>
<dd>
<p>
The web crawler will not follow links from the page (i.e. will not add links to the crawl queue).
The web crawler logs a <code class="literal">url_discover_denied</code> event for each link.
</p>
<p>The directive does not prevent the web crawler from indexing the page.</p>
</dd>
</dl>
</div>
<p>Examples:</p>
<div class="pre_wrapper lang-html">
<pre class="programlisting prettyprint lang-html">&lt;meta name="robots" content="noindex"&gt;
&lt;meta name="robots" content="nofollow"&gt;
&lt;meta name="robots" content="noindex, nofollow"&gt;</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-nofollow-link"></a>Nofollow link<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p><em>Nofollow links</em> are HTML links that instruct the crawler to not follow the URL.</p>
<p>The web crawler will not follow links that include <code class="literal">rel="nofollow"</code> (i.e. will not add links to the crawl queue).
The web crawler logs a <code class="literal">url_discover_denied</code> event for each link.</p>
<p>The link does not prevent the web crawler from indexing the page in which it appears.</p>
<p>Template:</p>
<div class="pre_wrapper lang-html">
<pre class="programlisting prettyprint lang-html">&lt;a rel="nofollow" href="{LINK_URL}"&gt;{LINK_TEXT}&lt;/a&gt;</pre>
</div>
<p>Example:</p>
<div class="pre_wrapper lang-html">
<pre class="programlisting prettyprint lang-html">&lt;a rel="nofollow" href="/admin/categories"&gt;Edit this category&lt;/a&gt;</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-crawl-status"></a>Crawl status<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>Each crawl has a <em>status</em>, which quickly communicates its state.</p>
<p>See <a class="xref" href="crawl-web-content.html#crawl-web-content-view-crawl-status" title="View crawl status">View crawl status</a> to view the status for a crawl.</p>
<p>All crawl statuses:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
Pending
</span>
</dt>
<dd>
The crawl is enqueued and will start after resources are available.
</dd>
<dt>
<span class="term">
Starting
</span>
</dt>
<dd>
The crawl is starting.
You may see this status briefly, while a crawl moves from <em>pending</em> to <em>running</em>.
</dd>
<dt>
<span class="term">
Running
</span>
</dt>
<dd>
The crawl is running.
</dd>
<dt>
<span class="term">
Success
</span>
</dt>
<dd>
The crawl completed without error.
The web crawler may stop a crawl due to hitting resource limits.
Such a crawl will report its status as <em>success</em>, as long as it completes without error.
</dd>
<dt>
<span class="term">
Canceling
</span>
</dt>
<dd>
The crawl is canceling.
You may see this status briefly, after choosing to cancel a crawl.
</dd>
<dt>
<span class="term">
Canceled
</span>
</dt>
<dd>
The crawl was intentionally canceled and therefore did not complete.
</dd>
<dt>
<span class="term">
Failed
</span>
</dt>
<dd>
The crawl ended unexpectedly.
<a class="xref" href="view-web-crawler-events-logs.html" title="View web crawler events logs">View the web crawler events logs</a> for a message providing an explanation.
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-web-crawler-schema"></a>Web crawler schema<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>The web crawler indexes search documents using the following schema:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">additional_urls</code>
</span>
</dt>
<dd>
The URLs of additional pages with the same content.
</dd>
<dt>
<span class="term">
<code class="literal">body_content</code>
</span>
</dt>
<dd>
The content of the page&#8217;s <code class="literal">&lt;body&gt;</code> tag with all HTML tags removed.
</dd>
<dt>
<span class="term">
<code class="literal">content_hash</code>
</span>
</dt>
<dd>
A "fingerprint" to uniquely identify this content.
Used for de-duplication and re-crawling.
</dd>
<dt>
<span class="term">
<code class="literal">domains</code>
</span>
</dt>
<dd>
The domains in which this content appears.
</dd>
<dt>
<span class="term">
<code class="literal">id</code>
</span>
</dt>
<dd>
The unique identifier for the page.
</dd>
<dt>
<span class="term">
<code class="literal">links</code>
</span>
</dt>
<dd>
Links found on the page.
</dd>
<dt>
<span class="term">
<code class="literal">meta_description</code>
</span>
</dt>
<dd>
The page&#8217;s description, taken from the <code class="literal">&lt;meta name="description"&gt;</code> tag.
</dd>
<dt>
<span class="term">
<code class="literal">meta_keywords</code>
</span>
</dt>
<dd>
The page&#8217;s keywords, taken from the <code class="literal">&lt;meta name="keywords"&gt;</code> tag.
</dd>
<dt>
<span class="term">
<code class="literal">title</code>
</span>
</dt>
<dd>
The title of the page, taken from the <code class="literal">&lt;title&gt;</code> tag.
</dd>
<dt>
<span class="term">
<code class="literal">url</code>
</span>
</dt>
<dd>
The URL of the page.
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-web-crawler-events-logs"></a>Web crawler events logs<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>See <a class="xref" href="view-web-crawler-events-logs.html" title="View web crawler events logs">View web crawler events logs</a>.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="web-crawler-reference-web-crawler-configuration-settings"></a>Web crawler configuration settings<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/7.11/app-search-docs/guides/web-crawler-reference.asciidoc">edit</a></h3>
</div></div></div>
<p>Operators can configure several web crawler settings.</p>
<p>See <a href="/guide/en/enterprise-search/7.11/configuration.html" class="ulink" target="_blank" rel="noopener">Configuration</a> in the Enterprise Search documentation for a complete list of Enterprise Search configuration settings.</p>
<p>Settings beginning with <code class="literal">crawler.</code> are specific to the web crawler.</p>
<p>These settings include:</p>
<div class="pre_wrapper lang-yaml">
<pre class="programlisting prettyprint lang-yaml">crawler.user_agent
crawler.workers.pool_size.limit
crawler.crawl.max_duration.limit
crawler.crawl.max_crawl_depth.limit
crawler.crawl.max_url_length.limit
crawler.crawl.max_url_segments.limit
crawler.crawl.max_url_params.limit
crawler.crawl.max_unique_url_count.limit</pre>
</div>
<p>Refer to the Enterprise Search documentation for an up-to-date list of all configuration settings.</p>
</div>

</div>
<div class="navfooter">
<span class="prev">
<a href="web-crawler-faq.html">« Web crawler (beta) FAQ</a>
</span>
<span class="next">
<a href="api-reference.html">API Reference »</a>
</span>
</div>
</div>
</body>
</html>
