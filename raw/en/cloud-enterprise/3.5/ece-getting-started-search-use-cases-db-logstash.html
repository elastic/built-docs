<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Ingest data from a relational database into Elastic Cloud Enterprise | Elastic Cloud Enterprise Reference [3.5] | Elastic</title>
<meta class="elastic" name="content" content="Ingest data from a relational database into Elastic Cloud Enterprise | Elastic Cloud Enterprise Reference [3.5]">

<link rel="home" href="index.html" title="Elastic Cloud Enterprise Reference [3.5]"/>
<link rel="up" href="ece-ingest-guides.html" title="Ingesting data from your application"/>
<link rel="prev" href="ece-getting-started-search-use-cases-beats-logstash.html" title="Ingest data from Beats to Elastic Cloud Enterprise with Logstash as a proxy"/>
<link rel="next" href="ece-getting-started-search-use-cases-python-logs.html" title="Ingest logs from a Python application using Filebeat"/>
<meta class="elastic" name="product_version" content="3.5"/>
<meta class="elastic" name="product_name" content="ECE"/>
<meta class="elastic" name="website_area" content="documentation"/>
<meta name="DC.type" content="Learn/Docs/CloudEnterprise/Reference/3.5"/>
<meta name="DC.subject" content="ECE"/>
<meta name="DC.identifier" content="3.5"/>
</head>
<body>
<div class="page_header">
A newer version is available. For the latest information, see the
<a href="../current/index.html">current release documentation</a>.
</div>
<div class="navheader">
<span class="prev">
<a href="ece-getting-started-search-use-cases-beats-logstash.html">« Ingest data from Beats to Elastic Cloud Enterprise with Logstash as a proxy</a>
</span>
<span class="next">
<a href="ece-getting-started-search-use-cases-python-logs.html">Ingest logs from a Python application using Filebeat »</a>
</span>
</div>
<div class="book" lang="en">
<div class="titlepage">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="index.html">Elastic Cloud Enterprise Reference [3.5]</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="ece-stack-getting-started.html">Working with deployments</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="ece-cloud-ingest-data.html">Adding data to Elasticsearch</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="ece-ingest-guides.html">Ingesting data from your application</a></span>
</div>
<div>
<div><h1 class="title"><a id="id-1"></a>Ingest data from a relational database into Elastic Cloud Enterprise</h1><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/cloud/edit/ms-81/docs/shared/ec-ce-getting-started-use-cases-db-logstash.asciidoc">edit</a></div>
</div>
<!--EXTRA-->
</div>
<div id="content">
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="ece-getting-started-search-use-cases-db-logstash"></a>Ingest data from a relational database into Elastic Cloud Enterprise<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/cloud/edit/ms-81/docs/shared/ec-ce-getting-started-use-cases-db-logstash.asciidoc">edit</a></h3>
</div></div></div>
<p>This guide explains how to ingest data from a relational database into Elastic Cloud Enterprise through <a href="/guide/en/logstash/8.15/introduction.html" class="ulink" target="_top">Logstash</a>, using the Logstash <a href="/guide/en/logstash/8.15/plugins-inputs-jdbc.html" class="ulink" target="_top">JDBC input plugin</a>. It demonstrates how Logstash can be used to efficiently copy records and to receive updates from a relational database, and then send them into Elasticsearch in an Elastic Cloud Enterprise deployment.</p>
<p>The code and methods presented here have been tested with MySQL. They should work with other relational databases.</p>
<p>The Logstash Java Database Connectivity (JDBC) input plugin enables you to pull in data from many popular relational databases including MySQL and Postgres. Conceptually, the JDBC input plugin runs a loop that periodically polls the relational database for records that were inserted or modified since the last iteration of this loop.</p>
<p>This document presents:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
<a class="xref" href="ece-getting-started-search-use-cases-db-logstash.html#ece-db-logstash-prerequisites" title="Prerequisites">Prerequisites</a>
</li>
<li class="listitem">
<a class="xref" href="ece-getting-started-search-use-cases-db-logstash.html#ece-db-logstash-deployment" title="Create a deployment">Create a deployment</a>
</li>
<li class="listitem">
<a class="xref" href="ece-getting-started-search-use-cases-db-logstash.html#ece-db-logstash-connect-securely" title="Connect securely">Connect securely</a>
</li>
<li class="listitem">
<a class="xref" href="ece-getting-started-search-use-cases-db-logstash.html#ece-db-logstash-driver" title="Get the MySQL JDBC driver">Get the MySQL JDBC driver</a>
</li>
<li class="listitem">
<a class="xref" href="ece-getting-started-search-use-cases-db-logstash.html#ece-db-logstash-database" title="Prepare a source MySQL database">Prepare a source MySQL database</a>
</li>
<li class="listitem">
<a class="xref" href="ece-getting-started-search-use-cases-db-logstash.html#ece-db-logstash-database-structure" title="Consider the database structure and design">Consider the database structure and design</a>
</li>
<li class="listitem">
<a class="xref" href="ece-getting-started-search-use-cases-db-logstash.html#ece-db-logstash-pipeline" title="Configure a Logstash pipeline with the JDBC input plugin">Configure a Logstash pipeline with the JDBC input plugin</a>
</li>
<li class="listitem">
<a class="xref" href="ece-getting-started-search-use-cases-db-logstash.html#ece-db-logstash-output" title="Output to Elasticsearch">Output to Elasticsearch</a>
</li>
</ol>
</div>
<p><em>Time required: 2 hours</em></p>
<h5><a id="ece-db-logstash-prerequisites"></a>Prerequisites<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/cloud/edit/ms-81/docs/shared/ec-ce-getting-started-use-cases-db-logstash.asciidoc">edit</a></h5>
<p>For this tutorial you need a source MySQL instance for Logstash to read from. A free version of MySQL is available from the <a href="https://dev.mysql.com/downloads/mysql/" class="ulink" target="_top">MySQL Community Server section</a> of the MySQL Community Downloads site.</p>
<h5><a id="ece-db-logstash-deployment"></a>Create a deployment<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/cloud/edit/ms-81/docs/shared/ec-ce-getting-started-use-cases-db-logstash.asciidoc">edit</a></h5>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Log into the Elastic Cloud Enterprise admin console.
</li>
<li class="listitem">
Select <span class="strong strong"><strong>Create deployment</strong></span>.
</li>
<li class="listitem">
Give your deployment a name. You can leave all other settings at their default values.
</li>
<li class="listitem">
Select <span class="strong strong"><strong>Create deployment</strong></span> and save your Elastic deployment credentials. You will need these credentials later on.
</li>
</ol>
</div>
<h5><a id="ece-db-logstash-connect-securely"></a>Connect securely<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/cloud/edit/ms-81/docs/shared/ec-ce-getting-started-use-cases-db-logstash.asciidoc">edit</a></h5>
<p>When connecting to Elastic Cloud Enterprise you can use a Cloud ID to specify the connection details. Find your Cloud ID by going to the Kibana main menu and selecting Management &gt; Integrations, and then selecting View deployment details.</p>
<p>To connect to, stream data to, and issue queries with Elastic Cloud Enterprise, you need to think about authentication. Two authentication mechanisms are supported, <em>API key</em> and <em>basic authentication</em>. Here, to get you started quickly, we’ll show you how to use basic authentication, but you can also generate API keys as shown later on. API keys are safer and preferred for production environments.</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
<a href="/downloads/logstash" class="ulink" target="_top">Download</a> and unpack Logstash version 7.12.0 on the local machine that hosts MySQL or another machine granted access to the MySQL machine.
</li>
</ol>
</div>
<h5><a id="ece-db-logstash-driver"></a>Get the MySQL JDBC driver<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/cloud/edit/ms-81/docs/shared/ec-ce-getting-started-use-cases-db-logstash.asciidoc">edit</a></h5>
<p>The Logstash JDBC input plugin does not include any database connection drivers. You need a JDBC driver for your relational database for the steps in the later section <a class="xref" href="ece-getting-started-search-use-cases-db-logstash.html#ece-db-logstash-pipeline" title="Configure a Logstash pipeline with the JDBC input plugin">Configure a Logstash pipeline with the JDBC input plugin</a>.</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Download and unpack the JDBC driver for MySQL from the <a href="https://dev.mysql.com/downloads/connector/j/" class="ulink" target="_top">Connector/J section</a> of the MySQL Community Downloads site.
</li>
<li class="listitem">
Make a note of the driver’s location as it’s needed in the steps that follow.
</li>
</ol>
</div>
<h5><a id="ece-db-logstash-database"></a>Prepare a source MySQL database<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/cloud/edit/ms-81/docs/shared/ec-ce-getting-started-use-cases-db-logstash.asciidoc">edit</a></h5>
<p>Let’s look at a simple database from which you&#8217;ll import data and send it to Elastic Cloud Enterprise. This example uses a MySQL database with timestamped records. The timestamps enable you to determine easily what’s changed in the database since the most recent data transfer to Elastic Cloud Enterprise.</p>
<h6><a id="ece-db-logstash-database-structure"></a>Consider the database structure and design<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/cloud/edit/ms-81/docs/shared/ec-ce-getting-started-use-cases-db-logstash.asciidoc">edit</a></h6>
<p>For this example, let’s create a new database <em>es_db</em> with table <em>es_table</em>, as the source of our Elasticsearch data.</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
<p>Run the following SQL statement to generate a new MySQL database with a three column table:</p>
<div class="pre_wrapper lang-txt">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-txt">CREATE DATABASE es_db;
USE es_db;
DROP TABLE IF EXISTS es_table;
CREATE TABLE es_table (
  id BIGINT(20) UNSIGNED NOT NULL,
  PRIMARY KEY (id),
  UNIQUE KEY unique_id (id),
  client_name VARCHAR(32) NOT NULL,
  modification_time TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
);</pre>
</div>
<p>Let’s explore the key concepts in this SQL snippet:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
es_table
</span>
</dt>
<dd>
The name of the table that stores the data.
</dd>
<dt>
<span class="term">
id
</span>
</dt>
<dd>
The unique identifier for records. <em>id</em> is defined as both a PRIMARY KEY and UNIQUE KEY to guarantee that each <em>id</em> appears only once in the current table. This is translated to <em>_id</em> for updating or inserting the document into Elasticsearch.
</dd>
<dt>
<span class="term">
client_name
</span>
</dt>
<dd>
The data that will ultimately be ingested into Elasticsearch. For simplicity, this example includes only a single data field.
</dd>
<dt>
<span class="term">
modification_time
</span>
</dt>
<dd>
The timestamp of when the record was inserted or last updated. Further in, you can use this timestamp to determine what has changed since the last data transfer into Elasticsearch.
</dd>
</dl>
</div>
</li>
<li class="listitem">
<p>Consider how to handle deletions and how to notify Elasticsearch about them. Often, deleting a record results in its immediate removal from the MySQL database. There’s no record of that deletion. The change isn’t detected by Logstash, so that record remains in Elasticsearch.</p>
<p>There are two possible ways to address this:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
You can use "soft deletes" in your source database. Essentially, a record is first marked for deletion through a boolean flag. Other programs that are currently using your source database would have to filter out "soft deletes" in their queries. The "soft deletes" are sent over to Elasticsearch, where they can be processed. After that, your source database and Elasticsearch must both remove these "soft deletes."
</li>
<li class="listitem">
You can periodically clear the Elasticsearch indices that are based off of the database, and then refresh Elasticsearch with a fresh ingest of the contents of the database.
</li>
</ul>
</div>
</li>
<li class="listitem">
<p>Log in to your MySQL server and add three records to your new database:</p>
<div class="pre_wrapper lang-txt">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-txt">use es_db
INSERT INTO es_table (id, client_name)
VALUES (1,"Targaryen"),
(2,"Lannister"),
(3,"Stark");</pre>
</div>
</li>
<li class="listitem">
<p>Verify your data with a SQL statement:</p>
<div class="pre_wrapper lang-txt">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-txt">select * from es_table;</pre>
</div>
<p>The output should look similar to the following:</p>
<div class="pre_wrapper lang-txt">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-txt">+----+-------------+---------------------+
| id | client_name | modification_time   |
+----+-------------+---------------------+
|  1 | Targaryen   | 2021-04-21 12:17:16 |
|  2 | Lannister   | 2021-04-21 12:17:16 |
|  3 | Stark       | 2021-04-21 12:17:16 |
+----+-------------+---------------------+</pre>
</div>
<p>Now, let’s go back to Logstash and configure it to ingest this data.</p>
</li>
</ol>
</div>
<h5><a id="ece-db-logstash-pipeline"></a>Configure a Logstash pipeline with the JDBC input plugin<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/cloud/edit/ms-81/docs/shared/ec-ce-getting-started-use-cases-db-logstash.asciidoc">edit</a></h5>
<p>Let’s set up a sample Logstash input pipeline to ingest data from your new JDBC Plugin and MySQL database. Beyond MySQL, you can input data from any database that supports JDBC.</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
In <code class="literal">&lt;localpath&gt;/logstash-7.12.0/</code>, create a new text file named <code class="literal">jdbc.conf</code>.
</li>
<li class="listitem">
<p>Copy and paste the following code into this new text file. This code creates a Logstash pipeline through a JDBC plugin.</p>
<div class="pre_wrapper lang-txt">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-txt">input {
  jdbc {
    jdbc_driver_library =&gt; "&lt;driverpath&gt;/mysql-connector-java-&lt;versionNumber&gt;.jar" <a id="CO19-1"></a><i class="conum" data-value="1"></i>
    jdbc_driver_class =&gt; "com.mysql.jdbc.Driver"
    jdbc_connection_string =&gt; "jdbc:mysql://&lt;MySQL host&gt;:3306/es_db" <a id="CO19-2"></a><i class="conum" data-value="2"></i>
    jdbc_user =&gt; "&lt;myusername&gt;" <a id="CO19-3"></a><i class="conum" data-value="3"></i>
    jdbc_password =&gt; "&lt;mypassword&gt;" <a id="CO19-4"></a><i class="conum" data-value="3"></i>
    jdbc_paging_enabled =&gt; true
    tracking_column =&gt; "unix_ts_in_secs"
    use_column_value =&gt; true
    tracking_column_type =&gt; "numeric"
    schedule =&gt; "*/5 * * * * *"
    statement =&gt; "SELECT *, UNIX_TIMESTAMP(modification_time) AS unix_ts_in_secs FROM es_table WHERE (UNIX_TIMESTAMP(modification_time) &gt; :sql_last_value AND modification_time &lt; NOW()) ORDER BY modification_time ASC"
  }
}
filter {
  mutate {
    copy =&gt; { "id" =&gt; "[@metadata][_id]"}
    remove_field =&gt; ["id", "@version", "unix_ts_in_secs"]
  }
}
output {
  stdout { codec =&gt;  "rubydebug"}
}</pre>
</div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO19-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>Specify the full path to your local JDBC driver .jar file (including version number). For example: <code class="literal">jdbc_driver_library =&gt; "/usr/share/mysql/mysql-connector-java-8.0.24.jar"</code></p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO19-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p>Provide the IP address or hostname and the port of your MySQL host. For example, <code class="literal">jdbc_connection_string =&gt; "jdbc:mysql://127.0.0.1:3306/es_db"</code></p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO19-3"><i class="conum" data-value="3"></i></a><a href="#CO19-4"></a></p>
</td>
<td align="left" valign="top">
<p>Provide your MySQL credentials. The username and password must both be enclosed in quotation marks.</p>
</td>
</tr>
</table>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>If you are using MariaDB (a popular open source community fork of MySQL), there are a couple of things that you need to do differently:</p>
<p>In place of the MySQL JDBC driver, download and unpack the <a href="https://downloads.mariadb.org/connector-java/" class="ulink" target="_top">JDBC driver for MariaDB</a>.</p>
<p>Substitute the following lines in the <code class="literal">jdbc.conf</code> code, including the <code class="literal">ANSI_QUOTES</code> snippet in the last line:</p>
<div class="pre_wrapper lang-txt">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-txt">jdbc_driver_library =&gt; "&lt;driverPath&gt;/mariadb-java-client-&lt;versionNumber&gt;.jar"
jdbc_driver_class =&gt; "org.mariadb.jdbc.Driver"
jdbc_connection_string =&gt; "jdbc:mariadb://&lt;mySQLHost&gt;:3306/es_db?sessionVariables=sql_mode=ANSI_QUOTES"</pre>
</div>
</div>
</div>
<p>Following are some additional details about the Logstash pipeline code:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
jdbc_driver_library
</span>
</dt>
<dd>
The Logstash JDBC plugin does not come packaged with JDBC driver libraries. The JDBC driver library must be passed explicitly into the plugin using the <code class="literal">jdbc_driver_library</code> configuration option.
</dd>
<dt>
<span class="term">
tracking_column
</span>
</dt>
<dd>
This parameter specifies the field <code class="literal">unix_ts_in_secs</code> that tracks the last document read by Logstash from MySQL, stored on disk in <a href="/guide/en/logstash/8.15/plugins-inputs-jdbc.html#plugins-inputs-jdbc-last_run_metadata_path" class="ulink" target="_top">logstash_jdbc_last_run</a>. The parameter determines the starting value for documents that Logstash requests in the next iteration of its polling loop. The value stored in <code class="literal">logstash_jdbc_last_run</code> can be accessed in a SELECT statement as <code class="literal">sql_last_value</code>.
</dd>
<dt>
<span class="term">
unix_ts_in_secs
</span>
</dt>
<dd>
The field generated by the SELECT statement, which contains the <code class="literal">modification_time</code> as a standard <a href="https://en.wikipedia.org/wiki/Unix_time" class="ulink" target="_top">Unix timestamp</a> (seconds since the epoch). The field is referenced by the <code class="literal">tracking column</code>. A Unix timestamp is used for tracking progress rather than a normal timestamp, as a normal timestamp may cause errors due to the complexity of correctly converting back and forth between UMT and the local timezone.
</dd>
<dt>
<span class="term">
sql_last_value
</span>
</dt>
<dd>
This is a <a href="/guide/en/logstash/8.15/plugins-inputs-jdbc.html#_predefined_parameters" class="ulink" target="_top">built-in parameter</a> containing the starting point of the current iteration of the Logstash polling loop, and it is referenced in the SELECT statement line of the JDBC input configuration. This parameter is set to the most recent value of <code class="literal">unix_ts_in_secs</code>, which is read from <code class="literal">.logstash_jdbc_last_run</code>. This value is the starting point for documents returned by the MySQL query that is executed in the Logstash polling loop. Including this variable in the query guarantees that we’re not resending data that is already stored in Elasticsearch.
</dd>
<dt>
<span class="term">
schedule
</span>
</dt>
<dd>
This uses cron syntax to specify how often Logstash should poll MySQL for changes. The specification <code class="literal">*/5 * * * * *</code> tells Logstash to contact MySQL every 5 seconds.  Input from this plugin can be scheduled to run periodically according to a specific schedule. This scheduling syntax is powered by <a href="https://github.com/jmettraux/rufus-scheduler" class="ulink" target="_top">rufus-scheduler</a>. The syntax is cron-like with some extensions specific to Rufus (for example, timezone support).
</dd>
<dt>
<span class="term">
modification_time &lt; NOW()
</span>
</dt>
<dd>
This portion of the SELECT is explained in detail in the next section.
</dd>
<dt>
<span class="term">
filter
</span>
</dt>
<dd>
In this section, the value <code class="literal">id</code> is copied from the MySQL record into a metadata field called <code class="literal">_id</code>, which is later referenced in the output to ensure that each document is written into Elasticsearch with the correct <code class="literal">_id</code> value. Using a metadata field ensures that this temporary value does not cause a new field to be created. The <code class="literal">id</code>, <code class="literal">@version</code>, and <code class="literal">unix_ts_in_secs</code> fields are also removed from the document, since they don&#8217;t need to be written to Elasticsearch.
</dd>
<dt>
<span class="term">
output
</span>
</dt>
<dd>
This section specifies that each document should be written to the standard output using the rubydebug output to help with debugging.
</dd>
</dl>
</div>
</li>
<li class="listitem">
<p>Launch Logstash with your new JDBC configuration file:</p>
<div class="pre_wrapper lang-txt">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-txt">bin/logstash -f jdbc.conf</pre>
</div>
<p>Logstash outputs your MySQL data through standard output (<code class="literal">stdout</code>), your command line interface. The results for the initial data load should look similar to the following:</p>
<div class="pre_wrapper lang-txt">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-txt">[INFO ] 2021-04-21 12:32:32.816 [Ruby-0-Thread-15: :1] jdbc - (0.009082s) SELECT * FROM (SELECT *, UNIX_TIMESTAMP(modification_time) AS unix_ts_in_secs FROM es_table WHERE (UNIX_TIMESTAMP(modification_time) &gt; 0 AND modification_time &lt; NOW()) ORDER BY modification_time ASC) AS 't1' LIMIT 100000 OFFSET 0
{
          "client_name" =&gt; "Targaryen",
    "modification_time" =&gt; 2021-04-21T12:17:16.000Z,
           "@timestamp" =&gt; 2021-04-21T12:17:16.923Z
}
{
          "client_name" =&gt; "Lannister",
    "modification_time" =&gt; 2021-04-21T12:17:16.000Z,
           "@timestamp" =&gt; 2021-04-21T12:17:16.961Z
}
{
          "client_name" =&gt; "Stark",
    "modification_time" =&gt; 2021-04-21T12:17:16.000Z,
           "@timestamp" =&gt; 2021-04-21T12:17:16.963Z
}</pre>
</div>
<p>The Logstash results periodically display SQL SELECT statements, even when there’s nothing new or modified in the MySQL database:</p>
<div class="pre_wrapper lang-txt">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-txt">[INFO ] 2021-04-21 12:33:30.407 [Ruby-0-Thread-15: :1] jdbc - (0.002835s) SELECT count(*) AS 'count' FROM (SELECT *, UNIX_TIMESTAMP(modification_time) AS unix_ts_in_secs FROM es_table WHERE (UNIX_TIMESTAMP(modification_time) &gt; 1618935436 AND modification_time &lt; NOW()) ORDER BY modification_time ASC) AS 't1' LIMIT 1</pre>
</div>
</li>
<li class="listitem">
<p>Open your MySQL console. Let’s insert another record into that database using the following SQL statement:</p>
<div class="pre_wrapper lang-txt">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-txt">use es_db
INSERT INTO es_table (id, client_name)
VALUES (4,"Baratheon");</pre>
</div>
<p>Switch back to your Logstash console. Logstash detects the new record and the console displays results similar to the following:</p>
<div class="pre_wrapper lang-txt">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-txt">[INFO ] 2021-04-21 12:37:05.303 [Ruby-0-Thread-15: :1] jdbc - (0.001205s) SELECT * FROM (SELECT *, UNIX_TIMESTAMP(modification_time) AS unix_ts_in_secs FROM es_table WHERE (UNIX_TIMESTAMP(modification_time) &gt; 1618935436 AND modification_time &lt; NOW()) ORDER BY modification_time ASC) AS 't1' LIMIT 100000 OFFSET 0
{
          "client_name" =&gt; "Baratheon",
    "modification_time" =&gt; 2021-04-21T12:37:01.000Z,
           "@timestamp" =&gt; 2021-04-21T12:37:05.312Z
}</pre>
</div>
</li>
<li class="listitem">
Review the Logstash output results to make sure your data looks correct. Use <code class="literal">CTRL + C</code> to shut down Logstash.
</li>
</ol>
</div>
<h5><a id="ece-db-logstash-output"></a>Output to Elasticsearch<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/cloud/edit/ms-81/docs/shared/ec-ce-getting-started-use-cases-db-logstash.asciidoc">edit</a></h5>
<p>In this section, we configure Logstash to send the MySQL data to Elasticsearch. We modify the configuration file created in the section <a class="xref" href="ece-getting-started-search-use-cases-db-logstash.html#ece-db-logstash-pipeline" title="Configure a Logstash pipeline with the JDBC input plugin">Configure a Logstash pipeline with the JDBC input plugin</a> so that data is output directly to Elasticsearch. We start Logstash to send the data, and then log into Elastic Cloud Enterprise to verify the data in Kibana.</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Open the <code class="literal">jdbc.conf</code> file in the Logstash folder for editing.
</li>
<li class="listitem">
<p>Update the output section with the one that follows:</p>
<div class="pre_wrapper lang-txt">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-txt">output {
  elasticsearch {
    index =&gt; "rdbms_idx"
    ilm_enabled =&gt; false
    cloud_id =&gt; "&lt;DeploymentName&gt;:&lt;ID&gt;" <a id="CO20-1"></a><i class="conum" data-value="1"></i>
    cloud_auth =&gt; "elastic:&lt;Password&gt;" <a id="CO20-2"></a><i class="conum" data-value="2"></i>
    ssl =&gt; true
    cacert =&gt; '/path/to/the/elastic-ece-ca-cert.pem' <a id="CO20-3"></a><i class="conum" data-value="3"></i>
    # api_key =&gt; "&lt;myAPIid:myAPIkey&gt;"
  }
}</pre>
</div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO20-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>Use the Cloud ID of your Elastic Cloud Enterprise deployment. You can include or omit the <code class="literal">&lt;DeploymentName&gt;:</code> prefix at the beginning of the Cloud ID. Both versions work fine. Find your Cloud ID by going to the Kibana main menu and selecting Management &gt; Integrations, and then selecting View deployment details.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO20-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p>the default usename is <code class="literal">elastic</code>.  It is not recommended to use the <code class="literal">elastic</code> account for ingesting data as this is a superuser.  We recommend using a user with reduced permissions, or an API Key with permissions specific to the indices or data streams that will be written to.  Check <a href="/guide/en/logstash/8.15/ls-security.html" class="ulink" target="_top">Configuring security in Logstash</a> for information on roles and API Keys. Use the password provided when you created the deployment if using the <code class="literal">elastic</code> user, or the password used when creating a new ingest user with the roles specified in the <a href="/guide/en/logstash/8.15/ls-security.html" class="ulink" target="_top">Configuring security in Logstash</a> documentation.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO20-3"><i class="conum" data-value="3"></i></a></p>
</td>
<td align="left" valign="top">
<p>This line is only used when you have a self signed certificate for your Elastic Cloud Enterprise proxy.  If needed, specify the full path to the PEM formatted root cetificate (Root CA) used for the Elastic Cloud Enterprise proxy.  You can retrieve the certificate chain from your ECE system by following the instructions in <a class="xref" href="ece-manage-certificates.html#ece-existing-security-certificates" title="Get existing ECE security certificates">Get existing ECE security certificates</a>.  Save the final certificate in the chain to a file. In this example, the file is named <code class="literal">elastic-ece-ca-cert.pem</code>.</p>
</td>
</tr>
</table>
</div>
<p>Following are some additional details about the configuration file settings:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
index
</span>
</dt>
<dd>
The name of the Elasticsearch index, <code class="literal">rdbms_idx</code>, to associate the documents.
</dd>
<dt>
<span class="term">
api_key
</span>
</dt>
<dd>
If you choose to use an API key to authenticate (as discussed in the next step), you can provide it here.
</dd>
</dl>
</div>
</li>
<li class="listitem">
<p><span class="strong strong"><strong>Optional</strong></span>: For additional security, you can generate an Elasticsearch API key through the Elastic Cloud Enterprise console and configure Logstash to use the new key to connect securely to Elastic Cloud Enterprise.</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
<a class="xref" href="ece-login.html" title="Log into the Cloud UI">Log into the Cloud UI</a>.
</li>
<li class="listitem">
<p>On the deployments page, select your deployment.</p>
<p>Narrow the list by name, ID, or choose from several other filters. To further define the list, use a combination of filters.</p>
</li>
<li class="listitem">
From your deployment menu, select <span class="strong strong"><strong>Elasticsearch</strong></span>, then <span class="strong strong"><strong>API console</strong></span>.
</li>
<li class="listitem">
Select <span class="strong strong"><strong>Post</strong></span> from the drop-down list and enter <code class="literal">/_security/api_key</code> in the field.
</li>
<li class="listitem">
<p>Enter the following JSON request:</p>
<div class="pre_wrapper lang-json">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-json">{
 "name": "logstash-apikey",
 "role_descriptors": {
   "logstash_read_write": {
     "cluster": ["manage_index_templates", "monitor"],
     "index": [
       {
         "names": ["logstash-*","rdbms_idx"],
         "privileges": ["create_index", "write", "read", "manage"]
       }
     ]
   }
 }
}</pre>
</div>
<p>This creates an API key with the cluster <code class="literal">monitor</code> privilege which gives read-only access for determining the cluster state, and <code class="literal">manage_index_templates</code> allows all operations on index templates. Some additional privileges also allow <code class="literal">create_index</code>, <code class="literal">write</code>, and <code class="literal">manage</code> operations for the specified index. The index <code class="literal">manage</code> privilege is added to enable index refreshes.</p>
</li>
<li class="listitem">
<p>Select <span class="strong strong"><strong>Submit</strong></span>. The output should be similar to the following:</p>
<div class="pre_wrapper lang-json">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-json">{
  "api_key": "tV1dnfF-GHI59ykgv4N0U3",
  "id": "2TBR42gBabmINotmvZjv",
  "name": "logstash_api_key"
}</pre>
</div>
</li>
<li class="listitem">
<p>Enter your new <code class="literal">api_key</code> value into the Logstash <code class="literal">jdbc.conf</code> file, in the format <code class="literal">&lt;id&gt;:&lt;api_key&gt;</code>. If your results were as shown in this example, you would enter <code class="literal">2TBR42gBabmINotmvZjv:tV1dnfF-GHI59ykgv4N0U3</code>. Remember to remove the pound (<code class="literal">#</code>) sign to uncomment the line, and comment out the <code class="literal">username</code> and <code class="literal">password</code> lines:</p>
<div class="pre_wrapper lang-txt">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-txt">output {
  elasticsearch {
    index =&gt; "rdbms_idx"
    cloud_id =&gt; "&lt;myDeployment&gt;"
    ssl =&gt; true
    ilm_enabled =&gt; false
    api_key =&gt; "2TBR42gBabmINotmvZjv:tV1dnfF-GHI59ykgv4N0U3"
    # user =&gt; "&lt;Username&gt;"
    # password =&gt; "&lt;Password&gt;"
  }
}</pre>
</div>
</li>
</ol>
</div>
</li>
<li class="listitem">
<p>At this point, if you simply restart Logstash as is with your new output, then no MySQL data is sent to our Elasticsearch index.</p>
<p>Why? Logstash retains the previous <code class="literal">sql_last_value</code> timestamp and sees that no new changes have occurred in the MySQL database since that time. Therefore, based on the SQL query that we configured, there’s no new data to send to Logstash.</p>
<p>Solution:  Add <code class="literal">clean_run =&gt; true</code> as a new line in the JDBC input section of the <code class="literal">jdbc.conf</code> file. When set to <code class="literal">true</code>, this parameter resets <code class="literal">sql_last_value</code> back to zero.</p>
<div class="pre_wrapper lang-txt">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-txt">input {
  jdbc {
      ...
      clean_run =&gt; true
      ...
    }
}</pre>
</div>
<p>After running Logstash once with <code class="literal">sql_last_value</code> set to <code class="literal">true</code> you can remove the <code class="literal">clean_run</code> line, unless you prefer the reset behavior to happen again at each restart of Logstash</p>
</li>
<li class="listitem">
<p>Open a command line interface instance, go to your Logstash installation path, and start Logstash:</p>
<div class="pre_wrapper lang-txt">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-txt">bin/logstash -f jdbc.conf</pre>
</div>
</li>
<li class="listitem">
<p>Logstash outputs the MySQL data to your Elastic Cloud Enterprise deployment. Let’s take a look in Kibana and verify that data:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
<a class="xref" href="ece-login.html" title="Log into the Cloud UI">Log into the Cloud UI</a>.
</li>
<li class="listitem">
<p>On the deployments page, select your deployment.</p>
<p>Narrow the list by name, ID, or choose from several other filters. To further define the list, use a combination of filters.</p>
</li>
<li class="listitem">
From your deployment menu, select <span class="strong strong"><strong>Kibana</strong></span>, then <span class="strong strong"><strong>Launch</strong></span>.
</li>
<li class="listitem">
Open the side-menu panel using the three vertical line button.
</li>
<li class="listitem">
Scroll down and expand Management.
</li>
<li class="listitem">
Select <span class="strong strong"><strong>Management</strong></span> and then <span class="strong strong"><strong>Dev Tools</strong></span>.
</li>
<li class="listitem">
<p>Copy and paste the following API GET request into the Console pane, and then use the Play arrow. This queries all records in the new <code class="literal">rdbms_idx</code> index.</p>
<div class="pre_wrapper lang-txt">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-txt">GET rdbms_idx/_search
{
  "query": {
    "match_all": {}
  }
}</pre>
</div>
</li>
<li class="listitem">
<p>The Results pane lists the <code class="literal">client_name</code> records originating from your MySQL database, similar to the following example:</p>
<p><span class="image"><img src="images/ec-logstash-db-results-scenarios.png" alt="A picture showing query results with three records" width="corresponding to the Targaryen" height="Lannister"></span></p>
</li>
</ol>
</div>
</li>
</ol>
</div>
<p>Now, you should have a good understanding of how to configure Logstash to ingest data from your relational database through the JDBC Plugin. You have some design considerations to track records that are new, modified, and deleted. You should have the basics needed to begin experimenting with your own database and Elasticsearch.</p>
</div>
</div>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  messageStyle: "none",
  tex2jax: {
    inlineMath: [["\\(", "\\)"]],
    displayMath: [["\\[", "\\]"]],
    ignoreClass: "nostem|nolatexmath"
  },
  asciimath2jax: {
    delimiters: [["\\$", "\\$"]],
    ignoreClass: "nostem|noasciimath"
  },
  TeX: { equationNumbers: { autoNumber: "none" } }
})
MathJax.Hub.Register.StartupHook("AsciiMath Jax Ready", function () {
  MathJax.InputJax.AsciiMath.postfilterHooks.Add(function (data, node) {
    if ((node = data.script.parentNode) && (node = node.parentNode) && node.classList.contains('stemblock')) {
      data.math.root.display = "block"
    }
    return data
  })
})
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>
</div><div class="navfooter">
<span class="prev">
<a href="ece-getting-started-search-use-cases-beats-logstash.html">« Ingest data from Beats to Elastic Cloud Enterprise with Logstash as a proxy</a>
</span>
<span class="next">
<a href="ece-getting-started-search-use-cases-python-logs.html">Ingest logs from a Python application using Filebeat »</a>
</span>
</div>
</body>
</html>
