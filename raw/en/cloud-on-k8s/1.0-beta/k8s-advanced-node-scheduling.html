<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Advanced Elasticsearch node scheduling | Elastic Cloud on Kubernetes [1.0-beta] | Elastic</title>
<link rel="home" href="index.html" title="Elastic Cloud on Kubernetes [1.0-beta]"/>
<link rel="up" href="k8s-elasticsearch-specification.html" title="Running Elasticsearch on ECK"/>
<link rel="prev" href="k8s-orchestration.html" title="Nodes orchestration"/>
<link rel="next" href="k8s-snapshots.html" title="Create automated snapshots"/>
<meta name="DC.type" content="Learn/Docs/Kubernetes/Reference/1.0-beta"/>
<meta name="DC.subject" content="ECK"/>
<meta name="DC.identifier" content="1.0-beta"/>
</head>
<body><div class="page_header">
You are looking at the documentation for a beta release.
</div>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="index.html">Elastic Cloud on Kubernetes [1.0-beta]</a></span>
»
<span class="breadcrumb-link"><a href="k8s-elasticsearch-specification.html">Running Elasticsearch on ECK</a></span>
»
<span class="breadcrumb-node">Advanced Elasticsearch node scheduling</span>
</div>
<div class="navheader">
<span class="prev">
<a href="k8s-orchestration.html">« Nodes orchestration</a>
</span>
<span class="next">
<a href="k8s-snapshots.html">Create automated snapshots »</a>
</span>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="k8s-advanced-node-scheduling"></a>Advanced Elasticsearch node scheduling<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/cloud-on-k8s/edit/1.0-beta/docs/advanced-node-scheduling.asciidoc">edit</a></h2>
</div></div></div>
<p>Elastic Cloud on Kubernetes (ECK) offers full control over Elasticsearch cluster nodes scheduling by combining Elasticsearch configuration with Kubernetes scheduling options:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="k8s-advanced-node-scheduling.html#k8s-define-elasticsearch-nodes-roles" title="Define Elasticsearch nodes roles">Define Elasticsearch nodes roles</a>
</li>
<li class="listitem">
<a class="xref" href="k8s-advanced-node-scheduling.html#k8s-affinity-options" title="Affinity options">Pod affinity and anti-affinity</a>
</li>
<li class="listitem">
<a class="xref" href="k8s-advanced-node-scheduling.html#k8s-availability-zone-awareness" title="Availability zone awareness">Availability zone and rack awareness</a>
</li>
<li class="listitem">
<a class="xref" href="k8s-advanced-node-scheduling.html#k8s-hot-warm-topologies" title="Hot-warm topologies">Hot-warm topologies</a>
</li>
</ul>
</div>
<p>These features can be combined together, to deploy a production-grade Elasticsearch cluster.</p>
<h4><a id="k8s-define-elasticsearch-nodes-roles"></a>Define Elasticsearch nodes roles<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/cloud-on-k8s/edit/1.0-beta/docs/advanced-node-scheduling.asciidoc">edit</a></h4>
<p>You can configure Elasticsearch nodes with <a href="/guide/en/elasticsearch/reference/current/modules-node.html" class="ulink" target="_top">one or multiple roles</a>. This allows you to describe an Elasticsearch cluster with 3 dedicated master nodes, for example:</p>
<div class="pre_wrapper lang-yaml">
<pre class="programlisting prettyprint lang-yaml">apiVersion: elasticsearch.k8s.elastic.co/v1beta1
kind: Elasticsearch
metadata:
  name: quickstart
spec:
  version: 7.11.1
  nodeSets:
  # 3 dedicated master nodes
  - name: master
    count: 3
    config:
      node.master: true
      node.data: false
      node.ingest: false
      cluster.remote.connect: false
  # 3 ingest-data nodes
  - name: ingest-data
    count: 3
    config:
      node.master: false
      node.data: true
      node.ingest: true</pre>
</div>
<h4><a id="k8s-affinity-options"></a>Affinity options<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/cloud-on-k8s/edit/1.0-beta/docs/advanced-node-scheduling.asciidoc">edit</a></h4>
<p>You can setup various <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity" class="ulink" target="_top">affinity and anti-affinity options</a> through the <code class="literal">podTemplate</code> section of the Elasticsearch resource specification.</p>
<h5><a id="k8s_a_single_elasticsearch_node_per_kubernetes_host_default"></a>A single Elasticsearch node per Kubernetes host (default)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/cloud-on-k8s/edit/1.0-beta/docs/advanced-node-scheduling.asciidoc">edit</a></h5>
<p>To avoid scheduling several Elasticsearch nodes from the same cluster on the same host, use a <code class="literal">podAntiAffinity</code> rule based on the hostname and the cluster name label:</p>
<div class="pre_wrapper lang-yaml">
<pre class="programlisting prettyprint lang-yaml">apiVersion: elasticsearch.k8s.elastic.co/v1beta1
kind: Elasticsearch
metadata:
  name: quickstart
spec:
  version: 7.11.1
  nodesSets:
  - name: default
    count: 3
    podTemplate:
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    elasticsearch.k8s.elastic.co/cluster-name: quickstart
                topologyKey: kubernetes.io/hostname</pre>
</div>
<p>This is ECK default behaviour if you don&#8217;t specify any <code class="literal">affinity</code> option.</p>
<p>To explicitly disable that behaviour, set an empty affinity object:</p>
<div class="pre_wrapper lang-yaml">
<pre class="programlisting prettyprint lang-yaml">apiVersion: elasticsearch.k8s.elastic.co/v1beta1
kind: Elasticsearch
metadata:
  name: quickstart
spec:
  version: 7.11.1
  nodeSets:
  - name: default
    count: 3
    podTemplate:
      spec:
        affinity: {}</pre>
</div>
<p>The default affinity is using <code class="literal">preferredDuringSchedulingIgnoredDuringExecution</code>, which acts as best effort and won&#8217;t prevent an Elasticsearch node from being scheduled on a host if there are no other hosts available. Scheduling a 4-nodes Elasticsearch cluster on a 3-host Kubernetes cluster would then successfully schedule 2 Elasticsearch nodes on the same host. To enforce a strict single node per host, specify <code class="literal">requiredDuringSchedulingIgnoredDuringExecution</code> instead:</p>
<div class="pre_wrapper lang-yaml">
<pre class="programlisting prettyprint lang-yaml">apiVersion: elasticsearch.k8s.elastic.co/v1beta1
kind: Elasticsearch
metadata:
  name: quickstart
spec:
  version: 7.11.1
  nodeSets:
  - name: default
    count: 3
    podTemplate:
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    elasticsearch.k8s.elastic.co/cluster-name: quickstart
              topologyKey: kubernetes.io/hostname</pre>
</div>
<h5><a id="k8s_local_persistent_volume_constraints"></a>Local Persistent Volume constraints<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/cloud-on-k8s/edit/1.0-beta/docs/advanced-node-scheduling.asciidoc">edit</a></h5>
<p>By default, volumes can be bound to a pod before the pod gets scheduled to a particular Kubernetes node. This can be a problem if the PersistentVolume can only be accessed from a particular host or set of hosts. Local persistent volumes are a good example: they are accessible from a single host. If the pod gets scheduled to a different host based on any affinity or anti-affinity rule, the volume may not be available.</p>
<p>To solve this problem, you can <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode" class="ulink" target="_top">set the Volume Binding Mode</a> of the <code class="literal">StorageClass</code> you are using. Make sure  <code class="literal">volumeBindingMode: WaitForFirstConsumer</code> is set, <a href="https://kubernetes.io/docs/concepts/storage/volumes/#local" class="ulink" target="_top">especially if you are using local persistent volumes</a>.</p>
<div class="pre_wrapper lang-yaml">
<pre class="programlisting prettyprint lang-yaml">kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer</pre>
</div>
<h5><a id="k8s_node_affinity"></a>Node affinity<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/cloud-on-k8s/edit/1.0-beta/docs/advanced-node-scheduling.asciidoc">edit</a></h5>
<p>To restrict the scheduling to a particular set of Kubernetes nodes based on labels, use a <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector" class="ulink" target="_top">NodeSelector</a>.
The following example schedules Elasticsearch pods on Kubernetes nodes tagged with both labels <code class="literal">diskType: ssd</code> and <code class="literal">environment: production</code>.</p>
<div class="pre_wrapper lang-yaml">
<pre class="programlisting prettyprint lang-yaml">apiVersion: elasticsearch.k8s.elastic.co/v1beta1
kind: Elasticsearch
metadata:
  name: quickstart
spec:
  version: 7.11.1
  nodeSets:
  - name: default
    count: 3
    podTemplate:
      spec:
        nodeSelector:
          diskType: ssd
          environment: production</pre>
</div>
<p>You can achieve the same (and more) with <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity-beta-feature" class="ulink" target="_top">node affinity</a>:</p>
<div class="pre_wrapper lang-yaml">
<pre class="programlisting prettyprint lang-yaml">apiVersion: elasticsearch.k8s.elastic.co/v1beta1
kind: Elasticsearch
metadata:
  name: quickstart
spec:
  version: 7.11.1
  nodeSets:
  - name: default
    count: 3
    podTemplate:
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: environment
                  operator: In
                  values:
                  - e2e
                  - production
            preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 1
                preference:
                  matchExpressions:
                  - key: diskType
                    operator: In
                    values:
                    - ssd</pre>
</div>
<p>This example restricts Elasticsearch nodes from being scheduled on Kubernetes hosts tagged with <code class="literal">environment: e2e</code> or <code class="literal">environment: production</code>. It favors nodes tagged with <code class="literal">diskType: ssd</code>.</p>
<h4><a id="k8s-availability-zone-awareness"></a>Availability zone awareness<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/cloud-on-k8s/edit/1.0-beta/docs/advanced-node-scheduling.asciidoc">edit</a></h4>
<p>By combining <a href="/guide/en/elasticsearch/reference/current/allocation-awareness.html#allocation-awareness" class="ulink" target="_top">Elasticsearch shard allocation awareness</a> with <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity-beta-feature" class="ulink" target="_top">Kubernetes node affinity</a>, you can setup an availability zone-aware Elasticsearch cluster:</p>
<div class="pre_wrapper lang-yaml">
<pre class="programlisting prettyprint lang-yaml">apiVersion: elasticsearch.k8s.elastic.co/v1beta1
kind: Elasticsearch
metadata:
  name: quickstart
spec:
  version: 7.11.1
  nodeSets:
  - name: zone-a
    count: 1
    config:
      node.attr.zone: europe-west3-a
      cluster.routing.allocation.awareness.attributes: zone
    podTemplate:
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: failure-domain.beta.kubernetes.io/zone
                  operator: In
                  values:
                  - europe-west3-a
  - name: zone-b
    count: 1
    config:
      node.attr.zone: europe-west3-b
      cluster.routing.allocation.awareness.attributes: zone
    podTemplate:
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: failure-domain.beta.kubernetes.io/zone
                  operator: In
                  values:
                  - europe-west3-b</pre>
</div>
<p>This example relies on:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Kubernetes nodes in each zone being labeled accordingly. <code class="literal">failure-domain.beta.kubernetes.io/zone</code> <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#interlude-built-in-node-labels" class="ulink" target="_top">is standard</a>, but any label can be used.
</li>
<li class="listitem">
node affinity for each group of nodes set to match the Kubernetes nodes' zone.
</li>
<li class="listitem">
Elasticsearch configured to <a href="/guide/en/elasticsearch/reference/current/allocation-awareness.html#allocation-awareness" class="ulink" target="_top">allocate shards based on node attributes</a>. Here we specified <code class="literal">node.attr.zone</code>, but any attribute name can be used. <code class="literal">node.attr.rack_id</code> is another common example.
</li>
</ul>
</div>
<h4><a id="k8s-hot-warm-topologies"></a>Hot-warm topologies<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/cloud-on-k8s/edit/1.0-beta/docs/advanced-node-scheduling.asciidoc">edit</a></h4>
<p>By combining <a href="/guide/en/elasticsearch/reference/current/allocation-awareness.html#allocation-awareness" class="ulink" target="_top">Elasticsearch shard allocation awareness</a> with <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity-beta-feature" class="ulink" target="_top">Kubernetes node affinity</a>, you can setup an Elasticsearch cluster with hot-warm topology:</p>
<div class="pre_wrapper lang-yaml">
<pre class="programlisting prettyprint lang-yaml">apiVersion: elasticsearch.k8s.elastic.co/v1beta1
kind: Elasticsearch
metadata:
  name: quickstart
spec:
  version: 7.11.1
  nodeSets:
  # hot nodes, with high CPU and fast IO
  - name: hot
    count: 3
    config:
      node.attr.data: hot
    podTemplate:
      spec:
        containers:
        - name: elasticsearch
          resources:
            limits:
              memory: 16Gi
              cpu: 4
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: beta.kubernetes.io/instance-type
                  operator: In
                  values:
                  - highio
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 1Ti
        storageClassName: local-storage
  # warm nodes, with high storage
  - name: warm
    count: 3
    config:
      node.attr.data: warm
    podTemplate:
      spec:
        containers:
        - name: elasticsearch
          resources:
            limits:
              memory: 16Gi
              cpu: 2
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: beta.kubernetes.io/instance-type
                  operator: In
                  values:
                  - highstorage
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 10Ti
        storageClassName: local-storage</pre>
</div>
<p>In this example, we configure two groups of Elasticsearch nodes:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
the first group has the <code class="literal">data</code> attribute set to <code class="literal">hot</code>. It is intended to run on hosts with high CPU resources and fast IO (SSD). Here we restrict pods to be scheduled on Kubernetes nodes labeled with <code class="literal">beta.kubernetes.io/instance-type: highio</code> (to adapt to your Kubernetes nodes' labels).
</li>
<li class="listitem">
the second group has the <code class="literal">data</code> attribute set to <code class="literal">warm</code>. It is intended to run on hosts with larger but maybe slower storage. Pods are only able to be scheduled on nodes labeled with <code class="literal">beta.kubernetes.io/instance-type: highstorage</code>.
</li>
</ul>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>this example uses <a href="https://kubernetes.io/docs/concepts/storage/volumes/#local" class="ulink" target="_top">Local Persistent Volumes</a> for both groups, but can be adapted to use high-performance volumes for <code class="literal">hot</code> Elasticsearch nodes and high-storage volumes for <code class="literal">warm</code> Elasticsearch nodes.</p>
</div>
</div>
<p>Finally, setup <a href="/guide/en/elasticsearch/reference/current/index-lifecycle-management.html" class="ulink" target="_top">Index Lifecycle Management</a> policies on your indices, <a href="/blog/implementing-hot-warm-cold-in-elasticsearch-with-index-lifecycle-management" class="ulink" target="_top">optimizing for hot-warm architectures</a>.</p>
</div>
<div class="navfooter">
<span class="prev">
<a href="k8s-orchestration.html">« Nodes orchestration</a>
</span>
<span class="next">
<a href="k8s-snapshots.html">Create automated snapshots »</a>
</span>
</div>
</div>
</body>
</html>
