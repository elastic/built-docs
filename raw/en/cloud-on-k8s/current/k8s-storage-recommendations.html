<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Storage recommendations | Elastic Cloud on Kubernetes [1.4] | Elastic</title>
<link rel="home" href="index.html" title="Elastic Cloud on Kubernetes [1.4]"/>
<link rel="up" href="k8s-elasticsearch-specification.html" title="Run Elasticsearch on ECK"/>
<link rel="prev" href="k8s-volume-claim-templates.html" title="Volume claim templates"/>
<link rel="next" href="k8s-http-settings-tls-sans.html" title="HTTP settings and TLS SANs"/>
<meta name="DC.type" content="Learn/Docs/Kubernetes/Reference/1.4"/>
<meta name="DC.subject" content="ECK"/>
<meta name="DC.identifier" content="1.4"/>
</head>
<body>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="index.html">Elastic Cloud on Kubernetes [1.4]</a></span>
»
<span class="breadcrumb-link"><a href="k8s-orchestrating-elastic-stack-applications.html">Orchestrating Elastic Stack applications</a></span>
»
<span class="breadcrumb-link"><a href="k8s-elasticsearch-specification.html">Run Elasticsearch on ECK</a></span>
»
<span class="breadcrumb-node">Storage recommendations</span>
</div>
<div class="navheader">
<span class="prev">
<a href="k8s-volume-claim-templates.html">« Volume claim templates</a>
</span>
<span class="next">
<a href="k8s-http-settings-tls-sans.html">HTTP settings and TLS SANs »</a>
</span>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="k8s-storage-recommendations"></a>Storage recommendations<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/cloud-on-k8s/edit/1.4/docs/orchestrating-elastic-stack-applications/elasticsearch/storage-recommendations.asciidoc">edit</a></h2>
</div></div></div>
<p>ECK does not come with its own storage mechanism for Elasticsearch data. It is compatible with any Kubernetes storage option. It is recommended to use PersistentVolumes, by configuring the <a class="xref" href="k8s-volume-claim-templates.html" title="Volume claim templates">VolumeClaimTemplates</a> section of the Elasticsearch resource.</p>
<p>Multiple PersistentVolume storage classes are available, depending on your Kubernetes setup. Their specifications impact Elasticsearch performance and operations. Evaluate the trade-offs among the various options and choose the solution that best fits your needs.</p>
<h4><a id="k8s_network_attached_or_local_persistentvolumes"></a>Network-attached or Local PersistentVolumes<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/cloud-on-k8s/edit/1.4/docs/orchestrating-elastic-stack-applications/elasticsearch/storage-recommendations.asciidoc">edit</a></h4>
<p>PersistentVolumes can be of two types: <span class="strong strong"><strong>Network-attached</strong></span> or <span class="strong strong"><strong>Local</strong></span>. ECK handles them in the same way, but they have different performance, price and operational characteristics.</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<span class="strong strong"><strong>Network-attached PersistentVolumes</strong></span> can generally be attached to a Pod regardless of the host they are scheduled on.
They provide a major operational benefit: if the host goes down, or needs to be replaced, the Pod can simply be deleted. Kubernetes reschedules it automatically on a different host, generally in the same region, and reattaches the same volume. This can take only a few seconds, and does not require any human intervention.
</li>
<li class="listitem">
<span class="strong strong"><strong>Local PersistentVolumes</strong></span> are bound to a particular host, and map a directory on the filesystem. They provide a major operational overhead: once bound to a Local PersistentVolume, a Pod can only be scheduled on the same host. If that host goes down, or needs to be replaced, the Pod cannot be scheduled on a different host. It remains in a <code class="literal">Pending</code> state until the host is available, or until the PersistentVolumeClaim is manually deleted. For that reason, Local PersistentVolumes bring more operational overhead.
</li>
</ul>
</div>
<p>In both cases, the performance depends on the underlying hardware and implementation. In general, local SSDs give the best performance. The fastest network-attached volumes from major Cloud providers can also provide acceptable performance, depending on your Elasticsearch use cases. To better evaluate your performance requirements, you can <a href="https://github.com/elastic/rally" class="ulink" target="_top">benchmark</a> your storage options against the expected Elasticsearch usage.</p>
<h4><a id="k8s_local_persistentvolumes_operations"></a>Local PersistentVolumes operations<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/cloud-on-k8s/edit/1.4/docs/orchestrating-elastic-stack-applications/elasticsearch/storage-recommendations.asciidoc">edit</a></h4>
<h5><a id="k8s_host_maintenance"></a>Host maintenance<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/cloud-on-k8s/edit/1.4/docs/orchestrating-elastic-stack-applications/elasticsearch/storage-recommendations.asciidoc">edit</a></h5>
<p>To take a host out of the Kubernetes cluster temporarily, it is common to cordon, then drain it. Kubernetes deletes Elasticsearch Pods scheduled on that host automatically, as long as the <a class="xref" href="k8s-pod-disruption-budget.html" title="Pod disruption budget">PodDisruptionBudget</a> allows it. By default, ECK manages a PodDisruptionBudget that allows one Pod to be taken down, as long as the cluster has a green health. Once deleted, that Pod cannot be scheduled again on the cordoned host: the Pod stays <code class="literal">Pending</code>, waiting for that host to come back online. The next Pod can be automatically deleted when the Elasticsearch cluster health becomes green again.</p>
<p>Some hosted Kubernetes offerings only respect the PodDisruptionBudget for a certain amount of time, before killing all Pods on the node. For example, <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-upgrades" class="ulink" target="_top">GKE automated version upgrade</a> rotates all nodes without preserving local volumes, and respects the PodDisruptionBudget for a maximum of one hour. In such cases it is preferable to <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-upgrades#upgrading_manually" class="ulink" target="_top">manually handle the cluster version upgrade</a>.</p>
<h5><a id="k8s_host_removal"></a>Host removal<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/cloud-on-k8s/edit/1.4/docs/orchestrating-elastic-stack-applications/elasticsearch/storage-recommendations.asciidoc">edit</a></h5>
<p>If a host has a failure, or is permanently removed, its local data is likely lost. The corresponding Pod stays <code class="literal">Pending</code> because it can no longer attach the PersistentVolume. To schedule the Pod on a different host with a new empty volume, you have to manually remove both the PersistenteVolumeClaim and the Pod. A new Pod is automatically created with a new PersistentVolumeClaim, which is then matched with a PersistentVolume. Then, Elasticsearch shard replication makes sure that data is recovered on the new instance.</p>
<h4><a id="k8s_local_persistentvolums_provisioners"></a>Local PersistentVolums provisioners<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/cloud-on-k8s/edit/1.4/docs/orchestrating-elastic-stack-applications/elasticsearch/storage-recommendations.asciidoc">edit</a></h4>
<p>You can provision Local PersistentVolumes in one of the following ways:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<span class="strong strong"><strong>Manual provisioning</strong></span>: Manually <a href="https://kubernetes.io/blog/2018/04/13/local-persistent-volumes-beta/#creating-a-local-persistent-volume" class="ulink" target="_top">create PersistentVolume resources</a>, matching a local path to a specific host. Data must be manually removed once the PersistentVolumes are released.
</li>
<li class="listitem">
<span class="strong strong"><strong>Static provisioning</strong></span>: Run a program that automatically discovers the existing partitions on each host, and creates the corresponding PersistentVolume resources. The <a href="https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner" class="ulink" target="_top">Local PersistentVolume Static Provisioner</a> is a great way to get started.
</li>
<li class="listitem">
<span class="strong strong"><strong>Dynamic provisioning</strong></span>: Run a controller to dynamically create PersistentVolumes of the requested storage size to match  PersistentVolumeClaims. <a href="https://github.com/topolvm/topolvm" class="ulink" target="_top">TopoLVM</a> is a great example. It dynamically provisions LVM volumes, formats their filesystem, and supports resizing. It requires installing an extra scheduler for capacity-awareness, which may not be permitted on hosted Kubernetes offerings.
</li>
</ul>
</div>
<h4><a id="k8s_storage_class_settings"></a>Storage class settings<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/cloud-on-k8s/edit/1.4/docs/orchestrating-elastic-stack-applications/elasticsearch/storage-recommendations.asciidoc">edit</a></h4>
<h5><a id="k8s_volumebindingmode_waitforfirstconsumer"></a>volumeBindingMode: WaitForFirstConsumer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/cloud-on-k8s/edit/1.4/docs/orchestrating-elastic-stack-applications/elasticsearch/storage-recommendations.asciidoc">edit</a></h5>
<p>In the PersistentVolume StorageClass, it is important to set <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode" class="ulink" target="_top"><code class="literal">volumeBindingMode: WaitForFirstConsumer</code></a>, otherwise a Pod might be scheduled on a host that cannot access the existing PersistentVolume. This setting isn&#8217;t always applied by default on Cloud providers StorageClasses, but in most cases it is possible to create (or patch) StorageClasses to add the setting.</p>
<h5><a id="k8s_reclaim_policy"></a>Reclaim policy<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/cloud-on-k8s/edit/1.4/docs/orchestrating-elastic-stack-applications/elasticsearch/storage-recommendations.asciidoc">edit</a></h5>
<p>The reclaim policy of a StorageClass specifies whether a PersistentVolume should be automatically deleted once its corresponding PersistentVolumeClaim is deleted. It can be set to <code class="literal">Delete</code> or <code class="literal">Retain</code>.</p>
<p>ECK automatically deletes PersistentVolumeClaims when they are no longer needed, following a cluster downscale or deletion. However, ECK does not delete PersistentVolumes. The system cannot reuse a PersistentVolume with existing data from a different cluster. In this case Elasticsearch does not start, as it detects data that belongs to a different cluster. For this reason, it is recommended to use the <code class="literal">Delete</code> reclaim policy.</p>
</div>
<div class="navfooter">
<span class="prev">
<a href="k8s-volume-claim-templates.html">« Volume claim templates</a>
</span>
<span class="next">
<a href="k8s-http-settings-tls-sans.html">HTTP settings and TLS SANs »</a>
</span>
</div>
</div>
</body>
</html>
