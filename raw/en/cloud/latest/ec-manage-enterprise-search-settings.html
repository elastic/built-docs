<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Add Enterprise Search user settings | Elasticsearch Service Documentation | Elastic</title>
<link rel="home" href="index.html" title="Elasticsearch Service Documentation"/>
<link rel="up" href="ec-editing-user-settings.html" title="Edit your user settings"/>
<link rel="prev" href="ec-manage-appsearch-settings.html" title="Add App Search user settings"/>
<link rel="next" href="ec-manage-deployment.html" title="Manage your deployment"/>
<meta name="DC.type" content="Learn/Docs/Cloud/Reference"/>
<meta name="DC.subject" content="Elastic Cloud"/>
<meta name="DC.identifier" content="latest"/>
</head>
<body>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span><span class="chevron-right">›</span>
<span class="breadcrumb-link"><a href="index.html">Elasticsearch Service Documentation</a></span><span class="chevron-right">›</span>
<span class="breadcrumb-link"><a href="ec-prepare-production.html">Preparing a deployment for production</a></span><span class="chevron-right">›</span>
<span class="breadcrumb-link"><a href="ec-editing-user-settings.html">Edit your user settings</a></span>
</div>
<div class="navheader">
<span class="prev">
<a href="ec-manage-appsearch-settings.html">« Add App Search user settings</a>
</span>
<span class="next">
<a href="ec-manage-deployment.html">Manage your deployment »</a>
</span>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="ec-manage-enterprise-search-settings"></a>Add Enterprise Search user settings<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/cloud/edit/ms-84/docs/shared/ec-ce-manage-enterprise-search-settings.asciidoc">edit</a></h2>
</div></div></div>
<p>Change how Enterprise Search runs by providing your own user settings. User settings are appended to the <code class="literal">ent-search.yml</code> configuration file for your instance and provide custom configuration options.</p>
<div class="tip admon">
<div class="icon"></div>
<div class="admon_content">
<p>Some settings that could break your cluster if set incorrectly are blocked. Review the <a class="xref" href="ec-manage-enterprise-search-settings.html#ec-enterprise-search-settings" title="Supported Enterprise Search settings">list of settings</a> that are generally safe in cloud environments. For detailed information about Enterprise Search settings, check the <a href="/guide/en/enterprise-search/current/configuration.html" class="ulink" target="_top">Enterprise Search documentation</a>.</p>
</div>
</div>
<p>To add user settings:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Log in to the <a href="https://cloud.elastic.co?baymax=docs-body&amp;elektra=docs" class="ulink" target="_top">Elasticsearch Service Console</a>.
</li>
<li class="listitem">
<p>Find your deployment on the home page in the Elasticsearch Service card and click the gear icon to access it directly. Or, select Elasticsearch Service to go to the deployments page to view all of your deployments.</p>
<p>On the deployments page you can narrow your deployments by name, ID, or choose from several other filters. To customize your view, use a combination of filters, or change the format from a grid to a list.</p>
</li>
<li class="listitem">
From your deployment menu, go to the <span class="strong strong"><strong>Edit</strong></span> page.
</li>
<li class="listitem">
In the <span class="strong strong"><strong>Enterprise Search</strong></span> section, select <span class="strong strong"><strong>Edit user settings</strong></span>. For deployments with existing user settings, you may have to expand the <span class="strong strong"><strong>Edit enterprise-search.yml</strong></span> caret instead.
</li>
<li class="listitem">
Update the user settings.
</li>
<li class="listitem">
Select <span class="strong strong"><strong>Save changes</strong></span>.
</li>
</ol>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>If a setting is not supported by Elasticsearch Service, an error message displays when you try to save your settings.</p>
</div>
</div>
<h4><a id="ec-enterprise-search-settings"></a>Supported Enterprise Search settings<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/cloud/edit/ms-84/docs/shared/ec-ce-manage-enterprise-search-settings.asciidoc">edit</a></h4>
<p>Elasticsearch Service supports the following Enterprise Search settings.</p>
<p><span class="strong strong"><strong>For version 8.6 and later:</strong></span></p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">connector.crawler.http.head_requests.enabled</code>
</span>
</dt>
<dd>
Enable/disable performing HEAD requests before GET requests. This is disabled by default. For more details, check the <a href="/guide/en/enterprise-search/current/configuration.html#configuration-settings-elastic-crawler-advanced-tuning" class="ulink" target="_top">Crawler Advanced Tuning documentation</a>.
</dd>
</dl>
</div>
<p><span class="strong strong"><strong>For version 8.4 and later:</strong></span></p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">connector.crawler.logging.events.enabled</code>
</span>
</dt>
<dd>
Enable or disable indexing of Elasticsearch Crawler Event logs. These are enabled by default. Disabling these will impact dashboards and analytics.
</dd>
<dt>
<span class="term">
<code class="literal">connector.crawler.security.auth.allow_http</code>
</span>
</dt>
<dd>
Allow authenticated crawling of non-HTTPS URLs.
</dd>
<dt>
<span class="term">
<code class="literal">connector.crawler.content_extraction.enabled</code>
</span>
</dt>
<dd>
Enable extraction of non-HTML files found during a crawl.
</dd>
<dt>
<span class="term">
<code class="literal">connector.crawler.content_extraction.mime_types</code>
</span>
</dt>
<dd>
Extract files with these MIME types.
</dd>
<dt>
<span class="term">
<code class="literal">connector.crawler.http.compression.enabled</code>
</span>
</dt>
<dd>
Enable/disable HTTP content (gzip/deflate) compression in Crawler requests
</dd>
<dt>
<span class="term">
<code class="literal">connector.crawler.http.default_encoding</code>
</span>
</dt>
<dd>
Default encoding used for responses that do not specify a charset.
</dd>
<dt>
<span class="term">
<code class="literal">connector.crawler.http.proxy.host</code>
</span>
</dt>
<dd>
HTTP proxy host to use for all web crawler connections.
</dd>
<dt>
<span class="term">
<code class="literal">connector.crawler.http.proxy.port</code>
</span>
</dt>
<dd>
HTTP proxy port to use for all web crawler connections.
</dd>
<dt>
<span class="term">
<code class="literal">connector.crawler.http.proxy.protocol</code>
</span>
</dt>
<dd>
The protocol to use for proxy connections (<code class="literal">http</code> or <code class="literal">https</code>). Defaults to <code class="literal">http</code>.
</dd>
<dt>
<span class="term">
<code class="literal">connector.crawler.http.proxy.username</code>
</span>
</dt>
<dd>
The username to use for proxy authentication.
</dd>
<dt>
<span class="term">
<code class="literal">connector.crawler.http.proxy.password</code>
</span>
</dt>
<dd>
The password to use for proxy authentication.
</dd>
<dt>
<span class="term">
<code class="literal">connector.crawler.security.ssl.certificate_authorities</code>
</span>
</dt>
<dd>
A list of additional custom SSL certificate authority certificates to be used during crawling. Only inline certificate values are supported.
</dd>
<dt>
<span class="term">
<code class="literal">connector.crawler.security.ssl.certificate_authorities</code>
</span>
</dt>
<dd>
SSL verification mode to be used for all Crawler connections: <code class="literal">full</code> (this is the default), <code class="literal">certificate</code> or <code class="literal">none</code>.
</dd>
<dt>
<span class="term">
<code class="literal">connector.crawler.crawl.url_queue.url_count.limit</code>
</span>
</dt>
<dd>
The maximum size of the crawl frontier - the list of URLs the crawler needs to visit. The list is stored in Elasticsearch, so the limit could be increased as long as the Elasticsearch cluster has enough resources (disk space) to hold the queue index.
</dd>
<dt>
<span class="term">
<code class="literal">connector.crawler.workers.pool_size.limit</code>
</span>
</dt>
<dd>
The number of parallel crawls allowed per instance of Enterprise Search.
</dd>
<dt>
<span class="term">
<code class="literal">connector.crawler.crawl.threads.limit</code>
</span>
</dt>
<dd>
The number of parallel threads to use for each crawl.
</dd>
<dt>
<span class="term">
<code class="literal">connector.crawler.http.connection_timeout</code>
</span>
</dt>
<dd>
The maximum period (in seconds) to wait until abortion of the request when a connection is being initiated.
</dd>
<dt>
<span class="term">
<code class="literal">connector.crawler.http.read_timeout</code>
</span>
</dt>
<dd>
The maximum period (in seconds)  of inactivity (in seconds)  between two data packets, before the request is aborted.
</dd>
<dt>
<span class="term">
<code class="literal">connector.crawler.http.request_timeout</code>
</span>
</dt>
<dd>
The maximum period (in seconds) of the entire request before the request is aborted.
</dd>
<dt>
<span class="term">
<code class="literal">connector.crawler.http.response_size.limit</code>
</span>
</dt>
<dd>
The maximum size of an HTTP response (in bytes) supported by the crawler.
</dd>
<dt>
<span class="term">
<code class="literal">connector.crawler.http.redirects.limit</code>
</span>
</dt>
<dd>
The maximum number of HTTP redirects before a request is failed.
</dd>
<dt>
<span class="term">
<code class="literal">connector.crawler.extraction.title_size.limit</code>
</span>
</dt>
<dd>
The maximum size (in bytes) of the title field extracted from crawled pages.
</dd>
<dt>
<span class="term">
<code class="literal">connector.crawler.extraction.body_size.limit</code>
</span>
</dt>
<dd>
The maximum size (in bytes) of the body field extracted from crawled pages.
</dd>
<dt>
<span class="term">
<code class="literal">connector.crawler.extraction.keywords_size.limit</code>
</span>
</dt>
<dd>
The maximum size (in bytes) of the keywords field extracted from crawled pages.
</dd>
<dt>
<span class="term">
<code class="literal">connector.crawler.extraction.description_size.limit</code>
</span>
</dt>
<dd>
The maximum size (in bytes) of the description field extracted from crawled pages.
</dd>
<dt>
<span class="term">
<code class="literal">connector.crawler.extraction.extracted_links_count.limit</code>
</span>
</dt>
<dd>
The maximum number of links extracted from each page for further crawling.
</dd>
<dt>
<span class="term">
<code class="literal">connector.crawler.extraction.indexed_links_count.limit</code>
</span>
</dt>
<dd>
The maximum number of links extracted from each page and indexed in a document.
</dd>
<dt>
<span class="term">
<code class="literal">connector.crawler.extraction.headings_count.limit</code>
</span>
</dt>
<dd>
The maximum number of HTML headings (h1-h5 tags) to be extracted from each page.
</dd>
<dt>
<span class="term">
<code class="literal">connector.crawler.crawl.max_duration.limit</code>
</span>
</dt>
<dd>
Maximum duration (in seconds) of a crawl before the crawler will shut down automatically.
</dd>
<dt>
<span class="term">
<code class="literal">connector.crawler.crawl.max_crawl_depth.limit</code>
</span>
</dt>
<dd>
How many sequential links should the crawler follow into the site before stopping new link discovery.
</dd>
<dt>
<span class="term">
<code class="literal">connector.crawler.crawl.max_url_length.limit</code>
</span>
</dt>
<dd>
Maximum length of a URL (in characters) the crawler is allowed to follow.
</dd>
<dt>
<span class="term">
<code class="literal">connector.crawler.crawl.max_url_segments.limit</code>
</span>
</dt>
<dd>
Maximum number of path segments in a URL allowed to be followed by the crawler (for example, <code class="literal">/a/b/c/d</code> has 4).
</dd>
<dt>
<span class="term">
<code class="literal">connector.crawler.crawl.max_url_params.limit</code>
</span>
</dt>
<dd>
Maximum number of query parameters in a URL allowed to be followed by the crawler (for example, <code class="literal">/a?b=c&amp;d=e</code> has 2).
</dd>
<dt>
<span class="term">
<code class="literal">connector.crawler.crawl.max_unique_url_count.limit</code>
</span>
</dt>
<dd>
Maximum number of unique URLs the crawler is allowed to index before it stops looking for new content and finishes the crawl automatically.
</dd>
</dl>
</div>
<p><span class="strong strong"><strong>For version 8.2 and later:</strong></span></p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">feature_flag.elasticsearch_search_api</code>
</span>
</dt>
<dd>
Enables the Elasticsearch Search API (Technical Preview). Use the API to pass through and execute raw Elasticsearch queries against the indices that power Enterprise Search engines.
</dd>
</dl>
</div>
<p><span class="strong strong"><strong>For version 8.0 and later:</strong></span></p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">crawler.security.auth.allow_http</code>
</span>
</dt>
<dd>
Allow authenticated crawling of non-HTTPS URLs.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.http.compression.enabled</code>
</span>
</dt>
<dd>
Enable/disable HTTP content (gzip/deflate) compression in App Search Crawler requests
</dd>
<dt>
<span class="term">
<code class="literal">crawler.http.default_encoding</code>
</span>
</dt>
<dd>
Default encoding used for responses that do not specify a charset.
</dd>
</dl>
</div>
<p><span class="strong strong"><strong>For version 8.3 and later:</strong></span></p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">crawler.content_extraction.enabled</code>
</span>
</dt>
<dd>
Enable extraction of non-HTML files found during a crawl.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.content_extraction.mime_types</code>
</span>
</dt>
<dd>
Extract files with these MIME types.
</dd>
</dl>
</div>
<p><span class="strong strong"><strong>For version 7.16 and later:</strong></span></p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">monitoring.reporting_enabled</code>
</span>
</dt>
<dd>
Enable automatic monitoring metrics reporting to Elasticsearch through metricbeat. Defaults to <code class="literal">false</code>.
</dd>
<dt>
<span class="term">
<code class="literal">monitoring.reporting_period</code>
</span>
</dt>
<dd>
Configure metrics reporting frequency. Defaults to <code class="literal">10s</code>.
</dd>
<dt>
<span class="term">
<code class="literal">monitoring.metricsets</code>
</span>
</dt>
<dd>
Configure metricsets to be reported to Elasticsearch. Defaults to all metricsets.
</dd>
<dt>
<span class="term">
<code class="literal">monitoring.index_prefix</code>
</span>
</dt>
<dd>
Override the index name prefix used to index Enterprise Search metrics. Defaults to <code class="literal">metricbeat-ent-search</code>.
</dd>
<dt>
<span class="term">
<code class="literal">workplace_search.content_source.salesforce.enable_cases</code>
</span>
</dt>
<dd>
Whether or not Salesforce and Salesforce Sandbox content sources should synchronize Case objects. Default is true.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.http.proxy.host</code>
</span>
</dt>
<dd>
HTTP proxy host to use for all web crawler connections.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.http.proxy.port</code>
</span>
</dt>
<dd>
HTTP proxy port to use for all web crawler connections.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.http.proxy.protocol</code>
</span>
</dt>
<dd>
The protocol to use for proxy connections (<code class="literal">http</code> or <code class="literal">https</code>). Defaults to <code class="literal">http</code>.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.http.proxy.username</code>
</span>
</dt>
<dd>
The username to use for proxy authentication.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.http.proxy.password</code>
</span>
</dt>
<dd>
The password to use for proxy authentication.
</dd>
</dl>
</div>
<p><span class="strong strong"><strong>For version 7.15 and later:</strong></span></p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">workplace_search.content_source.sync.refresh_interval.full</code>
</span>
</dt>
<dd>
The refresh interval for full sync job (in ISO 8601 Duration format). Defaults to <code class="literal">P3D</code>.
</dd>
<dt>
<span class="term">
<code class="literal">workplace_search.content_source.sync.refresh_interval.incremental</code>
</span>
</dt>
<dd>
The refresh interval for incremental sync job (in ISO 8601 Duration format). Defaults to <code class="literal">PT2H</code>.
</dd>
<dt>
<span class="term">
<code class="literal">workplace_search.content_source.sync.refresh_interval.delete</code>
</span>
</dt>
<dd>
The refresh interval for delete sync job (in ISO 8601 Duration format). Defaults to <code class="literal">PT6H</code>.
</dd>
<dt>
<span class="term">
<code class="literal">workplace_search.content_source.sync.refresh_interval.permissions</code>
</span>
</dt>
<dd>
The refresh interval for permissions sync job (in ISO 8601 Duration format). Defaults to <code class="literal">PT5M</code>.
</dd>
<dt>
<span class="term">
<code class="literal">workplace_search.remote_sources.query_timeout</code>
</span>
</dt>
<dd>
The timeout setting (in milliseconds) when querying from remote sources using the Search API. Defaults to <code class="literal">10000</code>.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.extraction.default_deduplication_fields</code>
</span>
</dt>
<dd>
A list of default fields to be used for content hashing (deduplication). Replaces the old <code class="literal">crawler.extraction.content_hash_include</code> setting.
</dd>
</dl>
</div>
<p><span class="strong strong"><strong>For version 7.14 and later:</strong></span></p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">crawler.security.ssl.certificate_authorities</code>
</span>
</dt>
<dd>
A list of additional custom SSL certificate authority certificates to be used during crawling. Only inline certificate values are supported.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.security.ssl.certificate_authorities</code>
</span>
</dt>
<dd>
SSL verification mode to be used for all Crawler connections: <code class="literal">full</code> (this is the default), <code class="literal">certificate</code> or <code class="literal">none</code>.
</dd>
<dt>
<span class="term">
<code class="literal">kibana.preferred_auth_provider</code>
</span>
</dt>
<dd>
The name of the authentication provider to use when redirecting to Kibana for authentication.
</dd>
<dt>
<span class="term">
<code class="literal">workplace_search.content_source.indexing.rules.limit</code>
</span>
</dt>
<dd>
Maximum allowed indexing rules a content source can have.
</dd>
<dt>
<span class="term">
<code class="literal">workplace_search.synonyms.sets.limit</code>
</span>
</dt>
<dd>
The maximum number of synonym sets for a Workplace Search instance. Defaults to <code class="literal">256</code>.
</dd>
<dt>
<span class="term">
<code class="literal">workplace_search.synonyms.terms_per_set.limit</code>
</span>
</dt>
<dd>
The maximum number of terms per indiviual synonym set for a Workplace Search instance. Defaults to <code class="literal">32</code>.
</dd>
</dl>
</div>
<p><span class="strong strong"><strong>For version 7.13 and later:</strong></span></p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">crawler.crawl.url_queue.url_count.limit</code>
</span>
</dt>
<dd>
The maximum size of the crawl frontier - the list of URLs the crawler needs to visit. The list is stored in Elasticsearch, so the limit could be increased as long as the Elasticsearch cluster has enough resources (disk space) to hold the queue index.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.extraction.content_hash_include</code>
</span>
</dt>
<dd>
A list of fields to be used for content hashing (deduplication).
</dd>
</dl>
</div>
<p><span class="strong strong"><strong>For version 7.12 and later:</strong></span></p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">workplace_search.content_source.document_size.limit</code>
</span>
</dt>
<dd>
Maximum allowed document size for a content source.
</dd>
<dt>
<span class="term">
<code class="literal">workplace_search.content_source.total_fields.limit</code>
</span>
</dt>
<dd>
Maximum number of fields a content source can have.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.http.user_agent</code>
</span>
</dt>
<dd>
Custom User-Agent header value to be used by the crawler. Replaces the old <code class="literal">crawler.user_agent</code> setting.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.workers.pool_size.limit</code>
</span>
</dt>
<dd>
The number of parallel crawls allowed per instance of Enterprise Search.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.crawl.threads.limit</code>
</span>
</dt>
<dd>
The number of parallel threads to use for each crawl.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.http.connection_timeout</code>
</span>
</dt>
<dd>
The maximum period (in seconds) to wait until abortion of the request when a connection is being initiated.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.http.read_timeout</code>
</span>
</dt>
<dd>
The maximum period (in seconds)  of inactivity (in seconds)  between two data packets, before the request is aborted.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.http.request_timeout</code>
</span>
</dt>
<dd>
The maximum period (in seconds) of the entire request before the request is aborted.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.http.response_size.limit</code>
</span>
</dt>
<dd>
The maximum size of an HTTP response (in bytes) supported by the crawler.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.http.redirects.limit</code>
</span>
</dt>
<dd>
The maximum number of HTTP redirects before a request is failed.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.extraction.title_size.limit</code>
</span>
</dt>
<dd>
The maximum size (in bytes) of the title field extracted from crawled pages.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.extraction.body_size.limit</code>
</span>
</dt>
<dd>
The maximum size (in bytes) of the body field extracted from crawled pages.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.extraction.keywords_size.limit</code>
</span>
</dt>
<dd>
The maximum size (in bytes) of the keywords field extracted from crawled pages.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.extraction.description_size.limit</code>
</span>
</dt>
<dd>
The maximum size (in bytes) of the description field extracted from crawled pages.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.extraction.extracted_links_count.limit</code>
</span>
</dt>
<dd>
The maximum number of links extracted from each page for further crawling.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.extraction.indexed_links_count.limit</code>
</span>
</dt>
<dd>
The maximum number of links extracted from each page and indexed in a document.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.extraction.headings_count.limit</code>
</span>
</dt>
<dd>
The maximum number of HTML headings (h1-h5 tags) to be extracted from each page.
</dd>
</dl>
</div>
<p><span class="strong strong"><strong>For version 7.11 and later:</strong></span></p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">crawler.user_agent</code>
</span>
</dt>
<dd>
Custom User-Agent header value to be used by the crawler.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.crawl.max_duration.limit</code>
</span>
</dt>
<dd>
Maximum duration (in seconds) of a crawl before the crawler will shut down automatically.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.crawl.max_crawl_depth.limit</code>
</span>
</dt>
<dd>
How many sequential links should the crawler follow into the site before stopping new link discovery.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.crawl.max_url_length.limit</code>
</span>
</dt>
<dd>
Maximum length of a URL (in characters) the crawler is allowed to follow.
</dd>
<dt>
<span class="term">
<code class="literal">crawler.crawl.max_url_segments.limit</code>
</span>
</dt>
<dd>
Maximum number of path segments in a URL allowed to be followed by the crawler (for example, <code class="literal">/a/b/c/d</code> has 4).
</dd>
<dt>
<span class="term">
<code class="literal">crawler.crawl.max_url_params.limit</code>
</span>
</dt>
<dd>
Maximum number of query parameters in a URL allowed to be followed by the crawler (for example, <code class="literal">/a?b=c&amp;d=e</code> has 2).
</dd>
<dt>
<span class="term">
<code class="literal">crawler.crawl.max_unique_url_count.limit</code>
</span>
</dt>
<dd>
Maximum number of unique URLs the crawler is allowed to index before it stops looking for new content and finishes the crawl automatically.
</dd>
<dt>
<span class="term">
<code class="literal">workplace_search.content_source.sync.thumbnails.enabled</code>
</span>
</dt>
<dd>
Whether or not a content source should generate thumbnails for the documents it syncs.
</dd>
</dl>
</div>
<p><span class="strong strong"><strong>For version 7.10 and later:</strong></span></p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">workplace_search.content_source.sync.max_errors</code>
</span>
</dt>
<dd>
If an indexing job encounters more total errors than this value, the job will fail.
</dd>
<dt>
<span class="term">
<code class="literal">workplace_search.content_source.sync.max_consecutive_errors</code>
</span>
</dt>
<dd>
If an indexing job encounters more errors in a row than this value, the job will fail.
</dd>
<dt>
<span class="term">
<code class="literal">workplace_search.content_source.sync.max_error_ratio</code>
</span>
</dt>
<dd>
If an indexing job encounters an error ratio greater than this value in a given window, or overall at the end of the job, the job will fail.
</dd>
<dt>
<span class="term">
<code class="literal">workplace_search.content_source.sync.error_ratio_window_size</code>
</span>
</dt>
<dd>
Configure how large of a window to consider when calculating an error ratio (check <code class="literal">workplace_search.content_source.sync.max_error_ratio</code>).
</dd>
</dl>
</div>
<p><span class="strong strong"><strong>For version 7.9 and later:</strong></span></p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">ent_search.auth.&lt;auth_name&gt;.source</code>
</span>
</dt>
<dd>
(7.9.2+) The origin of authenticated Enterprise Search users. <code class="literal">&lt;auth_name&gt;</code> is the name of the SAML realm as defined in Elasticsearch. Set this to <code class="literal">elasticsearch-saml</code> if you are adding your own custom SAML realm. Otherwise, leave this unset.
</dd>
<dt>
<span class="term">
<code class="literal">ent_search.auth.&lt;auth_name&gt;.order</code>
</span>
</dt>
<dd>
<p>
(7.9.2+) The order in which to display this provider on the login screen. <code class="literal">&lt;auth_name&gt;</code> is the name of the SAML realm as defined in Elasticsearch.
</p>
<p><span class="strong strong"><strong>Required</strong></span> when <code class="literal">ent_search.auth.&lt;auth_name&gt;.source</code> is set.</p>
</dd>
<dt>
<span class="term">
<code class="literal">ent_search.auth.&lt;auth_name&gt;.description</code>
</span>
</dt>
<dd>
<p>
(7.9.2+) The name to be displayed on the login screen associated with this provider. <code class="literal">&lt;auth_name&gt;</code> is the name of the SAML realm as defined in Elasticsearch.
</p>
<p><span class="strong strong"><strong>Required</strong></span> when <code class="literal">ent_search.auth.&lt;auth_name&gt;.source</code> is set.</p>
</dd>
<dt>
<span class="term">
<code class="literal">ent_search.auth.&lt;auth_name&gt;.icon</code>
</span>
</dt>
<dd>
<p>
(7.9.2+) The URL to an icon to be displayed on the login screen associated with this provider. <code class="literal">&lt;auth_name&gt;</code> is the name of the SAML realm as defined in Elasticsearch.
</p>
<p><span class="strong strong"><strong>Required</strong></span> when <code class="literal">ent_search.auth.&lt;auth_name&gt;.source</code> is set.</p>
</dd>
<dt>
<span class="term">
<code class="literal">ent_search.auth.&lt;auth_name&gt;.hidden</code>
</span>
</dt>
<dd>
(7.9.2+) Boolean value to determine whether or not to display this login option on the login screen. <code class="literal">&lt;auth_name&gt;</code> is the name of the SAML realm as defined in Elasticsearch. Optional when <code class="literal">ent_search.auth.&lt;auth_name&gt;.source</code> is set.
</dd>
<dt>
<span class="term">
<code class="literal">ent_search.login_assistance_message</code>
</span>
</dt>
<dd>
(7.9-7.17) Adds a message to the login screen. This field supports Markdown.
</dd>
</dl>
</div>
<p><span class="strong strong"><strong>For version 7.7 and later:</strong></span></p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">ent_search.auth.source</code>
</span>
</dt>
<dd>
(7.7-7.9.1) The origin of authenticated Enterprise Search users. Set this to <code class="literal">elasticsearch-saml</code> if you are adding your own custom SAML realm. Otherwise, leave this unset.
</dd>
<dt>
<span class="term">
<code class="literal">ent_search.auth.name</code>
</span>
</dt>
<dd>
(7.7-7.9.1) The name of your SAML authentication realm as defined in Elasticsearch. Required when <code class="literal">ent_search.auth.source</code> is set.
</dd>
<dt>
<span class="term">
<code class="literal">workplace_search.custom_api_source.document_size.limit</code>
</span>
</dt>
<dd>
Maximum allowed document size for Custom API Sources (in bytes).
</dd>
<dt>
<span class="term">
<code class="literal">workplace_search.custom_api_source.total_fields.limit</code>
</span>
</dt>
<dd>
Number of fields a Custom API Source can have.
</dd>
<dt>
<span class="term">
<code class="literal">app_search.engine.document_size.limit</code>
</span>
</dt>
<dd>
Maximum allowed document size in App Search engines (in bytes).
</dd>
<dt>
<span class="term">
<code class="literal">app_search.engine.total_fields.limit</code>
</span>
</dt>
<dd>
Number of fields an App Search engine can have.
</dd>
<dt>
<span class="term">
<code class="literal">app_search.engine.source_engines_per_meta_engine.limit</code>
</span>
</dt>
<dd>
Number of source engines a meta engine can have.
</dd>
<dt>
<span class="term">
<code class="literal">app_search.engine.total_facet_values_returned.limit</code>
</span>
</dt>
<dd>
Number of facet values that can be returned by a search.
</dd>
<dt>
<span class="term">
<code class="literal">app_search.engine.query.limit</code>
</span>
</dt>
<dd>
Size of full-text queries that are allowed.
</dd>
<dt>
<span class="term">
<code class="literal">app_search.engine.synonyms.sets.limit</code>
</span>
</dt>
<dd>
Total number of synonym sets an engine can have.
</dd>
<dt>
<span class="term">
<code class="literal">app_search.engine.synonyms.terms_per_set.limit</code>
</span>
</dt>
<dd>
Total number of terms a synonym set can have.
</dd>
<dt>
<span class="term">
<code class="literal">app_search.engine.analytics.total_tags.limit</code>
</span>
</dt>
<dd>
Number of analytics tags that can be associated with a single query or clickthrough.
</dd>
<dt>
<span class="term">
<code class="literal">log_level</code>
</span>
</dt>
<dd>
Minimum log level. One of <em>debug</em>, <em>info</em>, <em>warn</em>, <em>error</em>, <em>fatal</em> or <em>unknown</em>. Defaults to <em>info</em>.
</dd>
</dl>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>To change log level you must first <a class="xref" href="ec-enable-logging-and-monitoring.html" title="Enable logging and monitoring">enable deployment logging</a>.</p>
</div>
</div>
</div>
<div class="navfooter">
<span class="prev">
<a href="ec-manage-appsearch-settings.html">« Add App Search user settings</a>
</span>
<span class="next">
<a href="ec-manage-deployment.html">Manage your deployment »</a>
</span>
</div>
</div>
</body>
</html>
