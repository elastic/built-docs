<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Important Configuration Changes | Elasticsearch: The Definitive Guide [2.x] | Elastic</title>
<link rel="home" href="index.html" title="Elasticsearch: The Definitive Guide [2.x]"/>
<link rel="up" href="deploy.html" title="Production Deployment"/>
<link rel="prev" href="_configuration_management.html" title="Configuration Management"/>
<link rel="next" href="_don_8217_t_touch_these_settings.html" title="Don&#8217;t Touch These Settings!"/>
<meta name="DC.type" content="Learn/Docs/Legacy/Elasticsearch/Definitive Guide/2.x"/>
<meta name="DC.subject" content="Elasticsearch"/>
<meta name="DC.identifier" content="2.x"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body><div class="page_header">
This information applies to version 2.x of Elasticsearch. For the
most up to date information, see the current version of the
<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html">
Elasticsearch Reference</a>.
</div>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="index.html">Elasticsearch: The Definitive Guide [2.x]</a></span>
»
<span class="breadcrumb-link"><a href="administration.html">Administration, Monitoring, and Deployment</a></span>
»
<span class="breadcrumb-link"><a href="deploy.html">Production Deployment</a></span>
»
<span class="breadcrumb-node">Important Configuration Changes</span>
</div>
<div class="navheader">
<span class="prev">
<a href="_configuration_management.html">« Configuration Management</a>
</span>
<span class="next">
<a href="_don_8217_t_touch_these_settings.html">Don&#8217;t Touch These Settings! »</a>
</span>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="important-configuration-changes"></a>Important Configuration Changes<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-definitive-guide/edit/2.x/510_Deployment/40_config.asciidoc">edit</a></h2>
</div></div></div>
<p>Elasticsearch ships with <em>very good</em> defaults, especially when it comes to performance-
related settings and options.  When in doubt, just leave
the settings alone.  We have witnessed countless dozens of clusters ruined
by errant settings because the administrator thought he could turn a knob
and gain 100-fold improvement.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>Please read this entire section!  All configurations presented are equally
important, and are not listed in any particular order.  Please read
through all configuration options and apply them to your cluster.</p>
</div>
</div>
<p>Other databases may require tuning, but by and large, Elasticsearch does not.
If you are hitting performance problems, the solution is usually better data
layout or more nodes.  There are very few "magic knobs" in Elasticsearch.
If there were, we&#8217;d have turned them already!</p>
<p>With that said, there are some <em>logistical</em> configurations that should be changed
for production.  These changes are necessary either to make your life easier, or because
there is no way to set a good default (because it depends on your cluster layout).</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="_assign_names"></a>Assign Names<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-definitive-guide/edit/2.x/510_Deployment/40_config.asciidoc">edit</a></h3>
</div></div></div>
<p>Elasticseach by default starts a cluster named <code class="literal">elasticsearch</code>.  It is wise
to rename your production cluster to something else, simply to prevent accidents
whereby someone&#8217;s laptop joins the cluster.  A simple change to <code class="literal">elasticsearch_production</code>
can save a lot of heartache.</p>
<p>This can be changed in your <code class="literal">elasticsearch.yml</code> file:</p>
<div class="pre_wrapper lang-yaml">
<pre class="programlisting prettyprint lang-yaml">cluster.name: elasticsearch_production</pre>
</div>
<p>Similarly, it is wise to change the names of your nodes. As you&#8217;ve probably
noticed by now, Elasticsearch assigns a random Marvel superhero name
to your nodes at startup.  This is cute in development&#8212;&#8203;but less cute when it is
3a.m. and you are trying to remember which physical machine was Tagak the Leopard Lord.</p>
<p>More important, since these names are generated on startup, each time you
restart your node, it will get a new name. This can make logs confusing,
since the names of all the nodes are constantly changing.</p>
<p>Boring as it might be, we recommend you give each node a name that makes sense
to you&#8212;&#8203;a plain, descriptive name.  This is also configured in your <code class="literal">elasticsearch.yml</code>:</p>
<div class="pre_wrapper lang-yaml">
<pre class="programlisting prettyprint lang-yaml">node.name: elasticsearch_005_data</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="_paths"></a>Paths<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-definitive-guide/edit/2.x/510_Deployment/40_config.asciidoc">edit</a></h3>
</div></div></div>
<p>By default, Elasticsearch will place the plug-ins,
 logs, and&#8212;&#8203;most important&#8212;&#8203;your data in the installation directory.  This can lead to
unfortunate accidents, whereby the installation directory is accidentally overwritten
by a new installation of Elasticsearch. If you aren&#8217;t careful, you can erase all your data.</p>
<p>Don&#8217;t laugh&#8212;&#8203;we&#8217;ve seen it happen more than a few times.</p>
<p>The best thing to do is relocate your data directory outside the installation
location.  You can optionally move your plug-in and log directories as well.</p>
<p>This can be changed as follows:</p>
<div class="pre_wrapper lang-yaml">
<pre class="programlisting prettyprint lang-yaml">path.data: /path/to/data1,/path/to/data2 <a id="CO301-1"></a><i class="conum" data-value="1"></i>

# Path to log files:
path.logs: /path/to/logs

# Path to where plugins are installed:
path.plugins: /path/to/plugins</pre>
</div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO301-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>Notice that you can specify more than one directory for data by using comma-separated lists.</p>
</td>
</tr>
</table>
</div>
<p>Data can be saved to multiple directories, and if each directory
is mounted on a different hard drive, this is a simple and effective way to
set up a software RAID 0.  Elasticsearch will automatically stripe
data between the different directories, boosting performance.</p>
<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<h3>Multiple data path safety and performance</h3>
<p>Like any RAID 0 configuration, only a single copy of your data is saved to the
hard drives.  If you lose a hard drive, you are <em>guaranteed</em> to lose a portion
of your data on that machine.  With luck you&#8217;ll have replicas elsewhere in the
cluster which can recover the data, and/or a recent <a class="xref" href="backing-up-your-cluster.html" title="Backing Up Your Cluster">backup</a>.</p>
<p>Elasticsearch attempts to minimize the extent of data loss by striping entire
shards to a drive.  That means that <code class="literal">Shard 0</code> will be placed entirely on a single
drive. Elasticsearch will not stripe a shard across multiple drives, since the
loss of one drive would corrupt the entire shard.</p>
<p>This has ramifications for performance: if you are adding multiple drives
to improve the performance of a single index, it is unlikely to help since
most nodes will only have one shard, and thus one active drive.  Multiple data
paths only helps if you have many indices/shards on a single node.</p>
<p>Multiple data paths is a nice convenience feature, but at the end of the day,
Elasticsearch is not a software RAID package. If you need more advanced configuration,
robustness and flexibility, we encourage you to use actual software RAID packages
instead of the multiple data path feature.</p>
</div>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="_minimum_master_nodes"></a>Minimum Master Nodes<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-definitive-guide/edit/2.x/510_Deployment/40_config.asciidoc">edit</a></h3>
</div></div></div>
<p>The <code class="literal">minimum_master_nodes</code> setting is <em>extremely</em> important to the
stability of your cluster.  This setting helps prevent <em>split brains</em>, the existence of two masters in a single cluster.</p>
<p>When you have a split brain, your cluster is at danger of losing data.  Because
the master is considered the supreme ruler of the cluster, it decides
when new indices can be created, how shards are moved, and so forth.  If you have <em>two</em>
masters, data integrity becomes perilous, since you have two nodes
that think they are in charge.</p>
<p>This setting tells Elasticsearch to not elect a master unless there are enough
master-eligible nodes available.  Only then will an election take place.</p>
<p>This setting should always be configured to a quorum (majority) of your master-eligible nodes.  A quorum is <code class="literal">(number of master-eligible nodes / 2) + 1</code>.
Here are some examples:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
If you have ten regular nodes (can hold data, can become master), a quorum is
<code class="literal">6</code>.
</li>
<li class="listitem">
If you have three dedicated master nodes and a hundred data nodes, the quorum is <code class="literal">2</code>,
since you need to count only nodes that are master eligible.
</li>
<li class="listitem">
If you have two regular nodes, you are in a conundrum.  A quorum would be
<code class="literal">2</code>, but this means a loss of one node will make your cluster inoperable.  A
setting of <code class="literal">1</code> will allow your cluster to function, but doesn&#8217;t protect against
split brain.  It is best to have a minimum of three nodes in situations like this.
</li>
</ul>
</div>
<p>This setting can be configured in your <code class="literal">elasticsearch.yml</code> file:</p>
<div class="pre_wrapper lang-yaml">
<pre class="programlisting prettyprint lang-yaml">discovery.zen.minimum_master_nodes: 2</pre>
</div>
<p>But because Elasticsearch clusters are dynamic, you could easily add or remove
nodes that will change the quorum.  It would be extremely irritating if you had
to push new configurations to each node and restart your whole cluster just to
change the setting.</p>
<p>For this reason, <code class="literal">minimum_master_nodes</code> (and other settings) can be configured
via a dynamic API call.  You can change the setting while your cluster is online:</p>
<div class="pre_wrapper lang-js">
<pre class="programlisting prettyprint lang-js">PUT /_cluster/settings
{
    "persistent" : {
        "discovery.zen.minimum_master_nodes" : 2
    }
}</pre>
</div>
<p>This will become a persistent setting that takes precedence over whatever is
in the static configuration.  You should modify this setting whenever you add or
remove master-eligible nodes.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="_recovery_settings"></a>Recovery Settings<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-definitive-guide/edit/2.x/510_Deployment/40_config.asciidoc">edit</a></h3>
</div></div></div>
<p>Several settings affect the behavior of shard recovery when
your cluster restarts.  First, we need to understand what happens if nothing is
configured.</p>
<p>Imagine you have ten nodes, and each node holds a single shard&#8212;&#8203;either a primary
or a replica&#8212;&#8203;in a 5 primary / 1 replica index.  You take your
entire cluster offline for maintenance (installing new drives, for example).  When you
restart your cluster, it just so happens that five nodes come online before
the other five.</p>
<p>Maybe the switch to the other five is being flaky, and they didn&#8217;t
receive the restart command right away.  Whatever the reason, you have five nodes
online.  These five nodes will gossip with each other, elect a master, and form a
cluster.  They notice that data is no longer evenly distributed, since five
nodes are missing from the cluster, and immediately start replicating new
shards between each other.</p>
<p>Finally, your other five nodes turn on and join the cluster.  These nodes see
that <em>their</em> data is being replicated to other nodes, so they delete their local
data (since it is now redundant, and may be outdated).  Then the cluster starts
to rebalance even more, since the cluster size just went from five to ten.</p>
<p>During this whole process, your nodes are thrashing the disk and network, moving
data around&#8212;&#8203;for no good reason. For large clusters with terabytes of data,
this useless shuffling of data can take a <em>really long time</em>.  If all the nodes
had simply waited for the cluster to come online, all the data would have been
local and nothing would need to move.</p>
<p>Now that we know the problem, we can configure a few settings to alleviate it.
First, we need to give Elasticsearch a hard limit:</p>
<div class="pre_wrapper lang-yaml">
<pre class="programlisting prettyprint lang-yaml">gateway.recover_after_nodes: 8</pre>
</div>
<p>This will prevent Elasticsearch from starting a recovery until at least eight (data or master) nodes
are present.  The value for this setting is a matter of personal preference: how
many nodes do you want present before you consider your cluster functional?
In this case, we are setting it to <code class="literal">8</code>, which means the cluster is inoperable
unless there are at least eight nodes.</p>
<p>Then we tell Elasticsearch how many nodes <em>should</em> be in the cluster, and how
long we want to wait for all those nodes:</p>
<div class="pre_wrapper lang-yaml">
<pre class="programlisting prettyprint lang-yaml">gateway.expected_nodes: 10
gateway.recover_after_time: 5m</pre>
</div>
<p>What this means is that Elasticsearch will do the following:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Wait for eight nodes to be present
</li>
<li class="listitem">
Begin recovering after 5 minutes <em>or</em> after ten nodes have joined the cluster,
whichever comes first.
</li>
</ul>
</div>
<p>These three settings allow you to avoid the excessive shard swapping that can
occur on cluster restarts.  It can literally make recovery take seconds instead
of hours.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>These settings can only be set in the <code class="literal">config/elasticsearch.yml</code> file or on
the command line (they are not dynamically updatable) and they are only relevant
during a full cluster restart.</p>
</div>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="unicast"></a>Prefer Unicast over Multicast<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-definitive-guide/edit/2.x/510_Deployment/40_config.asciidoc">edit</a></h3>
</div></div></div>
<p>Elasticsearch is configured to use unicast discovery out of the box to prevent
nodes from accidentally joining a cluster. Only nodes running on the same
machine will automatically form cluster.</p>
<p>While multicast is still <a href="/guide/en/elasticsearch/plugins/current/discovery-multicast.html" class="ulink" target="_top">provided
as a plugin</a>, it should never be used in production. The
last thing you want is for nodes to accidentally join your production network, simply
because they received an errant multicast ping.  There is nothing wrong with
multicast <em>per se</em>.  Multicast simply leads to silly problems, and can be a bit
more fragile (for example, a network engineer fiddles with the network without telling
you&#8212;&#8203;and all of a sudden nodes can&#8217;t find each other anymore).</p>
<p>To use unicast, you provide Elasticsearch a list of nodes that it should try to contact.
When a node contacts a member of the unicast list, it receives a full cluster
state that lists all of the nodes in the cluster.  It then contacts
the master and joins the cluster.</p>
<p>This means your unicast list does not need to include all of the nodes in your cluster.
It just needs enough nodes that a new node can find someone to talk to.  If you
use dedicated masters, just list your three dedicated masters and call it a day.
This setting is configured in <code class="literal">elasticsearch.yml</code>:</p>
<div class="pre_wrapper lang-yaml">
<pre class="programlisting prettyprint lang-yaml">discovery.zen.ping.unicast.hosts: ["host1", "host2:port"]</pre>
</div>
<p>For more information about how Elasticsearch nodes find each other, see
<a href="/guide/en/elasticsearch/reference/2.4/modules-discovery.html" class="ulink" target="_top">Discovery and cluster formation</a>
in the Elasticsearch Reference.</p>
</div>

</div>
<div class="navfooter">
<span class="prev">
<a href="_configuration_management.html">« Configuration Management</a>
</span>
<span class="next">
<a href="_don_8217_t_touch_these_settings.html">Don&#8217;t Touch These Settings! »</a>
</span>
</div>
</div>
</body>
</html>
