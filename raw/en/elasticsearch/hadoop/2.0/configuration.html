<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="description" content="Reference documentation of elasticsearch-hadoop">
<title>Configuration | Elasticsearch for Apache Hadoop [2.0] | Elastic</title>
<meta class="elastic" name="content" content="Configuration | Elasticsearch for Apache Hadoop [2.0]">

<link rel="home" href="index.html" title="Elasticsearch for Apache Hadoop [2.0]"/>
<link rel="up" href="reference.html" title="Reference"/>
<link rel="prev" href="arch.html" title="Architecture"/>
<link rel="next" href="configuration-runtime.html" title="Hadoop runtime options"/>
<meta class="elastic" name="product_version" content="2.0"/>
<meta class="elastic" name="product_name" content="Elasticsearch"/>
<meta class="elastic" name="website_area" content="documentation"/>
<meta name="DC.type" content="Learn/Docs/Elasticsearch/Apache Hadoop/2.0"/>
<meta name="DC.subject" content="Elasticsearch"/>
<meta name="DC.identifier" content="2.0"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body>
<div class="page_header">
<p>
  <strong>WARNING</strong>: Version 2.0 has passed its 
  <a href="https://www.elastic.co/support/eol">EOL date</a>. 
</p>  
<p>
  This documentation is no longer being maintained and may be removed. 
  If you are running this version, we strongly advise you to upgrade. 
  For the latest information, see the 
  <a href="../current/index.html">current release documentation</a>. 
</p>
</div>
<div class="navheader">
<span class="prev">
<a href="arch.html">« Architecture</a>
</span>
<span class="next">
<a href="configuration-runtime.html">Hadoop runtime options »</a>
</span>
</div>
<div class="book" lang="en">
<div class="titlepage">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="index.html">Elasticsearch for Apache Hadoop [2.0]</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="reference.html">Reference</a></span>
</div>
<div>
<div><h1 class="title"><a id="id-1"></a>Configuration</h1><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/2.0/docs/src/reference/asciidoc/core/configuration.adoc">edit</a></div>
</div>
<!--EXTRA-->
</div>
<div id="content">
<div id="url-to-v3" class="version-warning">
    <strong>IMPORTANT</strong>: This documentation is no longer updated. Refer to <a href="https://www.elastic.co/support/eol">Elastic's version policy</a> and the <a href="https://www.elastic.co/docs/reference/elasticsearch-hadoop/configuration">latest documentation</a>.
</div>
<div class="chapter">
<div class="titlepage"><div><div>
<div class="position-relative"><h2 class="title"><a id="configuration"></a>Configuration</h2><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/2.0/docs/src/reference/asciidoc/core/configuration.adoc">edit</a></div>
</div></div></div>
<p>elasticsearch-hadoop behavior can be customized through the properties below, typically by setting them on the target job Hadoop <code class="literal">Configuration</code>. However some of them can be specified through other means depending on the library used (see the relevant section).</p>
<div class="sidebar">
<div class="titlepage"></div>
<p>elasticsearch-hadoop uses the same conventions and reasonable defaults as Elasticsearch so you can give it a try out without bothering with the configuration. Most of the time, these defaults are just fine for running a production cluster; if you are fine-tunning your cluster or wondering about the effect of certain configuration option, please <em>do ask</em> for more information.</p>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>All configuration properties start with the <code class="literal">es</code> prefix. The namespace <code class="literal">es.internal</code> is reserved by the library for its internal use and should <em>not</em> be used by the user at any point.</p>
</div>
</div>
<div class="position-relative"><h3><a id="_required_settings"></a>Required settings</h3><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/2.0/docs/src/reference/asciidoc/core/configuration.adoc">edit</a></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">es.resource</code>
</span>
</dt>
<dd>
Elasticsearch resource location, where data is read <em>and</em> written to. Requires the format <code class="literal">&lt;index&gt;/&lt;type&gt;</code> (relative to the Elasticsearch host/port (see <a class="xref" href="configuration.html#cfg-network" title="Network">below</a>))).
</dd>
</dl>
</div>
<div class="pre_wrapper lang-ini">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-ini">es.resource = twitter/tweet   # index 'twitter', type 'tweet'</pre>
</div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">es.resource.read</code> (defaults to <code class="literal">es.resource</code>)
</span>
</dt>
<dd>
Elasticsearch resource used for reading (but not writing) data. Useful when reading and writing data to different Elasticsearch indices within the <em>same</em> job. Typically set automatically (expect for the Map/Reduce module which requires manual configuration).
</dd>
<dt>
<span class="term">
<code class="literal">es.resource.write</code>(defaults to <code class="literal">es.resource</code>)
</span>
</dt>
<dd>
Elasticsearch resource used for writing (but not reading) data. Used typically for <em>dynamic resource</em> writes or when writing and reading data to different Elasticsearch indices within the <em>same</em> job. Typically set automatically (expect for the Map/Reduce module which requires manual configuration).
</dd>
</dl>
</div>
<div class="position-relative"><h5><a id="cfg-multi-writes"></a>Dynamic/multi resource writes</h5><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/2.0/docs/src/reference/asciidoc/core/configuration.adoc">edit</a></div>
<p>For writing, elasticsearch-hadoop allows the target resource to be resolved at runtime by using patterns (by using the <code class="literal">{&lt;field-name&gt;}</code> format), resolved at runtime based on the data being streamed to Elasticsearch. That is, one can save documents to a certain <code class="literal">index</code> or <code class="literal">type</code> based on one or multiple fields resolved from the document about to be saved.</p>
<p>For example, assuming the following document set (described here in JSON for readability - feel free to translate this into the actual Java objects):</p>
<div class="pre_wrapper lang-json">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-json">{
    "media_type":"game",
    "title":"Final Fantasy VI",
    "year":"1994"
},
{
    "media_type":"book",
    "title":"Harry Potter",
    "year":"2010"
},
{
    "media_type":"music",
    "title":"Surfing With The Alien",
    "year":"1987"
}</pre>
</div>
<p>to index each of them based on their <code class="literal">media_type</code> one would use the following pattern:</p>
<div class="pre_wrapper lang-ini">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-ini"># index the documents based on their type
es.resource.write = my-collection/{media_type}</pre>
</div>
<p>which would result in <code class="literal">Final Fantasy VI</code> indexed under <code class="literal">my-collection/game</code>, <code class="literal">Harry Potter</code> under <code class="literal">my-collection/book</code> and <code class="literal">Surfing With The Alien</code> under <code class="literal">my-collection/music</code>.
For more information, please refer to the dedicated integration section.</p>
<div class="important admon">
<div class="icon"></div>
<div class="admon_content">
<p>Dynamic resources are supported <em>only</em> for writing, for doing multi-index/types reads, use an appropriate <a href="http://www.elastic.co/guide/en/elasticsearch/reference/1.7/search-search.html" class="ulink" target="_top">search query</a>.</p>
</div>
</div>
<div class="position-relative"><h5><a id="cfg-multi-writes-format"></a>Formatting dynamic/multi resource writes</h5><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/2.0/docs/src/reference/asciidoc/core/configuration.adoc">edit</a></div>
<p>When using dynamic/multi writes, one can also specify a formatting of the value returned by the field. Out of the box, elasticsearch-hadoop provides formatting for date/timestamp fields which is useful for automatically grouping time-based data (such as logs)
 within a certain time range under the same index. By using the Java <a href="http://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html" class="ulink" target="_top">SimpleDataFormat</a> syntax, one can format and parse the date in a locale-sensitive manager.</p>
<p>For example assuming the data contains a <code class="literal">@timestamp</code> field, one can group the documents in <em>daily</em> indices using the following configuration:</p>
<div class="pre_wrapper lang-ini">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-ini"># index the documents based on their date
es.resource.write = my-collection/{@timestamp:YYYY.MM.dd} <a id="CO7-1"></a><i class="conum" data-value="1"></i></pre>
</div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO7-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p><code class="literal">@timestamp</code> field formatting - in this case <code class="literal">YYYY.MM.dd</code></p>
</td>
</tr>
</table>
</div>
<p>The same configuration property is used (<code class="literal">es.resource.write</code>) however, through the special <code class="literal">:</code> characters a formatting pattern is specified.
Please refer to the <a href="http://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html" class="ulink" target="_top">SimpleDataFormat</a> javadocs for more information on the syntax supported.
In this case <code class="literal">YYYY.MM.dd</code> translates the date into the year (specified by four digits), month by 2 digits followed by the day by two digits (such as <code class="literal">2015.01.28</code>).</p>
<p><a href="http://logstash.net/" class="ulink" target="_top">Logstash</a> users will find this <em>pattern</em> quite <a href="http://logstash.net/docs/latest/filters/date" class="ulink" target="_top">familiar</a>.</p>
<div class="position-relative"><h3><a id="_essential_settings"></a>Essential settings</h3><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/2.0/docs/src/reference/asciidoc/core/configuration.adoc">edit</a></div>
<div class="position-relative"><h4><a id="cfg-network"></a>Network</h4><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/2.0/docs/src/reference/asciidoc/core/configuration.adoc">edit</a></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">es.nodes</code> (default localhost)
</span>
</dt>
<dd>
List of Elasticsearch nodes to connect to. When using Elasticsearch remotely, <em>do</em> set this option. Note that the list does <em>not</em> have to contain <em>every</em> node inside the Elasticsearch cluster; these are discovered automatically by elasticsearch-hadoop by default (see below). Each node can have its HTTP/REST port specified manually (e.g. <code class="literal">mynode:9600</code>).
</dd>
<dt>
<span class="term">
<code class="literal">es.port</code> (default 9200)
</span>
</dt>
<dd>
Default HTTP/REST port used for connecting to Elasticsearch - this setting is applied to the nodes in <code class="literal">es.nodes</code> that do not have any port specified.
</dd>
</dl>
</div>
<div class="position-relative"><h4><a id="_querying"></a>Querying</h4><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/2.0/docs/src/reference/asciidoc/core/configuration.adoc">edit</a></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">es.query</code> (default none)
</span>
</dt>
<dd>
<p>
Holds the query used for reading data from the specified <code class="literal">es.resource</code>. By default it is not set/empty, meaning the entire data under the specified index/type is returned.
<code class="literal">es.query</code> can have three forms:
</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
uri query
</span>
</dt>
<dd>
using the form <code class="literal">?uri_query</code>, one can specify a <a href="http://www.elastic.co/guide/en/elasticsearch/reference/1.7/search-uri-request.html" class="ulink" target="_top">query string</a>. Notice the leading <code class="literal">?</code>.
</dd>
<dt>
<span class="term">
query dsl
</span>
</dt>
<dd>
using the form <code class="literal">query_dsl</code> - note the query dsl needs to start with <code class="literal">{</code> and end with <code class="literal">}</code> as mentioned <a href="http://www.elastic.co/guide/en/elasticsearch/reference/1.7/search-request-body.html" class="ulink" target="_top">here</a>
</dd>
<dt>
<span class="term">
external resource
</span>
</dt>
<dd>
if none of the the two above do match, elasticsearch-hadoop will try to interpret the parameter as a path within the HDFS file-system. If that is not the case, it will try to load the resource from the classpath or, if that fails, from the Hadoop <code class="literal">DistributedCache</code>. The resource should contain either a <code class="literal">uri query</code> or a <code class="literal">query dsl</code>.
</dd>
</dl>
</div>
</dd>
</dl>
</div>
<p>To wit, here is an example:</p>
<div class="pre_wrapper lang-ini">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-ini"># uri (or parameter) query
es.query = ?q=costinl

# query dsl
es.query = { "query" : { "term" : { "user" : "costinl" } } }

# external resource
es.query = org/mypackage/myquery.json</pre>
</div>
<p>In other words, <code class="literal">es.query</code> is flexible enough so that you can use whatever search api you prefer, either inline or by loading it from an external resource.</p>
<div class="tip admon">
<div class="icon"></div>
<div class="admon_content">
<p>We recommend using query dsl externalized in a file, included within the job jar (and thus available on its classpath). This makes it easy
to identify, debug and organize your queries.
Through-out the documentation we use the uri query to save text and increase readability - real-life queries quickly become unwielding when used as uris.</p>
</div>
</div>
<div class="position-relative"><h4><a id="_operation"></a>Operation</h4><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/2.0/docs/src/reference/asciidoc/core/configuration.adoc">edit</a></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">es.input.json</code> (default false)
</span>
</dt>
<dd>
Whether the input is already in JSON format or not (the default). Please see the appropriate section of each
integration for more details about using JSON directly.
</dd>
<dt>
<span class="term">
<code class="literal">es.write.operation</code> (default index)
</span>
</dt>
<dd>
<p>
The write operation elasticsearch-hadoop should peform - can be any of:
</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">index</code> (default)
</span>
</dt>
<dd>
new data is added while existing data (based on its id) is replaced (reindexed).
</dd>
<dt>
<span class="term">
<code class="literal">create</code>
</span>
</dt>
<dd>
adds new data - if the data already exists (based on its id), an exception is thrown.
</dd>
<dt>
<span class="term">
<code class="literal">update</code>
</span>
</dt>
<dd>
updates existing data (based on its id). If no data is found, an exception is thrown.
</dd>
<dt>
<span class="term">
<code class="literal">upsert</code>
</span>
</dt>
<dd>
known as <em>merge</em> or insert if the data does not exist, updates if the data exists (based on its id).
</dd>
</dl>
</div>
</dd>
</dl>
</div>
<div class="position-relative"><h4><a id="_mapping"></a>Mapping</h4><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/2.0/docs/src/reference/asciidoc/core/configuration.adoc">edit</a></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">es.mapping.id</code> (default none)
</span>
</dt>
<dd>
The document field/property name containing the document id.
</dd>
<dt>
<span class="term">
<code class="literal">es.mapping.parent</code> (default none)
</span>
</dt>
<dd>
The document field/property name containing the document parent. To specify a constant, use the <code class="literal">&lt;CONSTANT&gt;</code> format.
</dd>
<dt>
<span class="term">
<code class="literal">es.mapping.version</code> (default none)
</span>
</dt>
<dd>
The document field/property name containing the document version. To specify a constant, use the <code class="literal">&lt;CONSTANT&gt;</code> format.
</dd>
<dt>
<span class="term">
<code class="literal">es.mapping.version.type</code> (default depends on <code class="literal">es.mapping.version</code>)
</span>
</dt>
<dd>
Indicates the <a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/docs-index_.html#_version_types" class="ulink" target="_top">type of versioning</a> used.
If <code class="literal">es.mapping.version</code> is undefined (default), its value is unspecified. If <code class="literal">es.mapping.version</code> is specified, its value becomes <code class="literal">external</code>.
</dd>
<dt>
<span class="term">
<code class="literal">es.mapping.routing</code> (default none)
</span>
</dt>
<dd>
The document field/property name containing the document routing. To specify a constant, use the <code class="literal">&lt;CONSTANT&gt;</code> format.
</dd>
<dt>
<span class="term">
<code class="literal">es.mapping.ttl</code> (default none)
</span>
</dt>
<dd>
The document field/property name containing the document time-to-live. To specify a constant, use the <code class="literal">&lt;CONSTANT&gt;</code> format.
</dd>
<dt>
<span class="term">
<code class="literal">es.mapping.timestamp</code> (default none)
</span>
</dt>
<dd>
The document field/property name containing the document timestamp. To specify a constant, use the <code class="literal">&lt;CONSTANT&gt;</code> format.
</dd>
</dl>
</div>
<p>For example:</p>
<div class="pre_wrapper lang-ini">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-ini"># extracting the id from the field called 'uuid'
es.mapping.id = uuid

# specifying a parent with id '123'
es.mapping.parent = &lt;123&gt;</pre>
</div>
<div class="position-relative"><h4><a id="_update_settings"></a>Update settings</h4><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/2.0/docs/src/reference/asciidoc/core/configuration.adoc">edit</a></div>
<p>One using the <code class="literal">update</code> or <code class="literal">upsert</code> operation, additional settings (that mirror the <a href="http://www.elastic.co/guide/en/elasticsearch/reference/1.7/docs-update.html" class="ulink" target="_top">update</a> API) are available:
<code class="literal">es.update.script</code> (default none)::
Script used for updating the document.</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">es.update.script.lang</code> (default none)
</span>
</dt>
<dd>
Script language. By default, no value is specified applying the node configuration.
</dd>
<dt>
<span class="term">
<code class="literal">es.update.script.params</code> (default none)
</span>
</dt>
<dd>
Script parameters (if any). The document (currently read) field/property who&#8217;s value is used. To specify a constant, use the <code class="literal">&lt;CONSTANT&gt;</code> format.
Multiple values can be specified through commas (<code class="literal">,</code>)
</dd>
</dl>
</div>
<p>For example:</p>
<div class="pre_wrapper lang-ini">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-ini"># specifying 2 parameters, one extracting the value from field 'number', the other containing the value '123':
es.update.script.params = param1:number,param2:&lt;123&gt;</pre>
</div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">es.update.script.params.json</code>
</span>
</dt>
<dd>
Script parameters specified in <code class="literal">raw</code>, JSON format. The specified value is passed as is, without any further processing or filtering. Typically used for migrating existing update scripts.
</dd>
</dl>
</div>
<p>For example:</p>
<div class="pre_wrapper lang-ini">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-ini">es.update.script.params.json = {"param1":1, "param2":2}</pre>
</div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">es.update.retry.on.conflict</code> (default 0)
</span>
</dt>
<dd>
How many times an update to a document is retried in case of conflict. Useful in concurrent environments.
</dd>
</dl>
</div>
<div class="position-relative"><h3><a id="_advanced_settings"></a>Advanced settings</h3><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/2.0/docs/src/reference/asciidoc/core/configuration.adoc">edit</a></div>
<div class="position-relative"><h4><a id="configuration-options-index"></a>Index</h4><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/2.0/docs/src/reference/asciidoc/core/configuration.adoc">edit</a></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">es.index.auto.create</code> (default yes)
</span>
</dt>
<dd>
Whether elasticsearch-hadoop should create an index (if its missing) when writing data to Elasticsearch or fail.
</dd>
<dt>
<span class="term">
<code class="literal">es.index.read.missing.as.empty</code> (default no)
</span>
</dt>
<dd>
Whether elasticsearch-hadoop will allow reading of non existing indices (and return an empty data set) or not (and throw an exception)
</dd>
<dt>
<span class="term">
<code class="literal">es.field.read.empty.as.null</code> (default yes)
</span>
</dt>
<dd>
Whether elasticsearch-hadoop will treat empty fields as <code class="literal">null</code>. This settings is typically not needed (as elasticsearch-hadoop already handles the
null case) but is enabled for making it easier to work with text fields that haven&#8217;t been sanitized yet.
</dd>
<dt>
<span class="term">
<code class="literal">es.field.read.validate.presence</code> (default warn)
</span>
</dt>
<dd>
<p>
To help out spot possible mistakes when querying data from Hadoop (which results in incorrect data being returned), elasticsearch-hadoop can perform validation spotting missing fields and potential typos. Possible values are :
</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">ignore</code>
</span>
</dt>
<dd>
no validation is performed
</dd>
<dt>
<span class="term">
<code class="literal">warn</code>
</span>
</dt>
<dd>
a warning message is logged in case the validation fails
</dd>
<dt>
<span class="term">
<code class="literal">strict</code>
</span>
</dt>
<dd>
an exception is thrown, halting the job, if a field is missing
</dd>
</dl>
</div>
</dd>
</dl>
</div>
<p>The default (<code class="literal">warn</code>) will log any typos to the console when the job starts:</p>
<div class="pre_wrapper lang-bash">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-bash">WARN main mr.EsInputFormat - Field(s) [naem, adress] not found
   in the Elasticsearch mapping specified; did you mean [name, location.address]?</pre>
</div>
<div class="position-relative"><h4><a id="_network"></a>Network</h4><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/2.0/docs/src/reference/asciidoc/core/configuration.adoc">edit</a></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">es.nodes.discovery</code> (default true)
</span>
</dt>
<dd>
Whether to discovery the nodes within the Elasticsearch cluster or only to use the ones given in <code class="literal">es.nodes</code> for metadata queries. Note that when reading and writing, elasticsearch-hadoop uses the target index shards (and their hosting nodes), regardless of this setting.
</dd>
<dt>
<span class="term">
<code class="literal">es.http.timeout</code> (default 1m)
</span>
</dt>
<dd>
Timeout for HTTP/REST connections to Elasticsearch.
</dd>
<dt>
<span class="term">
<code class="literal">es.http.retries</code> (default 3)
</span>
</dt>
<dd>
Number of retries for establishing a (broken) http connection. The retries are applied for each <em>conversation</em> with an Elasticsearch node. Once the retries are depleted, the connection will automatically be re-reouted to the next
available Elasticsearch node (based on the declaration of <code class="literal">es.nodes</code>, followed by the discovered nodes - if enabled).
</dd>
<dt>
<span class="term">
<code class="literal">es.scroll.keepalive</code> (default 10m)
</span>
</dt>
<dd>
The maximum duration of result scrolls between query requests.
</dd>
<dt>
<span class="term">
<code class="literal">es.scroll.size</code> (default 50)
</span>
</dt>
<dd>
Number of results/items returned by each individual scroll.
</dd>
<dt>
<span class="term">
<code class="literal">es.action.heart.beat.lead</code> (default 15s)
</span>
</dt>
<dd>
The lead to task timeout before elasticsearch-hadoop informs Hadoop the task is still running to prevent task restart.
</dd>
</dl>
</div>
<div class="position-relative"><h4><a id="_proxy"></a>Proxy</h4><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/2.0/docs/src/reference/asciidoc/core/configuration.adoc">edit</a></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">es.net.proxy.http.host</code>
</span>
</dt>
<dd>
Http proxy host name
</dd>
<dt>
<span class="term">
<code class="literal">es.net.proxy.http.port</code>
</span>
</dt>
<dd>
Http proxy port
</dd>
<dt>
<span class="term">
<code class="literal">es.net.proxy.http.user</code>
</span>
</dt>
<dd>
Http proxy user name
</dd>
<dt>
<span class="term">
<code class="literal">es.net.proxy.http.pass</code>
</span>
</dt>
<dd>
Http proxy password
</dd>
<dt>
<span class="term">
<code class="literal">es.net.proxy.http.use.system.props</code>(default yes)
</span>
</dt>
<dd>
Whether the use the system Http proxy properties (namely <code class="literal">http.proxyHost</code> and <code class="literal">http.proxyPort</code>) or not
</dd>
<dt>
<span class="term">
<code class="literal">es.net.proxy.socks.host</code>
</span>
</dt>
<dd>
Http proxy host name
</dd>
<dt>
<span class="term">
<code class="literal">es.net.proxy.socks.port</code>
</span>
</dt>
<dd>
Http proxy port
</dd>
<dt>
<span class="term">
<code class="literal">es.net.proxy.socks.user</code>
</span>
</dt>
<dd>
Http proxy user name
</dd>
<dt>
<span class="term">
<code class="literal">es.net.proxy.socks.pass</code>
</span>
</dt>
<dd>
Http proxy password
</dd>
<dt>
<span class="term">
<code class="literal">es.net.proxy.socks.use.system.props</code>(default yes)
</span>
</dt>
<dd>
Whether the use the system Socks proxy properties (namely <code class="literal">socksProxyHost</code> and <code class="literal">socksProxyHost</code>) or not
</dd>
</dl>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>elasticsearch-hadoop allows proxy settings to be applied only to its connection using the setting above. Take extra care when there is already a JVM-wide proxy setting (typically through system properties) to avoid unexpected behavior.</p>
</div>
</div>
<div class="position-relative"><h4><a id="_serialization"></a>Serialization</h4><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/2.0/docs/src/reference/asciidoc/core/configuration.adoc">edit</a></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">es.batch.size.bytes</code> (default 1mb)
</span>
</dt>
<dd>
Size (in bytes) for batch writes using Elasticsearch <a href="http://www.elastic.co/guide/en/elasticsearch/reference/1.7/docs-bulk.html" class="ulink" target="_top">bulk</a> API. Note the bulk size is allocated <em>per task</em> instance. Always multiply by the number of tasks within a Hadoop job to get the total bulk size at runtime hitting Elasticsearch.
</dd>
<dt>
<span class="term">
<code class="literal">es.batch.size.entries</code> (default 1000)
</span>
</dt>
<dd>
Size (in entries) for batch writes using Elasticsearch <a href="http://www.elastic.co/guide/en/elasticsearch/reference/1.7/docs-bulk.html" class="ulink" target="_top">bulk</a> API - (0 disables it). Companion to <code class="literal">es.batch.size.bytes</code>, once one matches, the batch update is executed. Similar to the size, this setting is <em>per task</em> instance; it gets multiplied at runtime by the total number of Hadoop tasks running.
</dd>
<dt>
<span class="term">
<code class="literal">es.batch.write.refresh</code> (default true)
</span>
</dt>
<dd>
Whether to invoke an <a href="http://www.elastic.co/guide/en/elasticsearch/reference/1.7/indices-refresh.html" class="ulink" target="_top">index refresh</a> or not after a bulk update has been completed. Note this is called only after the entire write (meaning multiple bulk updates) have been executed.
</dd>
<dt>
<span class="term">
<code class="literal">es.batch.write.retry.count</code> (default 3)
</span>
</dt>
<dd>
Number of retries for a given batch in case Elasticsearch is overloaded and data is rejected. Note that only the rejected data is retried. If there is still data rejected after the retries have been performad, the Hadoop job is cancelled (and fails).
</dd>
<dt>
<span class="term">
<code class="literal">es.batch.write.retry.wait</code> (default 10s)
</span>
</dt>
<dd>
Time to wait between batch write retries.
</dd>
<dt>
<span class="term">
<code class="literal">es.ser.writer.value.class</code> (default <em>depends on the library used</em>)
</span>
</dt>
<dd>
Name of the <code class="literal">ValueReader</code> implementation for converting JSON to objects. This is set by the framework depending on the library (Map/Reduce, Cascading, Hive, Pig, etc&#8230;&#8203;) used.
</dd>
<dt>
<span class="term">
<code class="literal">es.ser.reader.value.class</code> (default <em>depends on the library used</em>)
</span>
</dt>
<dd>
Name of the <code class="literal">ValueWriter</code> implementation for converting objects to JSON. This is set by the framework depending on the library (Map/Reduce, Cascading, Hive, Pig, etc&#8230;&#8203;) used.
</dd>
</dl>
</div>
</div>
</div>
</div><div class="navfooter">
<span class="prev">
<a href="arch.html">« Architecture</a>
</span>
<span class="next">
<a href="configuration-runtime.html">Hadoop runtime options »</a>
</span>
</div>
</body>
</html>
