<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="description" content="Reference documentation of elasticsearch-hadoop">
<title>Apache Spark support | Elasticsearch for Apache Hadoop [2.0] | Elastic</title>
<link rel="home" href="index.html" title="Elasticsearch for Apache Hadoop [2.0]"/>
<link rel="up" href="reference.html" title="Reference"/>
<link rel="prev" href="pig.html" title="Apache Pig support"/>
<link rel="next" href="mapping.html" title="Mapping and Types"/>
<meta name="DC.type" content="Learn/Docs/Elasticsearch/Apache Hadoop/2.0"/>
<meta name="DC.subject" content="Elasticsearch"/>
<meta name="DC.identifier" content="2.0"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body><div class="page_header">
<p>
  <strong>WARNING</strong>: Version 2.0 has passed its 
  <a href="https://www.elastic.co/support/eol">EOL date</a>. 
</p>  
<p>
  This documentation is no longer being maintained and may be removed. 
  If you are running this version, we strongly advise you to upgrade. 
  For the latest information, see the 
  <a href="../current/index.html">current release documentation</a>. 
</p>
</div>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="index.html">Elasticsearch for Apache Hadoop [2.0]</a></span>
»
<span class="breadcrumb-link"><a href="reference.html">Reference</a></span>
»
<span class="breadcrumb-node">Apache Spark support</span>
</div>
<div class="navheader">
<span class="prev">
<a href="pig.html">« Apache Pig support</a>
</span>
<span class="next">
<a href="mapping.html">Mapping and Types »</a>
</span>
</div>
<div class="chapter">
<div class="titlepage"><div><div>
<h2 class="title"><a id="spark"></a>Apache Spark support<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/2.0/docs/src/reference/asciidoc/core/spark.adoc">edit</a></h2>
</div></div></div>
<div class="blockquote">
<table border="0" class="blockquote" summary="Block quote">
<tr>
<td valign="top" width="10%"></td>
<td valign="top" width="80%">
<p><a href="http://spark.apache.org" class="ulink" target="_top">Apache Spark</a> is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala and Python, and an optimized engine that supports general execution graphs.</p>
</td>
<td valign="top" width="10%"></td>
</tr>
<tr>
<td valign="top" width="10%"></td>
<td align="right" colspan="2" valign="top">
-- <span class="attribution">Spark website</span>
</td>
</tr>
</table>
</div>
<p>Spark provides fast iterative/functional-like capabilities over large data sets, typically by <em>caching</em> data in memory. As opposed to the rest of the libraries mentioned in this documentation, Apache Spark is computing framework that is not tied to Map/Reduce itself however it does integrate with Hadoop, mainly to HDFS.</p>
<h3><a id="spark-installation"></a>Installation<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/2.0/docs/src/reference/asciidoc/core/spark.adoc">edit</a></h3>
<p>Just like other libraries, elasticsearch-hadoop needs to be available in Spark&#8217;s classpath. As Spark has multiple deployment modes, this can translate to the target classpath, whether it is on only one node (as is the case with the local mode - which will be used through-out the documentation) or per-node depending on the desired infrastructure.</p>
<h3><a id="_configuration_3"></a>Configuration<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/2.0/docs/src/reference/asciidoc/core/spark.adoc">edit</a></h3>
<p>Through elasticsearch-hadoop, Spark can integrate with Elasticsearch through its dedicated <code class="literal">InputFormat</code>, and in case of writing, through <code class="literal">OutputFormat</code>. These are described at length in the <a class="xref" href="mapreduce.html" title="Map/Reduce integration">Map/Reduce</a> chapter so please refer to that for an in-depth explanation.</p>
<p>In short, one needs to setup a basic Hadoop <code class="literal">Configuration</code> object with the target Elasticsearch cluster and index, potentially a query, and she&#8217;s good to go.</p>
<p>From Spark&#8217;s perspective, they only thing required is setting up serialization - Spark relies by default on Java serialization which is convenient but fairly inefficient. This is the reason why Hadoop itself introduced its own serialization mechanism and its own types - namely <code class="literal">Writable</code>s. As such, <code class="literal">InputFormat</code> and <code class="literal">OutputFormat</code>s are required to return <code class="literal">Writables</code> which, out of the box, Spark does not understand.
The good news is, one can easily enable a different serialization (<a href="https://github.com/EsotericSoftware/kryo" class="ulink" target="_top">Kryo</a>) which handles the conversion automatically and also does this quite efficiently.</p>
<div class="pre_wrapper lang-java">
<pre class="programlisting prettyprint lang-java">SparkConf sc = new SparkConf(); //.setMaster("local");
sc.set("spark.serializer", KryoSerializer.class.getName()); <a id="CO41-1"></a><i class="conum" data-value="1"></i>

// needed only when using the Java API
JavaSparkContext jsc = new JavaSparkContext(sc);</pre>
</div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO41-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>Enable the Kryo serialization support with Spark</p>
</td>
</tr>
</table>
</div>
<p>Or if you prefer Scala</p>
<div class="pre_wrapper lang-scala">
<pre class="programlisting prettyprint lang-scala">val sc = new SparkConf(...)
sc.set("spark.serializer", classOf[KryoSerializer].getName)    <a id="CO42-1"></a><i class="conum" data-value="1"></i></pre>
</div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO42-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>Enable the Kryo serialization support with Spark</p>
</td>
</tr>
</table>
</div>
<p>Note that the Kryo serialization is used as a work-around for dealing with <code class="literal">Writable</code> types; one can choose to convert the types directly (from <code class="literal">Writable</code> to <code class="literal">Serializable</code> types) - which is fine however for getting started, the one liner above seems to be the most effective.</p>
<h3><a id="_reading_data_from_elasticsearch_5"></a>Reading data from Elasticsearch<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/2.0/docs/src/reference/asciidoc/core/spark.adoc">edit</a></h3>
<p>To read data, simply pass in the <code class="literal">org.elasticsearch.hadoop.mr.EsInputFormat</code> class - since it supports both the <code class="literal">old</code> and the <code class="literal">new</code> Map/Reduce APIs, you are free to use either method on <code class="literal">SparkContext</code>'s, <code class="literal">hadoopRDD</code> (which we recommend for conciseness reasons) or <code class="literal">newAPIHadoopRDD</code>. Which ever you chose, stick with it to avoid confusion and problems down the road.</p>
<h4><a id="_old_org_apache_hadoop_mapred_api_3"></a><em>Old</em> (<code class="literal">org.apache.hadoop.mapred</code>) API<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/2.0/docs/src/reference/asciidoc/core/spark.adoc">edit</a></h4>
<div class="pre_wrapper lang-java">
<pre class="programlisting prettyprint lang-java">JobConf conf = new JobConf();                   <a id="CO43-1"></a><i class="conum" data-value="1"></i>
conf.set("es.resource", "radio/artists");       <a id="CO43-2"></a><i class="conum" data-value="2"></i>
conf.set("es.query", "?q=me*");                 <a id="CO43-3"></a><i class="conum" data-value="3"></i>

JavaPairRDD esRDD = jsc.hadoopRDD(conf, EsInputFormat.class,
                                        Text.class, MapWritable.class); <a id="CO43-4"></a><i class="conum" data-value="4"></i>
long docCount = esRDD.count();</pre>
</div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO43-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>Create the Hadoop object (use the old API)</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO43-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p>Configure the source (index)</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO43-3"><i class="conum" data-value="3"></i></a></p>
</td>
<td align="left" valign="top">
<p>Setup the query (optional)</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO43-4"><i class="conum" data-value="4"></i></a></p>
</td>
<td align="left" valign="top">
<p>Create a Spark <code class="literal">RDD</code> on top of Elasticsearch through <code class="literal">EsInputFormat</code> - the key represent the doc id, the value the doc itself</p>
</td>
</tr>
</table>
</div>
<p>The Scala version is below:</p>
<div class="pre_wrapper lang-scala">
<pre class="programlisting prettyprint lang-scala">val conf = new JobConf()                                <a id="CO44-1"></a><i class="conum" data-value="1"></i>
conf.set("es.resource", "radio/artists")                <a id="CO44-2"></a><i class="conum" data-value="2"></i>
conf.set("es.query", "?q=me*")                          <a id="CO44-3"></a><i class="conum" data-value="3"></i>
val esRDD = sc.hadoopRDD(conf, classOf[EsInputFormat[Text, MapWritable]],     <a id="CO44-4"></a><i class="conum" data-value="4"></i>
                               classOf[Text], classOf[MapWritable]))
val docCount = esRDD.count();</pre>
</div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO44-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>Create the Hadoop object (use the old API)</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO44-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p>Configure the source (index)</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO44-3"><i class="conum" data-value="3"></i></a></p>
</td>
<td align="left" valign="top">
<p>Setup the query (optional)</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO44-4"><i class="conum" data-value="4"></i></a></p>
</td>
<td align="left" valign="top">
<p>Create a Spark <code class="literal">RDD</code> on top of Elasticsearch through <code class="literal">EsInputFormat</code></p>
</td>
</tr>
</table>
</div>
<h4><a id="_new_org_apache_hadoop_mapreduce_api_3"></a><em>New</em> (<code class="literal">org.apache.hadoop.mapreduce</code>) API<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/2.0/docs/src/reference/asciidoc/core/spark.adoc">edit</a></h4>
<p>As expected, the <code class="literal">mapreduce</code> API version is strikingly similar - replace <code class="literal">hadoopRDD</code> with <code class="literal">newAPIHadoopRDD</code> and <code class="literal">JobConf</code> with <code class="literal">Configuration</code>. That&#8217;s about it.</p>
<div class="pre_wrapper lang-java">
<pre class="programlisting prettyprint lang-java">Configuration conf = new Configuration();                   <a id="CO45-1"></a><i class="conum" data-value="1"></i>
conf.set("es.resource", "radio/artists");       <a id="CO45-2"></a><i class="conum" data-value="2"></i>
conf.set("es.query", "?q=me*");                 <a id="CO45-3"></a><i class="conum" data-value="3"></i>

JavaPairRDD esRDD = jsc.newAPIHadoopRDD(conf, EsInputFormat.class,
                                              Text.class, MapWritable.class); <a id="CO45-4"></a><i class="conum" data-value="4"></i>
long docCount = esRDD.count();</pre>
</div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO45-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>Create the Hadoop object (use the new API)</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO45-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p>Configure the source (index)</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO45-3"><i class="conum" data-value="3"></i></a></p>
</td>
<td align="left" valign="top">
<p>Setup the query (optional)</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO45-4"><i class="conum" data-value="4"></i></a></p>
</td>
<td align="left" valign="top">
<p>Create a Spark <code class="literal">RDD</code> on top of Elasticsearch through <code class="literal">EsInputFormat</code> - the key represent the doc id, the value the doc itself</p>
</td>
</tr>
</table>
</div>
<p>The Scala version is below:</p>
<div class="pre_wrapper lang-scala">
<pre class="programlisting prettyprint lang-scala">val conf = new Configuration()                          <a id="CO46-1"></a><i class="conum" data-value="1"></i>
conf.set("es.resource", "radio/artists")                <a id="CO46-2"></a><i class="conum" data-value="2"></i>
conf.set("es.query", "?q=me*")                          <a id="CO46-3"></a><i class="conum" data-value="3"></i>
val esRDD = sc.newAPIHadoopRDD(conf, classOf[EsInputFormat[Text, MapWritable]],     <a id="CO46-4"></a><i class="conum" data-value="4"></i>
                                  classOf[Text], classOf[MapWritable]))
val docCount = esRDD.count();</pre>
</div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO46-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>Create the Hadoop object (use the new API)</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO46-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p>Configure the source (index)</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO46-3"><i class="conum" data-value="3"></i></a></p>
</td>
<td align="left" valign="top">
<p>Setup the query (optional)</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO46-4"><i class="conum" data-value="4"></i></a></p>
</td>
<td align="left" valign="top">
<p>Create a Spark RDD on top of Elasticsearch through <code class="literal">EsInputFormat</code></p>
</td>
</tr>
</table>
</div>
</div>
<div class="navfooter">
<span class="prev">
<a href="pig.html">« Apache Pig support</a>
</span>
<span class="next">
<a href="mapping.html">Mapping and Types »</a>
</span>
</div>
</div>
</body>
</html>
