<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="description" content="Reference documentation of elasticsearch-hadoop">
<title>Kerberos | Elasticsearch for Apache Hadoop [7.5] | Elastic</title>
<link rel="home" href="index.html" title="Elasticsearch for Apache Hadoop [7.5]"/>
<link rel="up" href="reference.html" title="Elasticsearch for Apache Hadoop"/>
<link rel="prev" href="errorhandlers.html" title="Error Handlers"/>
<link rel="next" href="metrics.html" title="Hadoop Metrics"/>
<meta name="DC.type" content="Learn/Docs/Elasticsearch/Apache Hadoop/7.5"/>
<meta name="DC.subject" content="Elasticsearch"/>
<meta name="DC.identifier" content="7.5"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body><div class="page_header">
<strong>IMPORTANT</strong>: No additional bug fixes or documentation updates
will be released for this version. For the latest information, see the
<a href="../current/index.html">current release documentation</a>.
</div>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="index.html">Elasticsearch for Apache Hadoop [7.5]</a></span>
»
<span class="breadcrumb-link"><a href="reference.html">Elasticsearch for Apache Hadoop</a></span>
»
<span class="breadcrumb-node">Kerberos</span>
</div>
<div class="navheader">
<span class="prev">
<a href="errorhandlers.html">« Error Handlers</a>
</span>
<span class="next">
<a href="metrics.html">Hadoop Metrics »</a>
</span>
</div>
<div class="chapter">
<div class="titlepage"><div><div>
<h2 class="title"><a id="kerberos"></a>Kerberos<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/7.5/docs/src/reference/asciidoc/core/kerberos.adoc">edit</a></h2>
</div></div></div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>Added in 6.7.</p>
</div>
</div>
<div class="important admon">
<div class="icon"></div>
<div class="admon_content">
<p>Kerberos support for Elasticsearch for Apache Hadoop requires Elasticsearch 6.7 or greater</p>
</div>
</div>
<p>Securing Hadoop means using Kerberos. Elasticsearch supports Kerberos as an authentication method. While the use of
Kerberos is not required for securing Elasticsearch, it is a convenient option for those who already deploy Kerberos to secure their
Hadoop clusters. This chapter aims to explain the steps needed to set up elasticsearch-hadoop to use Kerberos authentication for Elasticsearch.</p>
<p>Elasticsearch for Apache Hadoop communicates with Elasticsearch entirely over HTTP. In order to support Kerberos authentication over HTTP, elasticsearch-hadoop uses
the <a href="https://tools.ietf.org/html/rfc4178" class="ulink" target="_top">Simple and Protected GSSAPI Negotiation Mechanism (SPNEGO)</a> to negotiate which
underlying authentication method to use (in this case, Kerberos) and to transmit the agreed upon credentials to the server.
This authentication mechanism is performed using the <a href="https://tools.ietf.org/html/rfc4559" class="ulink" target="_top">HTTP Negotiate</a> authentication
standard, where a request is sent to the server and a response is received back with a payload that further advances
the negotiation. Once the negotiation between the client and server is complete, the request is accepted and a
successful response is returned.</p>
<p>Elasticsearch for Apache Hadoop makes use of Hadoop&#8217;s user management processes; The Kerberos credentials of the current Hadoop user are used when
authenticating to Elasticsearch. This means that Kerberos authentication in Hadoop must be enabled in order for elasticsearch-hadoop to obtain a
user&#8217;s Kerberos credentials. In the case of using an integration that does not depend on Hadoop&#8217;s runtime (e.g. Storm),
additional steps may be required to ensure that a running process has Kerberos credentials available for authentication.
It is recommended that you consult the documentation of each framework that you are using on how to configure security.</p>
<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="kerberos-settings"></a>Setting up your environment<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/7.5/docs/src/reference/asciidoc/core/kerberos.adoc">edit</a></h2>
</div></div></div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>This documentation assumes that you have already provisioned a Hadoop cluster with Kerberos authentication enabled
(required). The general process of deploying Kerberos and securing Hadoop is beyond the scope of this documentation.</p>
</div>
</div>
<p>Before starting, you will need to ensure that principals for your users are provisioned in your Kerberos deployment,
as well as service principals for each Elasticsearch node. To enable Kerberos authentication on Elasticsearch, it must be
<a href="/guide/en/elasticsearch/reference/current/configuring-kerberos-realm.html" class="ulink" target="_top">configured with a
Kerberos realm</a>. It is recommended that you familiarize yourself with how to configure Elasticsearch Kerberos realms so that you
can make appropriate adjustments to fit your deployment. You can find more information on how they work in the
<a href="/guide/en/elastic-stack-overview/current/kerberos-realm.html" class="ulink" target="_top">Elastic Stack documentation</a>.</p>
<p>Additionally, you will need to <a href="/guide/en/elasticsearch/reference/current/security-settings.html" class="ulink" target="_top">
configure the API Key Realm</a> in Elasticsearch. Hadoop and other distributed data processing frameworks only authenticate with
Kerberos in the process that launches a job. Once a job has been launched, the worker processes are often cut off from
the original Kerberos credentials and need some other form of authentication. Hadoop services often provide mechanisms for
obtaining <em>Delegation Tokens</em> during job submission. These tokens are then distributed to worker processes which use
the tokens to authenticate on behalf of the user running the job. Elasticsearch for Apache Hadoop obtains API Keys in order to provide tokens
for worker processes to authenticate with.</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="kerberos-settings-eshadoop"></a>Connector Settings<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/7.5/docs/src/reference/asciidoc/core/kerberos.adoc">edit</a></h3>
</div></div></div>
<p>The following settings are used to configure elasticsearch-hadoop to use Kerberos authentication:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">es.security.authentication</code> (default <code class="literal">simple</code>, or <code class="literal">basic</code> if <code class="literal">es.net.http.auth.user</code> is set)
</span>
</dt>
<dd>
Required. Similar to most Hadoop integrations, this property signals which method to use in order to authenticate with
Elasticsearch. By default, the value is <code class="literal">simple</code>, unless <code class="literal">es.net.http.auth.user</code> is set, in which case it will default to
<code class="literal">basic</code>. The available options for this setting are <code class="literal">simple</code> for no authentication, <code class="literal">basic</code> for basic http
authentication, <code class="literal">pki</code> if relying on certificates, and <code class="literal">kerberos</code> if Kerberos authentication over SPNEGO should be used.
</dd>
<dt>
<span class="term">
<code class="literal">es.net.spnego.auth.elasticsearch.principal</code> (default none)
</span>
</dt>
<dd>
Required if <code class="literal">es.security.authentication</code> is set to <code class="literal">kerberos</code>. Details the name of the service principal that the
Elasticsearch server is running as. This will usually be of the form <code class="literal">HTTP/node.address@REALM</code>. Since Elasticsearch is
distributed and should be using a service principal per node, you can use the <code class="literal">_HOST</code> pattern
(like so <code class="literal">HTTP/_HOST@REALM</code>) to have elasticsearch-hadoop substitute the address of the node it is communicating with at runtime. Note
that elasticsearch-hadoop will attempt to reverse resolve node IP addresses to hostnames in order to perform this substitution.
</dd>
<dt>
<span class="term">
<code class="literal">es.net.spnego.auth.mutual</code> (default false)
</span>
</dt>
<dd>
Optional. The SPNEGO mechanism assumes that authentication may take multiple back and forth request-response cycles for
a request to be fully accepted by the server. When a request is finally accepted by the server, the response contains a
payload that can be verified to ensure that the server is the principal they say they are. Setting this to <code class="literal">true</code>
instructs elasticsearch-hadoop to perform this mutual authentication, and to fail the response if it detects invalid credentials from
the server.
</dd>
</dl>
</div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="kerberos-hadoop"></a>Kerberos on Hadoop<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/7.5/docs/src/reference/asciidoc/core/kerberos.adoc">edit</a></h2>
</div></div></div>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="kerberos-hadoop-requirements"></a>Requirements<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/7.5/docs/src/reference/asciidoc/core/kerberos.adoc">edit</a></h3>
</div></div></div>
<p>Before using Kerberos authentication to Elasticsearch, Kerberos authentication must be enabled in Hadoop.</p>
<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="kerberos-hadoop-requirements-conf"></a>Configure elasticsearch-hadoop<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/7.5/docs/src/reference/asciidoc/core/kerberos.adoc">edit</a></h4>
</div></div></div>
<p>Elasticsearch for Apache Hadoop only needs <a class="xref" href="kerberos.html#kerberos-settings-eshadoop" title="Connector Settings">a few settings</a> to configure Kerberos authentication. It is best to
set these properties in your <code class="literal">core-site.xml</code> configuration so that they can be obtained across your entire Hadoop
deployment, just like you would for turning on security options for services in Hadoop.</p>
<div class="pre_wrapper lang-xml">
<pre class="programlisting prettyprint lang-xml">&lt;configuration&gt;
    ...
    &lt;property&gt;
        &lt;name&gt;es.security.authentication&lt;/name&gt;
        &lt;value&gt;kerberos&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;es.net.spnego.auth.elasticsearch.principal&lt;/name&gt;
        &lt;value&gt;HTTP/_HOST@REALM.NAME.HERE&lt;/value&gt;
    &lt;/property&gt;
    ...
&lt;/configuration&gt;</pre>
</div>
</div>

</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="kerberos-yarn"></a>Kerberos on YARN<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/7.5/docs/src/reference/asciidoc/core/kerberos.adoc">edit</a></h2>
</div></div></div>
<p>When applications launch on a YARN cluster, they send along all of their application credentials to the Resource
Manager process for them to be distributed to the containers. The Resource Manager has the ability to renew any tokens
in those credentials that are about to expire and to cancel tokens once a job has completed. The tokens from Elasticsearch have
a default lifespan of 7 days and they are not renewable. It is a best practice to configure YARN so that it is able to
cancel those tokens at the end of a run in order to lower the risk of unauthorized use, and to lower the amount of
bookkeeping Elasticsearch must perform to maintain them.</p>
<p>In order to configure YARN to allow it to cancel Elasticsearch tokens at the end of a run, you must add the elasticsearch-hadoop jar to the
Resource Manager&#8217;s classpath. You can do that by placing the jar on the Resource Manager&#8217;s local filesystem, and setting
the path to the jar in the <code class="literal">YARN_USER_CLASSPATH</code> environment variable. Once the jar is added, the Resource Manager will
need to be restarted.</p>
<div class="pre_wrapper lang-ini">
<pre class="programlisting prettyprint lang-ini">export YARN_USER_CLASSPATH=/path/to/elasticsearch-hadoop.jar</pre>
</div>
<p>Additionally, the connection information for elasticsearch-hadoop should be present in the Hadoop configuration,
preferably the <code class="literal">core-site.xml</code>. This is because when Resource Manager cancels a token, it does not take the job
configuration into account. Without the connection settings in the Hadoop configuration, the Resource Manager will not
be able to communicate to Elasticsearch in order to cancel the token.</p>
<p>Here is a few common security properties that you will need in order for the Resource Manager to contact Elasticsearch to cancel
tokens:</p>
<div class="pre_wrapper lang-xml">
<pre class="programlisting prettyprint lang-xml">&lt;configuration&gt;
    ...
    &lt;property&gt;
        &lt;name&gt;es.nodes&lt;/name&gt;
        &lt;value&gt;es-master-1,es-master-2,es-master-3&lt;/value&gt; <a id="CO106-1"></a><i class="conum" data-value="1"></i>
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;es.security.authentication&lt;/name&gt; <a id="CO106-2"></a><i class="conum" data-value="2"></i>
        &lt;value&gt;kerberos&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;es.net.spnego.auth.elasticsearch.principal&lt;/name&gt; <a id="CO106-3"></a><i class="conum" data-value="3"></i>
        &lt;value&gt;HTTP/_HOST@REALM&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;es.net.ssl&lt;/name&gt; <a id="CO106-4"></a><i class="conum" data-value="4"></i>
        &lt;value&gt;true&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;es.net.ssl.keystore.location&lt;/name&gt; <a id="CO106-5"></a><i class="conum" data-value="5"></i>
        &lt;value&gt;file:///path/to/ssl/keystore&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;es.net.ssl.truststore.location&lt;/name&gt; <a id="CO106-6"></a><i class="conum" data-value="6"></i>
        &lt;value&gt;file:///path/to/ssl/truststore&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;es.keystore.location&lt;/name&gt; <a id="CO106-7"></a><i class="conum" data-value="7"></i>
        &lt;value&gt;file:///path/to/es/secure/store&lt;/value&gt;
    &lt;/property&gt;
    ...
&lt;/configuration&gt;</pre>
</div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO106-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>The addresses of some Elasticsearch nodes. These can be any nodes (or all of them) as long as they all belong to the same
cluster.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO106-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p>Authentication must be configured as <code class="literal">kerberos</code> in the settings.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO106-3"><i class="conum" data-value="3"></i></a></p>
</td>
<td align="left" valign="top">
<p>The name of the Elasticsearch service principal is not required for token cancellation but having the property in the
<code class="literal">core-site.xml</code> is required for some integrations like Spark.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO106-4"><i class="conum" data-value="4"></i></a></p>
</td>
<td align="left" valign="top">
<p>SSL should be enabled if you are using a secured Elasticsearch deployment.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO106-5"><i class="conum" data-value="5"></i></a></p>
</td>
<td align="left" valign="top">
<p>Location on the local filesystem to reach the SSL Keystore.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO106-6"><i class="conum" data-value="6"></i></a></p>
</td>
<td align="left" valign="top">
<p>Location on the local filesystem to reach the SSL Truststore.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO106-7"><i class="conum" data-value="7"></i></a></p>
</td>
<td align="left" valign="top">
<p>Location on the local filesystem to reach the <a class="xref" href="security.html#keystore" title="Secure Settings">elasticsearch-hadoop secure store for secure settings</a>.</p>
</td>
</tr>
</table>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="kerberos-mr"></a>Kerberos with Map/Reduce<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/7.5/docs/src/reference/asciidoc/core/kerberos.adoc">edit</a></h2>
</div></div></div>
<p>Before launching your Map/Reduce job, you must add a delegation token for Elasticsearch to the job&#8217;s credential set. The
<code class="literal">EsMapReduceUtil</code> utility class can be used to do this for you. Simply pass your job to it before submitting it to the
cluster. Using the local Kerberos credentials, the utility will establish a connection to Elasticsearch, request an API Key, and
stow the key in the job&#8217;s credential set for the worker processes to use.</p>
<div class="pre_wrapper lang-java">
<pre class="programlisting prettyprint lang-java">Job job = Job.getInstance(getConf(), "My-Job-Name"); <a id="CO107-1"></a><i class="conum" data-value="1"></i>

// Configure Job Here...

EsMapReduceUtil.initCredentials(job); <a id="CO107-2"></a><i class="conum" data-value="2"></i>

if (!job.waitForCompletion(true)) { <a id="CO107-3"></a><i class="conum" data-value="3"></i>
    return 1;
}</pre>
</div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO107-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>Creating a new job instance</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO107-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p>EsMapReduceUtil obtains job delegation tokens for Elasticsearch</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO107-3"><i class="conum" data-value="3"></i></a></p>
</td>
<td align="left" valign="top">
<p>Submit the job to the cluster</p>
</td>
</tr>
</table>
</div>
<p>You can obtain the job delegation tokens at any time during the configuration of the Job object, as long as your elasticsearch-hadoop
specific configurations are set. It&#8217;s usually sufficient to do it right before submitting the job. You should only do
this once per job since each call will wastefully obtain another API Key.</p>
<p>Additionally, the utility is also compatible with the <code class="literal">mapred</code> API classes:</p>
<div class="pre_wrapper lang-java">
<pre class="programlisting prettyprint lang-java">JobConf jobConf = new JobConf(getConf()); <a id="CO108-1"></a><i class="conum" data-value="1"></i>
jobConf.setJobName("My-Job-Name");

// Configure JobConf Here...

EsMapReduceUtil.initCredentials(jobConf); <a id="CO108-2"></a><i class="conum" data-value="2"></i>

JobClient.runJob(jobConf).waitForCompletion(); <a id="CO108-3"></a><i class="conum" data-value="3"></i></pre>
</div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO108-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>Creating a new job configuration</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO108-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p>Obtain Elasticsearch delegation tokens</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO108-3"><i class="conum" data-value="3"></i></a></p>
</td>
<td align="left" valign="top">
<p>Submit the job to the cluster</p>
</td>
</tr>
</table>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="kerberos-hive"></a>Kerberos with Hive<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/7.5/docs/src/reference/asciidoc/core/kerberos.adoc">edit</a></h2>
</div></div></div>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="kerberos-hive-requirements"></a>Requirements<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/7.5/docs/src/reference/asciidoc/core/kerberos.adoc">edit</a></h3>
</div></div></div>
<div class="important admon">
<div class="icon"></div>
<div class="admon_content">
<p>Using Kerberos auth on Elasticsearch is only supported using HiveServer2.</p>
</div>
</div>
<p>Before using Kerberos authentication to Elasticsearch in Hive, Kerberos authentication must be enabled for Hadoop. Make sure you have
done all the required steps for <a class="xref" href="kerberos.html#kerberos-hadoop-requirements" title="Requirements">configuring your Hadoop cluster</a> as well as the steps
for <a class="xref" href="kerberos.html#kerberos-yarn" title="Kerberos on YARN">configuring your YARN services</a> before using Kerberos authentication for Elasticsearch.</p>
<p>Finally, ensure that Hive Security is enabled.</p>
<p>Since Hive relies on user impersonation in Elasticsearch it is advised that you familiarise yourself with
<a href="/guide/en/elastic-stack-overview/current/setting-up-authentication.html" class="ulink" target="_top">Elasticsearch authentication</a> and
<a href="/guide/en/elastic-stack-overview/current/authorization.html" class="ulink" target="_top">authorization</a>.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="kerberos-hive-proxy"></a>Configure user impersonation settings for Hive<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/7.5/docs/src/reference/asciidoc/core/kerberos.adoc">edit</a></h3>
</div></div></div>
<p>Hive&#8217;s security model follows a proxy-based approach. When a client submits a query to a secured Hive server, Hive
authenticates the client using Kerberos. Once Hive is sure of the client&#8217;s identity, it wraps its own identity with a
<em>proxy user</em>. The proxy user contains the client&#8217;s simple user name, but contains no credentials. Instead, it is
expected that all interactions are executed as the Hive principal impersonating the client user. This is why when
configuring Hive security, one must specify in the Hadoop configuration which users Hive is allowed to impersonate:</p>
<div class="pre_wrapper lang-xml">
<pre class="programlisting prettyprint lang-xml">&lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.hive.hosts&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.hive.groups&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
&lt;/property&gt;</pre>
</div>
<p>Elasticsearch <a href="/guide/en/elastic-stack-overview/current/run-as-privilege.html" class="ulink" target="_top">supports user impersonation</a>,
but only users from certain realm implementations can be impersonated. Most deployments of Kerberos include other identity
management components like
<a href="/guide/en/elasticsearch/reference/current/configuring-ldap-realm.html" class="ulink" target="_top">LDAP</a> or
<a href="/guide/en/elasticsearch/reference/current/configuring-ad-realm.html" class="ulink" target="_top">Active Directory</a>.
In those cases, you can configure those realms in Elasticsearch to allow for user impersonation.</p>
<p>If you are only using Kerberos, or you are using a solution for which Elasticsearch does not support user impersonation, you must
mirror your Kerberos principals to either a
<a href="/guide/en/elastic-stack-overview/current/native-realm.html" class="ulink" target="_top">native realm</a> or a
<a href="/guide/en/elastic-stack-overview/current/file-realm.html" class="ulink" target="_top">file realm</a> in Elasticsearch. When mirroring a
Kerberos principal to one of these realms, set the new user&#8217;s username to just the main part of the principal name, without
any realm or host information. For instance, <code class="literal">client@REALM</code> would just be <code class="literal">client</code> and <code class="literal">someservice/domain.name@REALM</code>
would just be <code class="literal">someservice</code>.</p>
<p>You can follow this step by step process for mirroring users:</p>
<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="kerberos-hive-proxy-user-role"></a>Create End User Roles<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/7.5/docs/src/reference/asciidoc/core/kerberos.adoc">edit</a></h4>
</div></div></div>
<p>Create a role for your end users that will be querying Hive. In this example, we will make a simple role for accessing
indices that match <code class="literal">hive-index-*</code>. All our Hive users will end up using this role to read, write, and update indices
in Elasticsearch.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT _xpack/security/role/hive_user_role <a id="CO109-1"></a><i class="conum" data-value="1"></i>
{
  "run_as": [],
  "cluster": ["monitor", "manage_token"], <a id="CO109-2"></a><i class="conum" data-value="2"></i>
  "indices": [
      {
        "names": [ "hive-index-*" ],  <a id="CO109-3"></a><i class="conum" data-value="3"></i>
        "privileges": [ "read", "write", "manage" ]
      }
  ]
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/1.console"></div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO109-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>Our example role name is <code class="literal">hive_user_role</code>.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO109-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p>User should be able to query basic cluster information and manage tokens.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO109-3"><i class="conum" data-value="3"></i></a></p>
</td>
<td align="left" valign="top">
<p>Our user will be able to perform read write and management operations on an index.</p>
</td>
</tr>
</table>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="kerberos-hive-proxy-user-mapping"></a>Create role mapping for Kerberos user principal<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/7.5/docs/src/reference/asciidoc/core/kerberos.adoc">edit</a></h4>
</div></div></div>
<p>Now that the user role is created, we must map the Kerberos user principals to the role. Elasticsearch does not know the complete
list of principals that are managed by Kerberos. As such, each principal that wishes to connect to Elasticsearch must be mapped to
a list of roles that they will be granted after authentication.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">POST /_xpack/security/role_mapping/hive_user_1_mapping
{
  "roles": [ "hive_user_role" ], <a id="CO110-1"></a><i class="conum" data-value="1"></i>
  "enabled": true,
  "rules": {
    "field" : { "username" : "hive.user.1@REALM" } <a id="CO110-2"></a><i class="conum" data-value="2"></i>
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/2.console"></div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO110-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>We set the roles for this mapping to be our example role <code class="literal">hive_user_role</code>.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO110-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p>When the user principal <code class="literal">hive.user.1@REALM</code> authenticates, it will be given the permissions from the
<code class="literal">hive_user_role</code>.</p>
</td>
</tr>
</table>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="kerberos-hive-proxy-user-mirror"></a>Mirror the user to the native realm<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/7.5/docs/src/reference/asciidoc/core/kerberos.adoc">edit</a></h4>
</div></div></div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>You may not have to perform this step if you are deploying
<a href="/guide/en/elasticsearch/reference/current/configuring-ldap-realm.html" class="ulink" target="_top">LDAP</a> or
<a href="/guide/en/elasticsearch/reference/current/configuring-ad-realm.html" class="ulink" target="_top">Active Directory</a> along
with Kerberos. Elasticsearch will perform user impersonation by looking up the user names in those realms as long as the simple
names (e.g. hive.user.1) on the Kerberos principals match the user names LDAP or Active Directory exactly.</p>
</div>
</div>
<p>Mirroring the user to the native realm will allow Elasticsearch to accept authentication requests from the original principal
as well as accept requests from Hive which is impersonating the user. You can create a user in the native realm like
so:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /_xpack/security/user/hive.user.1 <a id="CO111-1"></a><i class="conum" data-value="1"></i>
{
  "enabled" : true,
  "password" : "swordfish", <a id="CO111-2"></a><i class="conum" data-value="2"></i>
  "roles" : [ "hive_user_role" ], <a id="CO111-3"></a><i class="conum" data-value="3"></i>
  "metadata" : {
    "principal" : "hive.user.1@REALM" <a id="CO111-4"></a><i class="conum" data-value="4"></i>
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/3.console"></div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO111-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>The user name is <code class="literal">hive.user.1</code>, which is the simple name format of the <code class="literal">hive.user.1@REALM</code> principal we are
mirroring.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO111-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p>Provide a password here for the user. This should ideally be a securely generated random password since this
mirrored user is just for impersonation purposes.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO111-3"><i class="conum" data-value="3"></i></a></p>
</td>
<td align="left" valign="top">
<p>Setting the user&#8217;s roles to be the example role <code class="literal">hive_user_role</code>.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO111-4"><i class="conum" data-value="4"></i></a></p>
</td>
<td align="left" valign="top">
<p>This is not required, but setting the original principal on the user as metadata may be helpful for your own
bookkeeping.</p>
</td>
</tr>
</table>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="kerberos-hive-proxy-service-role"></a>Create a role to impersonate Hive users<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/7.5/docs/src/reference/asciidoc/core/kerberos.adoc">edit</a></h4>
</div></div></div>
<p>Once you have configured Elasticsearch with a role mapping for your Kerberos principals and native users for impersonation, you
must create a role that Hive will use to impersonate those users.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT _xpack/security/role/hive_proxier
{
  "run_as": ["hive.user.1"] <a id="CO112-1"></a><i class="conum" data-value="1"></i>
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/4.console"></div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO112-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>Hive&#8217;s proxy role should be limited to only run as the users who will be using Hive.</p>
</td>
</tr>
</table>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="kerberos-hive-proxy-service-mapping"></a>Create role mapping for Hive&#8217;s service principal<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/7.5/docs/src/reference/asciidoc/core/kerberos.adoc">edit</a></h4>
</div></div></div>
<p>Now that there are users to impersonate, and a role that can impersonate them, make sure to map the Hive principal to
the proxier role, as well as any of the roles that the users it is impersonating would have. This allows the Hive
principal to create and read indices, documents, or do anything else its impersonated users might be able to do.
While Hive is impersonating the user, it must have these roles or else it will not be able to fully impersonate that
user.</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">POST /_xpack/security/role_mapping/hive_hiveserver2_mapping
{
  "roles": [
    "hive_user_role", <a id="CO113-1"></a><i class="conum" data-value="1"></i>
    "hive_proxier" <a id="CO113-2"></a><i class="conum" data-value="2"></i>
  ],
  "enabled": true,
  "rules": {
    "field" : { "username" : "hive/hiveserver2.address@REALM" } <a id="CO113-3"></a><i class="conum" data-value="3"></i>
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/5.console"></div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO113-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>Here we set the roles to be the superset of the roles from the users we want to impersonate. In our example, the
<code class="literal">hive_user_role</code> role is set.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO113-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p>The role that allows Hive to impersonate Hive end users.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO113-3"><i class="conum" data-value="3"></i></a></p>
</td>
<td align="left" valign="top">
<p>The name of the Hive server principal to match against.</p>
</td>
</tr>
</table>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>If managing Kerberos role mappings via the API&#8217;s is not desired, they can instead be managed in a
<a href="/guide/en/elastic-stack-overview/current/mapping-roles.html#mapping-roles-file" class="ulink" target="_top">role mapping file</a>.</p>
</div>
</div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="kerberos-hive-running"></a>Running your Hive queries<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/7.5/docs/src/reference/asciidoc/core/kerberos.adoc">edit</a></h3>
</div></div></div>
<p>Once all user accounts are configured and all previous steps for enabling Kerberos auth in Hadoop and Hive are complete,
there should be no differences in creating Hive queries from before.</p>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="kerberos-pig"></a>Kerberos with Pig<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/7.5/docs/src/reference/asciidoc/core/kerberos.adoc">edit</a></h2>
</div></div></div>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="kerberos-pig-requirements"></a>Requirements<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/7.5/docs/src/reference/asciidoc/core/kerberos.adoc">edit</a></h3>
</div></div></div>
<p>Before using Kerberos authentication to Elasticsearch in Pig, Kerberos authentication must be enabled for Hadoop. Make sure you have
done all the required steps for <a class="xref" href="kerberos.html#kerberos-hadoop-requirements" title="Requirements">configuring your Hadoop cluster</a> as well as the steps
for <a class="xref" href="kerberos.html#kerberos-yarn" title="Kerberos on YARN">configuring your YARN services</a> before using Kerberos authentication for Elasticsearch.</p>
<p>If elasticsearch-hadoop is configured for Kerberos authentication and Hadoop security is enabled, elasticsearch-hadoop&#8217;s storage functions in Pig will
automatically obtain delegation tokens for jobs when submitting them to the cluster.</p>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="kerberos-spark"></a>Kerberos with Spark<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/7.5/docs/src/reference/asciidoc/core/kerberos.adoc">edit</a></h2>
</div></div></div>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="kerberos-spark-requirements"></a>Requirements<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/7.5/docs/src/reference/asciidoc/core/kerberos.adoc">edit</a></h3>
</div></div></div>
<p>Using Kerberos authentication in elasticsearch-hadoop for Spark has the following requirements:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Your Spark jobs must be deployed on YARN. Using Kerberos authentication in elasticsearch-hadoop does not support any other Spark cluster
deployments (Mesos, Standalone).
</li>
<li class="listitem">
Your version of Spark must be on or above version 2.1.0. It is this version that Spark added the ability to plug in
third-party credential providers to obtain delegation tokens.
</li>
</ol>
</div>
<p>Before using Kerberos authentication to Elasticsearch in Spark, Kerberos authentication must be enabled for Hadoop. Make sure you have
done all the required steps for <a class="xref" href="kerberos.html#kerberos-hadoop-requirements" title="Requirements">configuring your Hadoop cluster</a> as well as the steps
for <a class="xref" href="kerberos.html#kerberos-yarn" title="Kerberos on YARN">configuring your YARN services</a> before using Kerberos authentication for Elasticsearch.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="kerberos-spark-credprovider"></a>EsServiceCredentialProvider<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/7.5/docs/src/reference/asciidoc/core/kerberos.adoc">edit</a></h3>
</div></div></div>
<p>Before Spark submits an application to a YARN cluster,
<a href="https://spark.apache.org/docs/2.1.0/running-on-yarn.html#running-in-a-secure-cluster" class="ulink" target="_top">it loads a number of
credential provider implementations</a> that are used to determine if any additional credentials must be obtained before
the application is started. These implementations are loaded using Java&#8217;s <code class="literal">ServiceLoader</code> architecture. Thus, any jar
that is on the classpath when the Spark application is submitted can offer implementations to be loaded and used.
<code class="literal">EsServiceCredentialProvider</code> is one such implementation that is loaded whenever elasticsearch-hadoop is on the job&#8217;s classpath.</p>
<p>Once loaded, <code class="literal">EsServiceCredentialProvider</code> determines if Kerberos authentication is enabled for elasticsearch-hadoop. If it is determined
that Kerberos authentication is enabled for elasticsearch-hadoop, then the credential provider will automatically obtain delegation tokens
from Elasticsearch and add them to the credentials on the YARN application submission context. Additionally, in the case that
the job is a long lived process like a Spark Streaming job, the credential provider is used to update or obtain new
delegation tokens when the current tokens approach their expiration time.</p>
<p>The time that Spark&#8217;s credential providers are loaded and called depends on the cluster deploy mode when submitting your
Spark app. When running in <code class="literal">client</code> deploy mode, Spark runs the user&#8217;s driver code in the local JVM, and launches the
YARN application to oversee the processing as needed. The providers are loaded and run whenever the YARN application
first comes online. When running in <code class="literal">cluster</code> deploy mode, Spark launches the YARN application immediately, and the
user&#8217;s driver code is run from the resulting Application Master in YARN. The providers are loaded and run <em>immediately</em>,
before any user code is executed.</p>
<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="kerberos-spark-credprovider-conf"></a>Configuring the credential provider<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/7.5/docs/src/reference/asciidoc/core/kerberos.adoc">edit</a></h4>
</div></div></div>
<p>All implementations of the Spark credential providers use settings from only a few places:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
The entries from the local Hadoop configuration files
</li>
<li class="listitem">
The entries of the local Spark configuration file
</li>
<li class="listitem">
The entries that are specified from the command line when the job is initially launched
</li>
</ol>
</div>
<p>Settings that are configured from the user code are not used because the provider must run once for all jobs that are
submitted for a particular Spark application. User code is not guaranteed to be run before the provider is loaded.
To make things more complicated, a credential provider is only given the local Hadoop configuration to determine if
they should load delegation tokens.</p>
<p>These limitations mean that the settings to configure elasticsearch-hadoop for Kerberos authentication need to be in specific places:</p>
<p>First, <code class="literal">es.security.authentication</code> MUST be set in the local Hadoop configuration files as <em>kerberos</em>. If it is not set
in the Hadoop configurations, then the credential provider will assume that <em>simple</em> authentication is to be used, and
will not obtain delegation tokens.</p>
<p>Secondly, all general connection settings for elasticsearch-hadoop (like <code class="literal">es.nodes</code>, <code class="literal">es.ssl.enabled</code>, etc&#8230;&#8203;) must be specified either
<a class="xref" href="kerberos.html#kerberos-yarn" title="Kerberos on YARN">in the local Hadoop configuration files</a>, in the local Spark configuration file, or from the command
line. If these settings are not available here, then the credential provider will not be able to contact Elasticsearch in order
to obtain the delegation tokens that it requires.</p>
<div class="pre_wrapper lang-bash">
<pre class="programlisting prettyprint lang-bash">$&gt; bin/spark-submit \
    --class org.myproject.MyClass \
    --master yarn \
    --deploy-mode cluster \
    --jars path/to/elasticsearch-hadoop.jar \
    --conf 'spark.es.nodes=es-node-1,es-node-2,es-node-3' <a id="CO114-1"></a><i class="conum" data-value="1"></i>
    --conf 'spark.es.ssl.enabled=true'
    --conf 'spark.es.net.spnego.auth.elasticsearch.principal=HTTP/_HOST@REALM' <a id="CO114-2"></a><i class="conum" data-value="2"></i>
    path/to/jar.jar</pre>
</div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO114-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>An example of some connection settings specified at submission time</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO114-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p>Be sure to include the Elasticsearch service principal.</p>
</td>
</tr>
</table>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>Specifying this many configurations in the spark-submit command line is a pretty sure fire way to miss important
settings. Thus, it is advised to set them in the <a class="xref" href="kerberos.html#kerberos-yarn" title="Kerberos on YARN">cluster wide Hadoop config</a>.</p>
</div>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="kerberos-spark-credprovider-streaming"></a>Renewing credentials for streaming jobs<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/7.5/docs/src/reference/asciidoc/core/kerberos.adoc">edit</a></h4>
</div></div></div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>In the event that you are running a streaming job, it is best to use the <code class="literal">cluster</code> deploy mode to allow YARN to
manage running the driver code for the streaming application.</p>
</div>
</div>
<p>Since streaming jobs are expected to run continuously without stopping, you should configure Spark so that the
credential provider can obtain new tokens before the original tokens expire.</p>
<p>Configuring Spark to obtain new tokens is different from <a class="xref" href="kerberos.html#kerberos-yarn" title="Kerberos on YARN">configuring YARN to renew and cancel
tokens</a>. YARN can only renew existing tokens up to their maximum lifetime. Tokens from Elasticsearch are not renewable.
Instead, they have a simple lifetime of 7 days. After those 7 days elapse, the tokens are expired. In order for
an ongoing streaming job to continue running without interruption, completely new tokens must be obtained and
sent to worker tasks. Spark has facilities for automatically obtaining and distributing completely new tokens
once the original token lifetime has ended.</p>
<p>When submitting a Spark application on YARN, users can provide a principal and keytab file to the <code class="literal">spark-submit</code>
command. Spark will log in with these credentials instead of depending on the local Kerberos TGT Cache for the current
user. In the event that any delegation tokens are close to expiring, the loaded credential providers are given the
chance to obtain new tokens using the given principal and keytab before the current tokens fully expire. Any new tokens
are automatically distributed by Spark to the containers on the YARN cluster.</p>
<div class="pre_wrapper lang-bash">
<pre class="programlisting prettyprint lang-bash">$&gt; bin/spark-submit \
    --class org.myproject.MyClass \
    --master yarn \ <a id="CO115-1"></a><i class="conum" data-value="1"></i>
    --deploy-mode cluster \ <a id="CO115-2"></a><i class="conum" data-value="2"></i>
    --jars path/to/elasticsearch-hadoop.jar \
    --principal client@REALM <a id="CO115-3"></a><i class="conum" data-value="3"></i>
    --keytab path/to/keytab.kt \ <a id="CO115-4"></a><i class="conum" data-value="4"></i>
    path/to/jar.jar</pre>
</div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO115-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>YARN deployment is required for Kerberos</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO115-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p>Use cluster deploy mode to allow for the driver to be run in the YARN Application Master</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO115-3"><i class="conum" data-value="3"></i></a></p>
</td>
<td align="left" valign="top">
<p>Specify the principal to run the job as</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO115-4"><i class="conum" data-value="4"></i></a></p>
</td>
<td align="left" valign="top">
<p>The path to the keytab that will be used to reauthenticate when credentials expire</p>
</td>
</tr>
</table>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="kerberos-spark-credprovider-disable"></a>Disabling the credential provider<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/7.5/docs/src/reference/asciidoc/core/kerberos.adoc">edit</a></h4>
</div></div></div>
<p>When elasticsearch-hadoop is on the classpath, <code class="literal">EsServiceCredentialProvider</code> is ALWAYS loaded by Spark. If Kerberos authentication is
enabled for elasticsearch-hadoop in the local Hadoop configuration, then the provider will attempt to load delegation tokens for Elasticsearch
regardless of if they are needed for that particular job.</p>
<p>It is advised that you do not add elasticsearch-hadoop libraries to jobs that are not configured to connect to or interact with Elasticsearch.
This is the easiest way to avoid the confusion of unrelated jobs failing to launch because they cannot connect to Elasticsearch.</p>
<p>If you find yourself in a place where you cannot easily remove elasticsearch-hadoop from the classpath of jobs that do not need to
interact with Elasticsearch, then you can explicitly disable the credential provider by setting a property at launch time.
The property to set is dependent on your version of Spark:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
For Spark 2.3.0 and up: set the <code class="literal">spark.security.credentials.elasticsearch.enabled</code> property to <code class="literal">false</code>.
</li>
<li class="listitem">
For Spark 2.1.0-2.3.0: set the <code class="literal">spark.yarn.security.credentials.elasticsearch.enabled</code> property to <code class="literal">false</code>. This
property is still accepted in Spark 2.3.0+, but is marked as deprecated.
</li>
</ul>
</div>
</div>

</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="kerberos-storm"></a>Kerberos with Storm<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/7.5/docs/src/reference/asciidoc/core/kerberos.adoc">edit</a></h2>
</div></div></div>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="kerberos-storm-requirements"></a>Requirements<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/7.5/docs/src/reference/asciidoc/core/kerberos.adoc">edit</a></h3>
</div></div></div>
<p>Your Storm deployment should be secured, but configuring it for security is not strictly required.</p>
<p>Storm is not always deployed alongside a Hadoop distribution. Thus, configuring Kerberos authentication for Hadoop is not
required for using Kerberos authentication to Elasticsearch on Storm.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="kerberos-storm-autocred"></a>Using Storm&#8217;s AutoCredential plugins<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/7.5/docs/src/reference/asciidoc/core/kerberos.adoc">edit</a></h3>
</div></div></div>
<p>Storm provides a <a href="http://storm.apache.org/releases/2.0.0-SNAPSHOT/SECURITY.html#automatic-credentials-push-and-renewal" class="ulink" target="_top">
myriad of plugin interfaces</a> that can be loaded and used to collect, update, and renew credentials over the lifetime of
a running topology. elasticsearch-hadoop provides the <code class="literal">AutoElasticsearch</code> class which Storm can use to automatically obtain and renew
Elasticsearch delegation tokens for a topology.</p>
<p><code class="literal">AutoElasticsearch</code> implements Storm&#8217;s <code class="literal">INimbusCredentialPlugin</code>, <code class="literal">IAutoCredentials</code>, and <code class="literal">ICredentialsRenewer</code>
interfaces. The first of which is used to obtain delegation tokens on Nimbus before submitting a topology. The second
is used for updating the credentials on the worker nodes, and the third is used for obtaining new delegation tokens
when the current tokens are close to expiring.</p>
<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="kerberos-storm-autocred-settings"></a>Configuring AutoElasticsearch<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/7.5/docs/src/reference/asciidoc/core/kerberos.adoc">edit</a></h4>
</div></div></div>
<p>In order for the <code class="literal">AutoElasticsearch</code> plugin to obtain credentials, Kerberos authentication must be enabled for elasticsearch-hadoop in its
settings. You must specify the <code class="literal">es.security.authentication</code> setting in either the storm.yaml file or on the topology
configuration.</p>
<p>The <code class="literal">AutoElasticsearch</code> plugin provides two settings for denoting the principal and keytab to be used when executing:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">es.storm.autocredentials.user.principal</code> (default none)
</span>
</dt>
<dd>
Required. The principal that the plugin should use for obtaining credentials for this topology. Can be set in the storm.yaml
configuration or in the topology configuration.
</dd>
<dt>
<span class="term">
<code class="literal">es.storm.autocredentials.user.keytab</code> (default none)
</span>
</dt>
<dd>
Required. The path to the keytab on Nimbus that will be used for logging in as the given principal. This can be set in the
storm.yaml configuration or in the topology configuration. The file must exist on Nimbus.
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="kerberos-storm-autocred-nimbus"></a>Configuring Nimbus<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/7.5/docs/src/reference/asciidoc/core/kerberos.adoc">edit</a></h4>
</div></div></div>
<p>Nimbus must be configured to use <code class="literal">AutoElasticsearch</code> as a credential plugin from the <code class="literal">storm.yaml</code> configuration file.
It is safe to specify <code class="literal">AutoElasticsearch</code> in these settings even if your topology does not interact with Elasticsearch. The
plugin will perform no operations unless <code class="literal">AutoElasticsearch</code> is explicitly enabled on the topology.</p>
<div class="pre_wrapper lang-yaml">
<pre class="programlisting prettyprint lang-yaml">nimbus.autocredential.plugins.classes: ["org.elasticsearch.storm.security.AutoElasticsearch"] <a id="CO116-1"></a><i class="conum" data-value="1"></i>
nimbus.credential.renewers.classes: ["org.elasticsearch.storm.security.AutoElasticsearch"] <a id="CO116-2"></a><i class="conum" data-value="2"></i>
nimbus.credential.renewers.freq.secs: 30 <a id="CO116-3"></a><i class="conum" data-value="3"></i></pre>
</div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO116-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>The list of auto credential plugins to be run on Nimbus when submitting a topology</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO116-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p>The list of all the credential renewers available for Nimbus to run</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO116-3"><i class="conum" data-value="3"></i></a></p>
</td>
<td align="left" valign="top">
<p>The frequency at which the credential renewers on Nimbus should be executed to check and update credentials.</p>
</td>
</tr>
</table>
</div>
<p>In order for the plugin to be loaded, elasticsearch-hadoop must be present on the Nimbus classpath. You can add it to the classpath by
using an environment variable on Nimbus.</p>
<div class="pre_wrapper lang-bash">
<pre class="programlisting prettyprint lang-bash">export STORM_EXT_CLASSPATH=/path/to/elasticsearch-hadoop.jar</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="kerberos-storm-autocred-topology"></a>Configuring topologies<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch-hadoop/edit/7.5/docs/src/reference/asciidoc/core/kerberos.adoc">edit</a></h4>
</div></div></div>
<p>Once Nimbus is configured, you must add <code class="literal">AutoElasticsearch</code> to your topology configuration in order for delegation
tokens to be obtained and updated. If you do not specify it in the topology configuration, then Storm will not attempt
to obtain Elasticsearch delegation tokens when the topology is submitted.</p>
<div class="pre_wrapper lang-java">
<pre class="programlisting prettyprint lang-java">Config conf = new Config();
List plugins = new ArrayList();
plugins.add(AutoElasticsearch.class.getName());
conf.put(Config.TOPOLOGY_AUTO_CREDENTIALS, plugins); <a id="CO117-1"></a><i class="conum" data-value="1"></i>
...
conf.put(ConfigurationOptions.ES_SECURITY_AUTHENTICATION, "kerberos"); <a id="CO117-2"></a><i class="conum" data-value="2"></i>
conf.put(ConfigurationOptions.ES_NET_SPNEGO_AUTH_ELASTICSEARCH_PRINCIPAL, "HTTP/elasticsearch.node.address@REALM");
...</pre>
</div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO117-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>Configure the topology with <code class="literal">AutoElasticsearch</code> as one of the auto credential plugins for the topology. This list
of plugins may contain other auto credential plugins if you have need of them.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO117-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p>If you have not enabled Kerberos authentication for elasticsearch-hadoop in the storm.yaml configuration file, you will need to set
the properties here.</p>
</td>
</tr>
</table>
</div>
</div>

</div>

</div>

</div>
<div class="navfooter">
<span class="prev">
<a href="errorhandlers.html">« Error Handlers</a>
</span>
<span class="next">
<a href="metrics.html">Hadoop Metrics »</a>
</span>
</div>
</div>
</body>
</html>
