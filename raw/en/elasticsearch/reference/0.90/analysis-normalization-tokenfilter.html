<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Normalization Token Filter | Reference [0.90] | Elastic</title>
<link rel="home" href="index.html" title="Reference [0.90]"/>
<link rel="up" href="analysis-tokenfilters.html" title="Token Filters"/>
<link rel="prev" href="analysis-common-grams-tokenfilter.html" title="Common Grams Token Filter"/>
<link rel="next" href="analysis-keep-words-tokenfilter.html" title="Keep Words Token Filter"/>
<meta name="DC.type" content="Learn/Docs/Elasticsearch/Reference/0.90"/>
<meta name="DC.subject" content="Elasticsearch"/>
<meta name="DC.identifier" content="0.90"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body><div class="page_header">
<p>
  <strong>WARNING</strong>: Version 0.90 of Elasticsearch has passed its 
  <a href="https://www.elastic.co/support/eol">EOL date</a>. 
</p>  
<p>
  This documentation is no longer being maintained and may be removed. 
  If you are running this version, we strongly advise you to upgrade. 
  For the latest information, see the 
  <a href="../current/index.html">current release documentation</a>. 
</p>
</div>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="index.html">Reference [0.90]</a></span>
»
<span class="breadcrumb-link"><a href="analysis.html">Analysis</a></span>
»
<span class="breadcrumb-link"><a href="analysis-tokenfilters.html">Token Filters</a></span>
»
<span class="breadcrumb-node">Normalization Token Filter</span>
</div>
<div class="navheader">
<span class="prev">
<a href="analysis-common-grams-tokenfilter.html">« Common Grams Token Filter</a>
</span>
<span class="next">
<a href="analysis-keep-words-tokenfilter.html">Keep Words Token Filter »</a>
</span>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-normalization-tokenfilter"></a>Normalization Token Filter<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/0.90/docs/reference/analysis/tokenfilters/normalization-tokenfilter.asciidoc">edit</a></h2>
</div></div></div>
<p>There are several token filters available which try to normalize special
characters of a certain language.</p>
<p>You can currently choose between <code class="literal">arabic_normalization</code> and
<code class="literal">persian_normalization</code> normalization in your token filter
configuration. For more information check the
<a href="http://lucene.apache.org/core/4_3_1/analyzers-common/org/apache/lucene/analysis/ar/ArabicNormalizer.html" class="ulink" target="_top">ArabicNormalizer</a>
or the
<a href="http://lucene.apache.org/core/4_3_1/analyzers-common/org/apache/lucene/analysis/fa/PersianNormalizer.html" class="ulink" target="_top">PersianNormalizer</a>
documentation.</p>
<p><span class="strong strong"><strong>Note:</strong></span> These filters are available since <code class="literal">0.90.2</code></p>
</div>
<div class="navfooter">
<span class="prev">
<a href="analysis-common-grams-tokenfilter.html">« Common Grams Token Filter</a>
</span>
<span class="next">
<a href="analysis-keep-words-tokenfilter.html">Keep Words Token Filter »</a>
</span>
</div>
</div>
</body>
</html>
