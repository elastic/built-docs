<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Tokenizers | Elasticsearch Reference [5.0] | Elastic</title>
<link rel="home" href="index.html" title="Elasticsearch Reference [5.0]"/>
<link rel="up" href="analysis.html" title="Analysis"/>
<link rel="prev" href="analysis-custom-analyzer.html" title="Custom Analyzer"/>
<link rel="next" href="analysis-standard-tokenizer.html" title="Standard Tokenizer"/>
<meta name="DC.type" content="Learn/Docs/Elasticsearch/Reference/5.0"/>
<meta name="DC.subject" content="Elasticsearch"/>
<meta name="DC.identifier" content="5.0"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body><div class="page_header">
<p>
  <strong>WARNING</strong>: Version 5.0 of Elasticsearch has passed its 
  <a href="https://www.elastic.co/support/eol">EOL date</a>. 
</p>  
<p>
  This documentation is no longer being maintained and may be removed. 
  If you are running this version, we strongly advise you to upgrade. 
  For the latest information, see the 
  <a href="../current/index.html">current release documentation</a>. 
</p>
</div>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="index.html">Elasticsearch Reference [5.0]</a></span>
»
<span class="breadcrumb-link"><a href="analysis.html">Analysis</a></span>
»
<span class="breadcrumb-node">Tokenizers</span>
</div>
<div class="navheader">
<span class="prev">
<a href="analysis-custom-analyzer.html">« Custom Analyzer</a>
</span>
<span class="next">
<a href="analysis-standard-tokenizer.html">Standard Tokenizer »</a>
</span>
</div>
<div class="chapter">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-tokenizers"></a>Tokenizers<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/5.0/docs/reference/analysis/tokenizers.asciidoc">edit</a></h2>
</div></div></div>
<p>A <em>tokenizer</em>  receives a stream of characters, breaks it up into individual
<em>tokens</em> (usually individual words), and outputs a stream of <em>tokens</em>. For
instance, a <a class="xref" href="analysis-whitespace-tokenizer.html" title="Whitespace Analyzer"><code class="literal">whitespace</code></a> tokenizer breaks
text into tokens whenever it sees any whitespace.  It would convert the text
<code class="literal">"Quick brown fox!"</code> into the terms <code class="literal">[Quick, brown, fox!]</code>.</p>
<p>The tokenizer is also responsible for recording the order or <em>position</em> of
each term (used for phrase and word proximity queries) and the start and end
<em>character offsets</em> of the original word which the term represents (used for
highlighting search snippets).</p>
<p>Elasticsearch has a number of built in tokenizers which can be used to build
<a class="xref" href="analysis-custom-analyzer.html" title="Custom Analyzer">custom analyzers</a>.</p>
<h3><a id="_word_oriented_tokenizers"></a>Word Oriented Tokenizers<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/5.0/docs/reference/analysis/tokenizers.asciidoc">edit</a></h3>
<p>The following tokenizers are usually used for tokenizing full text into
individual words:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<a class="xref" href="analysis-standard-tokenizer.html" title="Standard Tokenizer">Standard Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">standard</code> tokenizer divides text into terms on word boundaries, as
defined by the Unicode Text Segmentation algorithm. It removes most
punctuation symbols. It is the best choice for most languages.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-letter-tokenizer.html" title="Letter Tokenizer">Letter Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">letter</code> tokenizer divides text into terms whenever it encounters a
character which is not a letter.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-lowercase-tokenizer.html" title="Lowercase Tokenizer">Lowercase Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">lowercase</code> tokenizer, like the <code class="literal">letter</code> tokenizer,  divides text into
terms whenever it encounters a character which is not a letter, but it also
lowercases all terms.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-whitespace-tokenizer.html" title="Whitespace Analyzer">Whitespace Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">whitespace</code> tokenizer divides text into terms whenever it encounters any
whitespace character.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-uaxurlemail-tokenizer.html" title="UAX URL Email Tokenizer">UAX URL Email Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">uax_url_email</code> tokenizer is like the <code class="literal">standard</code> tokenizer except that it
recognises URLs and email addresses as single tokens.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-classic-tokenizer.html" title="Classic Tokenizer">Classic Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">classic</code> tokenizer is a grammar based tokenizer for the English Language.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-thai-tokenizer.html" title="Thai Tokenizer">Thai Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">thai</code> tokenizer segments Thai text into words.
</dd>
</dl>
</div>
<h3><a id="_partial_word_tokenizers"></a>Partial Word Tokenizers<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/5.0/docs/reference/analysis/tokenizers.asciidoc">edit</a></h3>
<p>These tokenizers break up text or words into small fragments, for partial word
matching:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<a class="xref" href="analysis-ngram-tokenizer.html" title="NGram Tokenizer">N-Gram Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">ngram</code> tokenizer can break up text into words when it encounters any of
a list of specified characters (e.g. whitespace or punctuation), then it returns
n-grams of each word: a sliding window of continuous letters, e.g. <code class="literal">quick</code> &#8594;
<code class="literal">[qu, ui, ic, ck]</code>.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-edgengram-tokenizer.html" title="Edge NGram Tokenizer">Edge N-Gram Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">edge_ngram</code> tokenizer can break up text into words when it encounters any of
a list of specified characters (e.g. whitespace or punctuation), then it returns
n-grams of each word which are anchored to the start of the word, e.g. <code class="literal">quick</code> &#8594;
<code class="literal">[q, qu, qui, quic, quick]</code>.
</dd>
</dl>
</div>
<h3><a id="_structured_text_tokenizers"></a>Structured Text Tokenizers<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/5.0/docs/reference/analysis/tokenizers.asciidoc">edit</a></h3>
<p>The following tokenizers are usually used with structured text like
identifiers, email addresses, zip codes, and paths, rather than with full
text:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<a class="xref" href="analysis-keyword-tokenizer.html" title="Keyword Tokenizer">Keyword Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">keyword</code> tokenizer is a &#8220;noop&#8221; tokenizer that accepts whatever text it
is given and outputs the exact same text as a single term.  It can be combined
with token filters like <a class="xref" href="analysis-lowercase-tokenfilter.html" title="Lowercase Token Filter"><code class="literal">lowercase</code></a> to
normalise the analysed terms.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-pattern-tokenizer.html" title="Pattern Tokenizer">Pattern Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">pattern</code> tokenizer uses a regular expression to either split text into
terms whenever it matches a word separator, or to capture matching text as
terms.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-pathhierarchy-tokenizer.html" title="Path Hierarchy Tokenizer">Path Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">path_hierarchy</code> tokenizer takes a hierarchical value like a filesystem
path, splits on the path separator, and emits a term for each component in the
tree, e.g. <code class="literal">/foo/bar/baz</code> &#8594; <code class="literal">[/foo, /foo/bar, /foo/bar/baz ]</code>.
</dd>
</dl>
</div>












</div>
<div class="navfooter">
<span class="prev">
<a href="analysis-custom-analyzer.html">« Custom Analyzer</a>
</span>
<span class="next">
<a href="analysis-standard-tokenizer.html">Standard Tokenizer »</a>
</span>
</div>
</div>
</body>
</html>
