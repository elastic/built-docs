<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Collectors | Elasticsearch Reference [6.5] | Elastic</title>
<link rel="home" href="index.html" title="Elasticsearch Reference [6.5]"/>
<link rel="up" href="monitor-elasticsearch-cluster.html" title="Monitor a cluster"/>
<link rel="prev" href="monitoring-tribe.html" title="Configuring a tribe node to work with monitoring"/>
<link rel="next" href="es-monitoring-exporters.html" title="Exporters"/>
<meta name="DC.type" content="Learn/Docs/Elasticsearch/Reference/6.5"/>
<meta name="DC.subject" content="Elasticsearch"/>
<meta name="DC.identifier" content="6.5"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body><div class="page_header">
<strong>IMPORTANT</strong>: No additional bug fixes or documentation updates
will be released for this version. For the latest information, see the
<a href="../current/index.html">current release documentation</a>.
</div>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="index.html">Elasticsearch Reference [6.5]</a></span>
»
<span class="breadcrumb-link"><a href="monitor-elasticsearch-cluster.html">Monitor a cluster</a></span>
»
<span class="breadcrumb-node">Collectors</span>
</div>
<div class="navheader">
<span class="prev">
<a href="monitoring-tribe.html">« Configuring a tribe node to work with monitoring</a>
</span>
<span class="next">
<a href="es-monitoring-exporters.html">Exporters »</a>
</span>
</div>
<div class="chapter xpack">
<div class="titlepage"><div><div>
<h2 class="title"><a id="es-monitoring-collectors"></a>Collectors<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/6.5/docs/reference/monitoring/collectors.asciidoc">edit</a><a class="xpack_tag" href="/subscriptions"></a></h2>
</div></div></div>
<p>Collectors, as their name implies, collect things. Each collector runs once for
each collection interval to obtain data from the public APIs in Elasticsearch and X-Pack
that it chooses to monitor. When the data collection is finished, the data is
handed in bulk to the <a class="xref" href="es-monitoring-exporters.html" title="Exporters">exporters</a> to be sent to the
monitoring clusters. Regardless of the number of exporters, each collector only
runs once per collection interval.</p>
<p>There is only one collector per data type gathered. In other words, for any
monitoring document that is created, it comes from a single collector rather
than being merged from multiple collectors. X-Pack monitoring for Elasticsearch currently has
a few collectors because the goal is to minimize overlap between them for
optimal performance.</p>
<p>Each collector can create zero or more monitoring documents. For example,
the <code class="literal">index_stats</code> collector collects all index statistics at the same time to
avoid many unnecessary calls.</p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Collector</th>
<th align="left" valign="top">Data Types</th>
<th align="left" valign="top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p>Cluster Stats</p></td>
<td align="left" valign="top"><p><code class="literal">cluster_stats</code></p></td>
<td align="left" valign="top"><p>Gathers details about the cluster state, including parts of the actual cluster
state (for example <code class="literal">GET /_cluster/state</code>) and statistics about it (for example,
<code class="literal">GET /_cluster/stats</code>). This produces a single document type. In versions prior
to X-Pack 5.5, this was actually three separate collectors that resulted in
three separate types: <code class="literal">cluster_stats</code>, <code class="literal">cluster_state</code>, and <code class="literal">cluster_info</code>. In
5.5 and later, all three are combined into <code class="literal">cluster_stats</code>. This only runs on
the <em>elected</em> master node and the data collected (<code class="literal">cluster_stats</code>) largely
controls the UI. When this data is not present, it indicates either a
misconfiguration on the elected master node, timeouts related to the collection
of the data, or issues with storing the data. Only a single document is produced
per collection.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Index Stats</p></td>
<td align="left" valign="top"><p><code class="literal">indices_stats</code>, <code class="literal">index_stats</code></p></td>
<td align="left" valign="top"><p>Gathers details about the indices in the cluster, both in summary and
individually. This creates many documents that represent parts of the index
statistics output (for example, <code class="literal">GET /_stats</code>). This information only needs to
be collected once, so it is collected on the <em>elected</em> master node. The most
common failure for this collector relates to an extreme number of indices&#8201;&#8212;&#8201;and
therefore time to gather them&#8201;&#8212;&#8201;resulting in timeouts. One summary
<code class="literal">indices_stats</code> document is produced per collection and one <code class="literal">index_stats</code>
document is produced per index, per collection.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Index Recovery</p></td>
<td align="left" valign="top"><p><code class="literal">index_recovery</code></p></td>
<td align="left" valign="top"><p>Gathers details about index recovery in the cluster. Index recovery represents
the assignment of <em>shards</em> at the cluster level. If an index is not recovered,
it is not usable. This also corresponds to shard restoration via snapshots. This
information only needs to be collected once, so it is collected on the <em>elected</em>
master node. The most common failure for this collector relates to an extreme
number of shards&#8201;&#8212;&#8201;and therefore time to gather them&#8201;&#8212;&#8201;resulting in timeouts.
This creates a single document that contains all recoveries by default, which
can be quite large, but it gives the most accurate picture of recovery in the
production cluster.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Shards</p></td>
<td align="left" valign="top"><p><code class="literal">shards</code></p></td>
<td align="left" valign="top"><p>Gathers details about all <em>allocated</em> shards for all indices, particularly
including what node the shard is allocated to. This information only needs to be
collected once, so it is collected on the <em>elected</em> master node. The collector
uses the local cluster state to get the routing table without any network
timeout issues unlike most other collectors. Each shard is represented by a
separate monitoring document.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Jobs</p></td>
<td align="left" valign="top"><p><code class="literal">job_stats</code></p></td>
<td align="left" valign="top"><p>Gathers details about all machine learning job statistics (for example, <code class="literal">GET
/_xpack/ml/anomaly_detectors/_stats</code>). This information only needs to be
collected once, so it is collected on the <em>elected</em> master node. However, for
the master node to be able to perform the collection, the master node must have
<code class="literal">xpack.ml.enabled</code> set to true (default) and a license level that supports machine learning.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Node Stats</p></td>
<td align="left" valign="top"><p><code class="literal">node_stats</code></p></td>
<td align="left" valign="top"><p>Gathers details about the running node, such as memory utilization and CPU
usage (for example, <code class="literal">GET /_nodes/_local/stats</code>). This runs on <em>every</em> node with
X-Pack monitoring enabled. One common failure results in the timeout of the node
stats request due to too many segment files. As a result, the collector spends
too much time waiting for the file system stats to be calculated until it
finally times out. A single <code class="literal">node_stats</code> document is created per collection.
This is collected per node to help to discover issues with nodes communicating
with each other, but not with the monitoring cluster (for example, intermittent
network issues or memory pressure).</p></td>
</tr>
</tbody>
</table>
</div>
<p>X-Pack monitoring uses a single threaded scheduler to run the collection of Elasticsearch
monitoring data by all of the appropriate collectors on each node. This
scheduler is managed locally by each node and its interval is controlled by
specifying the <code class="literal">xpack.monitoring.collection.interval</code>, which defaults to 10
seconds (<code class="literal">10s</code>), at either the node or cluster level.</p>
<p>Fundamentally, each collector works on the same principle. Per collection
interval, each collector is checked to see whether it should run and then the
appropriate collectors run. The failure of an individual collector does not
impact any other collector.</p>
<p>Once collection has completed, all of the monitoring data is passed to the
exporters to route the monitoring data to the monitoring clusters.</p>
<p>If gaps exist in the monitoring charts in Kibana, it is typically because either
a collector failed or the monitoring cluster did not receive the data (for
example, it was being restarted). In the event that a collector fails, a logged
error should exist on the node that attempted to perform the collection.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>Collection is currently done serially, rather than in parallel, to avoid
      extra overhead on the elected master node. The downside to this approach
      is that collectors might observe a different version of the cluster state
      within the same collection period. In practice, this does not make a
      significant difference and running the collectors in parallel would not
      prevent such a possibility.</p>
</div>
</div>
<p>For more information about the configuration options for the collectors, see
<a class="xref" href="monitoring-settings.html#monitoring-collection-settings" title="Monitoring Collection Settings">Monitoring Collection Settings</a>.</p>
<h4><a id="es-monitoring-stack"></a>Collecting data from across the Elastic Stack<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/6.5/docs/reference/monitoring/collectors.asciidoc">edit</a></h4>
<p>X-Pack monitoring in Elasticsearch also receives monitoring data from other parts of the
Elastic Stack. In this way, it serves as an unscheduled monitoring data
collector for the stack.</p>
<p>By default, data collection is disabled. Elasticsearch monitoring data is not
collected and all monitoring data from other sources such as Kibana, Beats, and
Logstash is ignored. You must set <code class="literal">xpack.monitoring.collection.enabled</code> to <code class="literal">true</code>
to enable the collection of monitoring data. See <a class="xref" href="monitoring-settings.html" title="Monitoring settings in Elasticsearch">Monitoring settings</a>.</p>
<p>Once data is received, it is forwarded to the exporters
to be routed to the monitoring cluster like all monitoring data.</p>
<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<p>Because this stack-level "collector" lives outside of the collection
interval of X-Pack monitoring for Elasticsearch, it is not impacted by the
<code class="literal">xpack.monitoring.collection.interval</code> setting. Therefore, data is passed to the
exporters whenever it is received. This behavior can result in indices for Kibana,
Logstash, or Beats being created somewhat unexpectedly.</p>
</div>
</div>
<p>While the monitoring data is collected and processed, some production cluster
metadata is added to incoming documents. This metadata enables Kibana to link the
monitoring data to the appropriate cluster. If this linkage is unimportant to
the infrastructure that you&#8217;re monitoring, it might be simpler to configure
Logstash and Beats to report monitoring data directly to the monitoring cluster.
This scenario also prevents the production cluster from adding extra overhead
related to monitoring data, which can be very useful when there are a large
number of Logstash nodes or Beats.</p>
<p>For more information about typical monitoring architectures, see
<a class="xref" href="monitoring-overview.html" title="Monitoring overview"><em>Overview</em></a>.</p>
</div>
<div class="navfooter">
<span class="prev">
<a href="monitoring-tribe.html">« Configuring a tribe node to work with monitoring</a>
</span>
<span class="next">
<a href="es-monitoring-exporters.html">Exporters »</a>
</span>
</div>
</div>
</body>
</html>
