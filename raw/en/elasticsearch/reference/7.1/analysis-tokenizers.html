<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Tokenizers | Elasticsearch Reference [7.1] | Elastic</title>
<link rel="home" href="index.html" title="Elasticsearch Reference [7.1]"/>
<link rel="up" href="analysis.html" title="Analysis"/>
<link rel="prev" href="analysis-normalizers.html" title="Normalizers"/>
<link rel="next" href="analysis-standard-tokenizer.html" title="Standard Tokenizer"/>
<meta name="DC.type" content="Learn/Docs/Elasticsearch/Reference/7.1"/>
<meta name="DC.subject" content="Elasticsearch"/>
<meta name="DC.identifier" content="7.1"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body><div class="page_header">
<strong>IMPORTANT</strong>: No additional bug fixes or documentation updates
will be released for this version. For the latest information, see the
<a href="../current/index.html">current release documentation</a>.
</div>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="index.html">Elasticsearch Reference [7.1]</a></span>
»
<span class="breadcrumb-link"><a href="analysis.html">Analysis</a></span>
»
<span class="breadcrumb-node">Tokenizers</span>
</div>
<div class="navheader">
<span class="prev">
<a href="analysis-normalizers.html">« Normalizers</a>
</span>
<span class="next">
<a href="analysis-standard-tokenizer.html">Standard Tokenizer »</a>
</span>
</div>
<div class="chapter">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-tokenizers"></a>Tokenizers<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/7.1/docs/reference/analysis/tokenizers.asciidoc">edit</a></h2>
</div></div></div>
<p>A <em>tokenizer</em>  receives a stream of characters, breaks it up into individual
<em>tokens</em> (usually individual words), and outputs a stream of <em>tokens</em>. For
instance, a <a class="xref" href="analysis-whitespace-tokenizer.html" title="Whitespace Tokenizer"><code class="literal">whitespace</code></a> tokenizer breaks
text into tokens whenever it sees any whitespace.  It would convert the text
<code class="literal">"Quick brown fox!"</code> into the terms <code class="literal">[Quick, brown, fox!]</code>.</p>
<p>The tokenizer is also responsible for recording the order or <em>position</em> of
each term (used for phrase and word proximity queries) and the start and end
<em>character offsets</em> of the original word which the term represents (used for
highlighting search snippets).</p>
<p>Elasticsearch has a number of built in tokenizers which can be used to build
<a class="xref" href="analysis-custom-analyzer.html" title="Custom Analyzer">custom analyzers</a>.</p>
<h3><a id="_word_oriented_tokenizers"></a>Word Oriented Tokenizers<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/7.1/docs/reference/analysis/tokenizers.asciidoc">edit</a></h3>
<p>The following tokenizers are usually used for tokenizing full text into
individual words:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<a class="xref" href="analysis-standard-tokenizer.html" title="Standard Tokenizer">Standard Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">standard</code> tokenizer divides text into terms on word boundaries, as
defined by the Unicode Text Segmentation algorithm. It removes most
punctuation symbols. It is the best choice for most languages.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-letter-tokenizer.html" title="Letter Tokenizer">Letter Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">letter</code> tokenizer divides text into terms whenever it encounters a
character which is not a letter.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-lowercase-tokenizer.html" title="Lowercase Tokenizer">Lowercase Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">lowercase</code> tokenizer, like the <code class="literal">letter</code> tokenizer,  divides text into
terms whenever it encounters a character which is not a letter, but it also
lowercases all terms.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-whitespace-tokenizer.html" title="Whitespace Tokenizer">Whitespace Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">whitespace</code> tokenizer divides text into terms whenever it encounters any
whitespace character.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-uaxurlemail-tokenizer.html" title="UAX URL Email Tokenizer">UAX URL Email Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">uax_url_email</code> tokenizer is like the <code class="literal">standard</code> tokenizer except that it
recognises URLs and email addresses as single tokens.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-classic-tokenizer.html" title="Classic Tokenizer">Classic Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">classic</code> tokenizer is a grammar based tokenizer for the English Language.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-thai-tokenizer.html" title="Thai Tokenizer">Thai Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">thai</code> tokenizer segments Thai text into words.
</dd>
</dl>
</div>
<h3><a id="_partial_word_tokenizers"></a>Partial Word Tokenizers<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/7.1/docs/reference/analysis/tokenizers.asciidoc">edit</a></h3>
<p>These tokenizers break up text or words into small fragments, for partial word
matching:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<a class="xref" href="analysis-ngram-tokenizer.html" title="NGram Tokenizer">N-Gram Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">ngram</code> tokenizer can break up text into words when it encounters any of
a list of specified characters (e.g. whitespace or punctuation), then it returns
n-grams of each word: a sliding window of continuous letters, e.g. <code class="literal">quick</code> &#8594;
<code class="literal">[qu, ui, ic, ck]</code>.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-edgengram-tokenizer.html" title="Edge NGram Tokenizer">Edge N-Gram Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">edge_ngram</code> tokenizer can break up text into words when it encounters any of
a list of specified characters (e.g. whitespace or punctuation), then it returns
n-grams of each word which are anchored to the start of the word, e.g. <code class="literal">quick</code> &#8594;
<code class="literal">[q, qu, qui, quic, quick]</code>.
</dd>
</dl>
</div>
<h3><a id="_structured_text_tokenizers"></a>Structured Text Tokenizers<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/7.1/docs/reference/analysis/tokenizers.asciidoc">edit</a></h3>
<p>The following tokenizers are usually used with structured text like
identifiers, email addresses, zip codes, and paths, rather than with full
text:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<a class="xref" href="analysis-keyword-tokenizer.html" title="Keyword Tokenizer">Keyword Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">keyword</code> tokenizer is a &#8220;noop&#8221; tokenizer that accepts whatever text it
is given and outputs the exact same text as a single term.  It can be combined
with token filters like <a class="xref" href="analysis-lowercase-tokenfilter.html" title="Lowercase Token Filter"><code class="literal">lowercase</code></a> to
normalise the analysed terms.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-pattern-tokenizer.html" title="Pattern Tokenizer">Pattern Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">pattern</code> tokenizer uses a regular expression to either split text into
terms whenever it matches a word separator, or to capture matching text as
terms.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-simplepattern-tokenizer.html" title="Simple Pattern Tokenizer">Simple Pattern Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">simple_pattern</code> tokenizer uses a regular expression to capture matching
text as terms. It uses a restricted subset of regular expression features
and is generally faster than the <code class="literal">pattern</code> tokenizer.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-chargroup-tokenizer.html" title="Char Group Tokenizer">Char Group Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">char_group</code> tokenizer is configurable through sets of characters to split
on, which is usually less expensive than running regular expressions.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-simplepatternsplit-tokenizer.html" title="Simple Pattern Split Tokenizer">Simple Pattern Split Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">simple_pattern_split</code> tokenizer uses the same restricted regular expression
subset as the <code class="literal">simple_pattern</code> tokenizer, but splits the input at matches rather
than returning the matches as terms.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-pathhierarchy-tokenizer.html" title="Path Hierarchy Tokenizer">Path Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">path_hierarchy</code> tokenizer takes a hierarchical value like a filesystem
path, splits on the path separator, and emits a term for each component in the
tree, e.g. <code class="literal">/foo/bar/baz</code> &#8594; <code class="literal">[/foo, /foo/bar, /foo/bar/baz ]</code>.
</dd>
</dl>
</div>
















</div>
<div class="navfooter">
<span class="prev">
<a href="analysis-normalizers.html">« Normalizers</a>
</span>
<span class="next">
<a href="analysis-standard-tokenizer.html">Standard Tokenizer »</a>
</span>
</div>
</div>
</body>
</html>
