<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Standard Tokenizer | Elasticsearch Reference [7.6] | Elastic</title>
<link rel="home" href="index.html" title="Elasticsearch Reference [7.6]"/>
<link rel="up" href="analysis-tokenizers.html" title="Tokenizer reference"/>
<link rel="prev" href="analysis-simplepatternsplit-tokenizer.html" title="Simple Pattern Split Tokenizer"/>
<link rel="next" href="analysis-thai-tokenizer.html" title="Thai Tokenizer"/>
<meta name="DC.type" content="Learn/Docs/Elasticsearch/Reference/7.6"/>
<meta name="DC.subject" content="Elasticsearch"/>
<meta name="DC.identifier" content="7.6"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body><div class="page_header">
<strong>IMPORTANT</strong>: No additional bug fixes or documentation updates
will be released for this version. For the latest information, see the
<a href="../current/index.html">current release documentation</a>.
</div>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="index.html">Elasticsearch Reference [7.6]</a></span>
»
<span class="breadcrumb-link"><a href="analysis.html">Text analysis</a></span>
»
<span class="breadcrumb-link"><a href="analysis-tokenizers.html">Tokenizer reference</a></span>
»
<span class="breadcrumb-node">Standard Tokenizer</span>
</div>
<div class="navheader">
<span class="prev">
<a href="analysis-simplepatternsplit-tokenizer.html">« Simple Pattern Split Tokenizer</a>
</span>
<span class="next">
<a href="analysis-thai-tokenizer.html">Thai Tokenizer »</a>
</span>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-standard-tokenizer"></a>Standard Tokenizer<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/7.6/docs/reference/analysis/tokenizers/standard-tokenizer.asciidoc">edit</a></h2>
</div></div></div>
<p>The <code class="literal">standard</code> tokenizer provides grammar based tokenization (based on the
Unicode Text Segmentation algorithm, as specified in
<a href="http://unicode.org/reports/tr29/" class="ulink" target="_top">Unicode Standard Annex #29</a>) and works well
for most languages.</p>
<h3><a id="_example_output_17"></a>Example output<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/7.6/docs/reference/analysis/tokenizers/standard-tokenizer.asciidoc">edit</a></h3>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">POST _analyze
{
  "tokenizer": "standard",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/814.console"></div>
<p>The above sentence would produce the following terms:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ The, 2, QUICK, Brown, Foxes, jumped, over, the, lazy, dog's, bone ]</pre>
</div>
<h3><a id="_configuration_20"></a>Configuration<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/7.6/docs/reference/analysis/tokenizers/standard-tokenizer.asciidoc">edit</a></h3>
<p>The <code class="literal">standard</code> tokenizer accepts the following parameters:</p>
<div class="informaltable">
<table border="0" cellpadding="4px">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody valign="top">
<tr>
<td valign="top">
<p>
<code class="literal">max_token_length</code>
</p>
</td>
<td valign="top">
<p>
The maximum token length. If a token is seen that exceeds this length then
it is split at <code class="literal">max_token_length</code> intervals. Defaults to <code class="literal">255</code>.
</p>
</td>
</tr>
</tbody>
</table>
</div>
<h3><a id="_example_configuration_13"></a>Example configuration<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/7.6/docs/reference/analysis/tokenizers/standard-tokenizer.asciidoc">edit</a></h3>
<p>In this example, we configure the <code class="literal">standard</code> tokenizer to have a
<code class="literal">max_token_length</code> of 5 (for demonstration purposes):</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "standard",
          "max_token_length": 5
        }
      }
    }
  }
}

POST my_index/_analyze
{
  "analyzer": "my_analyzer",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/815.console"></div>
<p>The above example produces the following terms:</p>
<div class="pre_wrapper lang-text">
<pre class="programlisting prettyprint lang-text">[ The, 2, QUICK, Brown, Foxes, jumpe, d, over, the, lazy, dog's, bone ]</pre>
</div>
</div>
<div class="navfooter">
<span class="prev">
<a href="analysis-simplepatternsplit-tokenizer.html">« Simple Pattern Split Tokenizer</a>
</span>
<span class="next">
<a href="analysis-thai-tokenizer.html">Thai Tokenizer »</a>
</span>
</div>
</div>
</body>
</html>
