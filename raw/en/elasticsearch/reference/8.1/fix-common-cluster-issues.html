<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Fix common cluster issues | Elasticsearch Guide [8.1] | Elastic</title>
<meta class="elastic" name="content" content="Fix common cluster issues | Elasticsearch Guide [8.1]">

<link rel="home" href="index.html" title="Elasticsearch Guide [8.1]"/>
<link rel="up" href="how-to.html" title="How to"/>
<link rel="prev" href="tune-for-disk-usage.html" title="Tune for disk usage"/>
<link rel="next" href="size-your-shards.html" title="Size your shards"/>
<meta class="elastic" name="product_version" content="8.1"/>
<meta class="elastic" name="product_name" content="Elasticsearch"/>
<meta class="elastic" name="website_area" content="documentation"/>
<meta name="DC.type" content="Learn/Docs/Elasticsearch/Reference/8.1"/>
<meta name="DC.subject" content="Elasticsearch"/>
<meta name="DC.identifier" content="8.1"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body><div class="page_header">
<strong>IMPORTANT</strong>: No additional bug fixes or documentation updates
will be released for this version. For the latest information, see the
<a href="../current/index.html">current release documentation</a>.
</div>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="index.html">Elasticsearch Guide [8.1]</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="how-to.html">How to</a></span>
</div>
<div class="navheader">
<span class="prev">
<a href="tune-for-disk-usage.html">« Tune for disk usage</a>
</span>
<span class="next">
<a href="size-your-shards.html">Size your shards »</a>
</span>
</div>
<div class="chapter">
<div class="titlepage"><div><div>
<h2 class="title"><a id="fix-common-cluster-issues"></a>Fix common cluster issues<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.1/docs/reference/how-to/fix-common-cluster-issues.asciidoc">edit</a></h2>
</div></div></div>
<p>This guide describes how to fix common errors and problems with Elasticsearch clusters.</p>
<h3><a id="_error_disk_usage_exceeded_flood_stage_watermark_index_has_read_only_allow_delete_block"></a>Error: disk usage exceeded flood-stage watermark, index has read-only-allow-delete block<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.1/docs/reference/how-to/fix-common-cluster-issues.asciidoc">edit</a></h3>
<p>This error indicates a data node is critically low on disk space and has reached
the <a class="xref" href="modules-cluster.html#cluster-routing-flood-stage">flood-stage disk usage watermark</a>. To prevent
a full disk, when a node reaches this watermark, Elasticsearch blocks writes to any index
with a shard on the node. If the block affects related system indices, Kibana and
other Elastic Stack features may become unavailable.</p>
<p>Elasticsearch will automatically remove the write block when the affected node&#8217;s disk
usage goes below the <a class="xref" href="modules-cluster.html#cluster-routing-watermark-high">high disk watermark</a>. To
achieve this, Elasticsearch automatically moves some of the affected node&#8217;s shards to
other nodes in the same data tier.</p>
<p>To verify that shards are moving off the affected node, use the <a class="xref" href="cat-shards.html" title="cat shards API">cat
shards API</a>.</p>
<a id="a1070cf2f5969d42d71cda057223f152"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _cat/shards?v=true</pre>
</div>
<div class="console_widget" data-snippet="snippets/1720.console"></div>
<p>If shards remain on the node, use the <a class="xref" href="cluster-allocation-explain.html" title="Cluster allocation explain API">cluster
allocation explanation API</a> to get an explanation for their allocation status.</p>
<a id="934ced0998552cc95a28e48554147e8b"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _cluster/allocation/explain
{
  "index": "my-index",
  "shard": 0,
  "primary": false,
  "current_node": "my-node"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/1721.console"></div>
<p>To immediately restore write operations, you can temporarily increase the disk
watermarks and remove the write block.</p>
<a id="4060c130d56519faf63f58acffddb553"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT _cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.disk.watermark.low": "90%",
    "cluster.routing.allocation.disk.watermark.high": "95%",
    "cluster.routing.allocation.disk.watermark.flood_stage": "97%"
  }
}

PUT */_settings?expand_wildcards=all
{
  "index.blocks.read_only_allow_delete": null
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/1722.console"></div>
<p>As a long-term solution, we recommend you add nodes to the affected data tiers
or upgrade existing nodes to increase disk space. To free up additional disk
space, you can delete unneeded indices using the <a class="xref" href="indices-delete-index.html" title="Delete index API">delete
index API</a>.</p>
<a id="72d33fbd72b0766b2f14ea27d9ccf0fa"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">DELETE my-index</pre>
</div>
<div class="console_widget" data-snippet="snippets/1723.console"></div>
<p>When a long-term solution is in place, reset or reconfigure the disk watermarks.</p>
<a id="f9a97730b88d4b06c2944563a85b57d1"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT _cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.disk.watermark.low": null,
    "cluster.routing.allocation.disk.watermark.high": null,
    "cluster.routing.allocation.disk.watermark.flood_stage": null
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/1724.console"></div>
<h3><a id="circuit-breaker-errors"></a>Circuit breaker errors<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.1/docs/reference/how-to/fix-common-cluster-issues.asciidoc">edit</a></h3>
<p>Elasticsearch uses <a class="xref" href="circuit-breaker.html" title="Circuit breaker settings">circuit breakers</a> to prevent nodes from running out
of JVM heap memory. If Elasticsearch estimates an operation would exceed a
circuit breaker, it stops the operation and returns an error.</p>
<p>By default, the <a class="xref" href="circuit-breaker.html#parent-circuit-breaker" title="Parent circuit breaker">parent circuit breaker</a> triggers at
95% JVM memory usage. To prevent errors, we recommend taking steps to reduce
memory pressure if usage consistently exceeds 85%.</p>
<h4><a id="diagnose-circuit-breaker-errors"></a>Diagnose circuit breaker errors<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.1/docs/reference/how-to/fix-common-cluster-issues.asciidoc">edit</a></h4>
<p><span class="strong strong"><strong>Error messages</strong></span></p>
<p>If a request triggers a circuit breaker, Elasticsearch returns an error with a <code class="literal">429</code> HTTP
status code.</p>
<div class="pre_wrapper lang-js">
<pre class="programlisting prettyprint lang-js">{
  'error': {
    'type': 'circuit_breaking_exception',
    'reason': '[parent] Data too large, data for [&lt;http_request&gt;] would be [123848638/118.1mb], which is larger than the limit of [123273216/117.5mb], real usage: [120182112/114.6mb], new bytes reserved: [3666526/3.4mb]',
    'bytes_wanted': 123848638,
    'bytes_limit': 123273216,
    'durability': 'TRANSIENT'
  },
  'status': 429
}</pre>
</div>
<p>Elasticsearch also writes circuit breaker errors to <a class="xref" href="logging.html" title="Logging"><code class="literal">elasticsearch.log</code></a>. This
is helpful when automated processes, such as allocation, trigger a circuit
breaker.</p>
<div class="pre_wrapper lang-txt">
<pre class="programlisting prettyprint lang-txt">Caused by: org.elasticsearch.common.breaker.CircuitBreakingException: [parent] Data too large, data for [&lt;transport_request&gt;] would be [num/numGB], which is larger than the limit of [num/numGB], usages [request=0/0b, fielddata=num/numKB, in_flight_requests=num/numGB, accounting=num/numGB]</pre>
</div>
<p><span class="strong strong"><strong>Check JVM memory usage</strong></span></p>
<p>If you&#8217;ve enabled Stack Monitoring, you can view JVM memory usage in Kibana. In
the main menu, click <span class="strong strong"><strong>Stack Monitoring</strong></span>. On the Stack Monitoring <span class="strong strong"><strong>Overview</strong></span>
page, click <span class="strong strong"><strong>Nodes</strong></span>. The <span class="strong strong"><strong>JVM Heap</strong></span> column lists the current memory usage
for each node.</p>
<p>You can also use the <a class="xref" href="cat-nodes.html" title="cat nodes API">cat nodes API</a> to get the current
<code class="literal">heap.percent</code> for each node.</p>
<a id="d8a82511cb94f49b4fe4828fee3ba074"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _cat/nodes?v=true&amp;h=name,node*,heap*</pre>
</div>
<div class="console_widget" data-snippet="snippets/1725.console"></div>
<p>To get the JVM memory usage for each circuit breaker, use the
<a class="xref" href="cluster-nodes-stats.html" title="Nodes stats API">node stats API</a>.</p>
<a id="e14a5a5a1c880031486bfff43031fa3a"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _nodes/stats/breaker</pre>
</div>
<div class="console_widget" data-snippet="snippets/1726.console"></div>
<h4><a id="prevent-circuit-breaker-errors"></a>Prevent circuit breaker errors<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.1/docs/reference/how-to/fix-common-cluster-issues.asciidoc">edit</a></h4>
<p><span class="strong strong"><strong>Reduce JVM memory pressure</strong></span></p>
<p>High JVM memory pressure often causes circuit breaker errors. See
<a class="xref" href="fix-common-cluster-issues.html#high-jvm-memory-pressure" title="High JVM memory pressure">High JVM memory pressure</a>.</p>
<p><span class="strong strong"><strong>Avoid using fielddata on <code class="literal">text</code> fields</strong></span></p>
<p>For high-cardinality <code class="literal">text</code> fields, fielddata can use a large amount of JVM
memory. To avoid this, Elasticsearch disables fielddata on <code class="literal">text</code> fields by default. If
you&#8217;ve enabled fielddata and triggered the <a class="xref" href="circuit-breaker.html#fielddata-circuit-breaker" title="Field data circuit breaker">fielddata
circuit breaker</a>, consider disabling it and using a <code class="literal">keyword</code> field instead.
See <a class="xref" href="fielddata.html" title="fielddata mapping parameter"><code class="literal">fielddata</code> mapping parameter</a>.</p>
<p><span class="strong strong"><strong>Clear the fieldata cache</strong></span></p>
<p>If you&#8217;ve triggered the fielddata circuit breaker and can&#8217;t disable fielddata,
use the <a class="xref" href="indices-clearcache.html" title="Clear cache API">clear cache API</a> to clear the fielddata cache.
This may disrupt any in-flight searches that use fielddata.</p>
<a id="3218f8ccd59c8c90349816e0428e8fb8"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">POST _cache/clear?fielddata=true</pre>
</div>
<div class="console_widget" data-snippet="snippets/1727.console"></div>
<h3><a id="high-cpu-usage"></a>High CPU usage<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.1/docs/reference/how-to/fix-common-cluster-issues.asciidoc">edit</a></h3>
<p>Elasticsearch uses <a class="xref" href="modules-threadpool.html" title="Thread pools">thread pools</a> to manage CPU resources for
concurrent operations. High CPU usage typically means one or more thread pools
are running low.</p>
<p>If a thread pool is depleted, Elasticsearch will <a class="xref" href="fix-common-cluster-issues.html#rejected-requests" title="Rejected requests">reject requests</a>
related to the thread pool. For example, if the <code class="literal">search</code> thread pool is
depleted, Elasticsearch will reject search requests until more threads are available.</p>
<h4><a id="diagnose-high-cpu-usage"></a>Diagnose high CPU usage<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.1/docs/reference/how-to/fix-common-cluster-issues.asciidoc">edit</a></h4>
<p><span class="strong strong"><strong>Check CPU usage</strong></span></p>
<div class="tabs" data-tab-group="host">
  <div role="tablist" aria-label="Check CPU usage">
    <button role="tab"
            aria-selected="true"
            aria-controls="cloud-tab-cpu"
            id="cloud-cpu">
      Elasticsearch Service
    </button>
    <button role="tab"
            aria-selected="false"
            aria-controls="self-managed-tab-cpu"
            id="self-managed-cpu"
            tabindex="-1">
      Self-managed
    </button>
  </div>
  <div tabindex="0"
       role="tabpanel"
       id="cloud-tab-cpu"
       aria-labelledby="cloud-cpu">
<p>From your deployment menu, click <span class="strong strong"><strong>Performance</strong></span>. The page&#8217;s <span class="strong strong"><strong>CPU Usage</strong></span> chart
shows your deployment&#8217;s CPU usage as a percentage.</p>
<p>High CPU usage can also deplete your CPU credits. CPU credits let Elasticsearch Service provide
smaller clusters with a performance boost when needed. The <span class="strong strong"><strong>CPU credits</strong></span>
chart shows your remaining CPU credits, measured in seconds of CPU time.</p>
<p>You can also use the <a class="xref" href="cat-nodes.html" title="cat nodes API">cat nodes API</a> to get the current CPU usage
for each node.</p>
<a id="1637ef51d673b35cc8894ee80cd61c87"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _cat/nodes?v=true&amp;s=cpu:desc</pre>
</div>
<div class="console_widget" data-snippet="snippets/1728.console"></div>
<p>The response&#8217;s <code class="literal">cpu</code> column contains the current CPU usage as a percentage. The
<code class="literal">node</code> column contains the node&#8217;s name.</p>
  </div>
  <div tabindex="0"
       role="tabpanel"
       id="self-managed-tab-cpu"
       aria-labelledby="self-managed-cpu"
       hidden="">
<p>Use the <a class="xref" href="cat-nodes.html" title="cat nodes API">cat nodes API</a> to get the current CPU usage for each node.</p>
<a id="1637ef51d673b35cc8894ee80cd61c87"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _cat/nodes?v=true&amp;s=cpu:desc</pre>
</div>
<div class="console_widget" data-snippet="snippets/1729.console"></div>
<p>The response&#8217;s <code class="literal">cpu</code> column contains the current CPU usage as a percentage. The
<code class="literal">node</code> column contains the node&#8217;s name.</p>
  </div>
</div>
<p><span class="strong strong"><strong>Check hot threads</strong></span></p>
<p>If a node has high CPU usage, use the <a class="xref" href="cluster-nodes-hot-threads.html" title="Nodes hot threads API">nodes hot
threads API</a> to check for resource-intensive threads running on the node.</p>
<a id="1745ac9e6d22a2ffe7ac381f9ba238f9"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _nodes/my-node,my-other-node/hot_threads</pre>
</div>
<div class="console_widget" data-snippet="snippets/1730.console"></div>
<p>This API returns a breakdown of any hot threads in plain text.</p>
<h4><a id="reduce-cpu-usage"></a>Reduce CPU usage<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.1/docs/reference/how-to/fix-common-cluster-issues.asciidoc">edit</a></h4>
<p>The following tips outline the most common causes of high CPU usage and their
solutions.</p>
<p><span class="strong strong"><strong>Scale your cluster</strong></span></p>
<p>Heavy indexing and search loads can deplete smaller thread pools. To better
handle heavy workloads, add more nodes to your cluster or upgrade your existing
nodes to increase capacity.</p>
<p><span class="strong strong"><strong>Spread out bulk requests</strong></span></p>
<p>While more efficient than individual requests, large <a class="xref" href="docs-bulk.html" title="Bulk API">bulk indexing</a>
or <a class="xref" href="search-multi-search.html" title="Multi search API">multi-search</a> requests still require CPU resources. If
possible, submit smaller requests and allow more time between them.</p>
<p><span class="strong strong"><strong>Cancel long-running searches</strong></span></p>
<p>Long-running searches can block threads in the <code class="literal">search</code> thread pool. To check
for these searches, use the <a class="xref" href="tasks.html" title="Task management API">task management API</a>.</p>
<a id="8f4a7f68f2ca3698abdf20026a2d8c5f"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _tasks?actions=*search&amp;detailed</pre>
</div>
<div class="console_widget" data-snippet="snippets/1731.console"></div>
<p>The response&#8217;s <code class="literal">description</code> contains the search request and its queries.
<code class="literal">running_time_in_nanos</code> shows how long the search has been running.</p>
<a id="a5a360d0325b5a8d67f2a87cf140dab9"></a>
<div class="pre_wrapper lang-console-result">
<pre class="programlisting prettyprint lang-console-result">{
  "nodes" : {
    "oTUltX4IQMOUUVeiohTt8A" : {
      "name" : "my-node",
      "transport_address" : "127.0.0.1:9300",
      "host" : "127.0.0.1",
      "ip" : "127.0.0.1:9300",
      "tasks" : {
        "oTUltX4IQMOUUVeiohTt8A:464" : {
          "node" : "oTUltX4IQMOUUVeiohTt8A",
          "id" : 464,
          "type" : "transport",
          "action" : "indices:data/read/search",
          "description" : "indices[my-index], search_type[QUERY_THEN_FETCH], source[{\"query\":...}]",
          "start_time_in_millis" : 4081771730000,
          "running_time_in_nanos" : 13991383,
          "cancellable" : true
        }
      }
    }
  }
}</pre>
</div>
<p>To cancel a search and free up resources, use the API&#8217;s <code class="literal">_cancel</code> endpoint.</p>
<a id="84c69fb07050f0e89720007a6507a221"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">POST _tasks/oTUltX4IQMOUUVeiohTt8A:464/_cancel</pre>
</div>
<div class="console_widget" data-snippet="snippets/1732.console"></div>
<p>For additional tips on how to track and avoid resource-intensive searches, see
<a class="xref" href="fix-common-cluster-issues.html#avoid-expensive-searches">Avoid expensive searches</a>.</p>
<h3><a id="high-jvm-memory-pressure"></a>High JVM memory pressure<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.1/docs/reference/how-to/fix-common-cluster-issues.asciidoc">edit</a></h3>
<p>High JVM memory usage can degrade cluster performance and trigger
<a class="xref" href="fix-common-cluster-issues.html#circuit-breaker-errors" title="Circuit breaker errors">circuit breaker errors</a>. To prevent this, we recommend
taking steps to reduce memory pressure if a node&#8217;s JVM memory usage consistently
exceeds 85%.</p>
<h4><a id="diagnose-high-jvm-memory-pressure"></a>Diagnose high JVM memory pressure<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.1/docs/reference/how-to/fix-common-cluster-issues.asciidoc">edit</a></h4>
<p><span class="strong strong"><strong>Check JVM memory pressure</strong></span></p>
<div class="tabs" data-tab-group="host">
  <div role="tablist" aria-label="Check JVM memory pressure">
    <button role="tab"
            aria-selected="true"
            aria-controls="cloud-tab"
            id="cloud-jvm">
      Elasticsearch Service
    </button>
    <button role="tab"
            aria-selected="false"
            aria-controls="self-managed-tab"
            id="self-managed-jvm"
            tabindex="-1">
      Self-managed
    </button>
  </div>
  <div tabindex="0"
       role="tabpanel"
       id="cloud-tab"
       aria-labelledby="cloud-jvm">
<p>From your deployment menu, click <span class="strong strong"><strong>Elasticsearch</strong></span>. Under <span class="strong strong"><strong>Instances</strong></span>, each
instance displays a <span class="strong strong"><strong>JVM memory pressure</strong></span> indicator. When the JVM memory
pressure reaches 75%, the indicator turns red.</p>
<p>You can also use the <a class="xref" href="cluster-nodes-stats.html" title="Nodes stats API">nodes stats API</a> to calculate the
current JVM memory pressure for each node.</p>
<a id="765c9c8b40b67a42121648045dbf10fb"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _nodes/stats?filter_path=nodes.*.jvm.mem.pools.old</pre>
</div>
<div class="console_widget" data-snippet="snippets/1733.console"></div>
<p>Use the response to calculate memory pressure as follows:</p>
<p>JVM Memory Pressure = <code class="literal">used_in_bytes</code> / <code class="literal">max_in_bytes</code></p>
  </div>
  <div tabindex="0"
       role="tabpanel"
       id="self-managed-tab"
       aria-labelledby="self-managed-jvm"
       hidden="">
<p>To calculate the current JVM memory pressure for each node, use the
<a class="xref" href="cluster-nodes-stats.html" title="Nodes stats API">nodes stats API</a>.</p>
<a id="765c9c8b40b67a42121648045dbf10fb"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _nodes/stats?filter_path=nodes.*.jvm.mem.pools.old</pre>
</div>
<div class="console_widget" data-snippet="snippets/1734.console"></div>
<p>Use the response to calculate memory pressure as follows:</p>
<p>JVM Memory Pressure = <code class="literal">used_in_bytes</code> / <code class="literal">max_in_bytes</code></p>
  </div>
</div>
<p><span class="strong strong"><strong>Check garbage collection logs</strong></span></p>
<p>As memory usage increases, garbage collection becomes more frequent and takes
longer. You can track the frequency and length of garbage collection events in
<a class="xref" href="logging.html" title="Logging"><code class="literal">elasticsearch.log</code></a>. For example, the following event states Elasticsearch
spent more than 50% (21 seconds) of the last 40 seconds performing garbage
collection.</p>
<div class="pre_wrapper lang-log">
<pre class="programlisting prettyprint lang-log">[timestamp_short_interval_from_last][INFO ][o.e.m.j.JvmGcMonitorService] [node_id] [gc][number] overhead, spent [21s] collecting in the last [40s]</pre>
</div>
<h4><a id="reduce-jvm-memory-pressure"></a>Reduce JVM memory pressure<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.1/docs/reference/how-to/fix-common-cluster-issues.asciidoc">edit</a></h4>
<p><span class="strong strong"><strong>Reduce your shard count</strong></span></p>
<p>Every shard uses memory. In most cases, a small set of large shards uses fewer
resources than many small shards. For tips on reducing your shard count, see
<a class="xref" href="size-your-shards.html" title="Size your shards"><em>Size your shards</em></a>.</p>
<p><a id="avoid-expensive-searches"></a><span class="strong strong"><strong>Avoid expensive searches</strong></span></p>
<p>Expensive searches can use large amounts of memory. To better track expensive
searches on your cluster, enable <a class="xref" href="index-modules-slowlog.html" title="Slow Log">slow logs</a>.</p>
<p>Expensive searches may have a large <a class="xref" href="paginate-search-results.html" title="Paginate search results"><code class="literal">size</code> argument</a>,
use aggregations with a large number of buckets, or include
<a class="xref" href="query-dsl.html#query-dsl-allow-expensive-queries">expensive queries</a>. To prevent expensive
searches, consider the following setting changes:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Lower the <code class="literal">size</code> limit using the
<a class="xref" href="index-modules.html#index-max-result-window"><code class="literal">index.max_result_window</code></a> index setting.
</li>
<li class="listitem">
Decrease the maximum number of allowed aggregation buckets using the
<a class="xref" href="search-settings.html#search-settings-max-buckets">search.max_buckets</a> cluster setting.
</li>
<li class="listitem">
Disable expensive queries using the
<a class="xref" href="query-dsl.html#query-dsl-allow-expensive-queries"><code class="literal">search.allow_expensive_queries</code></a> cluster
setting.
</li>
</ul>
</div>
<a id="1295f51b9e5d4ba9987b02478146b50b"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT _settings
{
  "index.max_result_window": 5000
}

PUT _cluster/settings
{
  "persistent": {
    "search.max_buckets": 20000,
    "search.allow_expensive_queries": false
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/1735.console"></div>
<p><span class="strong strong"><strong>Prevent mapping explosions</strong></span></p>
<p>Defining too many fields or nesting fields too deeply can lead to
<a class="xref" href="mapping.html#mapping-limit-settings" title="Settings to prevent mapping explosion">mapping explosions</a> that use large amounts of memory.
To prevent mapping explosions, use the <a class="xref" href="mapping-settings-limit.html" title="Mapping limit settings">mapping limit
settings</a> to limit the number of field mappings.</p>
<p><span class="strong strong"><strong>Spread out bulk requests</strong></span></p>
<p>While more efficient than individual requests, large <a class="xref" href="docs-bulk.html" title="Bulk API">bulk indexing</a>
or <a class="xref" href="search-multi-search.html" title="Multi search API">multi-search</a> requests can still create high JVM
memory pressure. If possible, submit smaller requests and allow more time
between them.</p>
<p><span class="strong strong"><strong>Upgrade node memory</strong></span></p>
<p>Heavy indexing and search loads can cause high JVM memory pressure. To better
handle heavy workloads, upgrade your nodes to increase their memory capacity.</p>
<h3><a id="red-yellow-cluster-status"></a>Red or yellow cluster status<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.1/docs/reference/how-to/fix-common-cluster-issues.asciidoc">edit</a></h3>
<p>A red or yellow cluster status indicates one or more shards are missing or
unallocated. These unassigned shards increase your risk of data loss and can
degrade cluster performance.</p>
<h4><a id="diagnose-cluster-status"></a>Diagnose your cluster status<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.1/docs/reference/how-to/fix-common-cluster-issues.asciidoc">edit</a></h4>
<p><span class="strong strong"><strong>Check your cluster status</strong></span></p>
<p>Use the <a class="xref" href="cluster-health.html" title="Cluster health API">cluster health API</a>.</p>
<a id="b7df0848b2dc3093f931976db5b8cfff"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _cluster/health?filter_path=status,*_shards</pre>
</div>
<div class="console_widget" data-snippet="snippets/1736.console"></div>
<p>A healthy cluster has a green <code class="literal">status</code> and zero <code class="literal">unassigned_shards</code>. A yellow
status means only replicas are unassigned. A red status means one or
more primary shards are unassigned.</p>
<p><span class="strong strong"><strong>View unassigned shards</strong></span></p>
<p>To view unassigned shards, use the <a class="xref" href="cat-shards.html" title="cat shards API">cat shards API</a>.</p>
<a id="6705eca2095ade294548cfb25bf2dd86"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _cat/shards?v=true&amp;h=index,shard,prirep,state,node,unassigned.reason&amp;s=state</pre>
</div>
<div class="console_widget" data-snippet="snippets/1737.console"></div>
<p>Unassigned shards have a <code class="literal">state</code> of <code class="literal">UNASSIGNED</code>. The <code class="literal">prirep</code> value is <code class="literal">p</code> for
primary shards and <code class="literal">r</code> for replicas.</p>
<p>To understand why an unassigned shard is not being assigned and what action
you must take to allow Elasticsearch to assign it, use the
<a class="xref" href="cluster-allocation-explain.html" title="Cluster allocation explain API">cluster allocation explanation API</a>.</p>
<a id="f01d9bbab5b4535b9e067cde3f64ddbf"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _cluster/allocation/explain?filter_path=index,node_allocation_decisions.node_name,node_allocation_decisions.deciders.*
{
  "index": "my-index",
  "shard": 0,
  "primary": false,
  "current_node": "my-node"
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/1738.console"></div>
<h4><a id="fix-red-yellow-cluster-status"></a>Fix a red or yellow cluster status<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.1/docs/reference/how-to/fix-common-cluster-issues.asciidoc">edit</a></h4>
<p>A shard can become unassigned for several reasons. The following tips outline the
most common causes and their solutions.</p>
<p><span class="strong strong"><strong>Re-enable shard allocation</strong></span></p>
<p>You typically disable allocation during a <a class="xref" href="restart-cluster.html" title="Full-cluster restart and rolling restart">restart</a> or other
cluster maintenance. If you forgot to re-enable allocation afterward, Elasticsearch will
be unable to assign shards. To re-enable allocation, reset the
<code class="literal">cluster.routing.allocation.enable</code> cluster setting.</p>
<a id="73ebc89cb32adb389ae16bb088d7c7e6"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT _cluster/settings
{
  "persistent" : {
    "cluster.routing.allocation.enable" : null
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/1739.console"></div>
<p><span class="strong strong"><strong>Recover lost nodes</strong></span></p>
<p>Shards often become unassigned when a data node leaves the cluster. This can
occur for several reasons, ranging from connectivity issues to hardware failure.
After you resolve the issue and recover the node, it will rejoin the cluster.
Elasticsearch will then automatically allocate any unassigned shards.</p>
<p>To avoid wasting resources on temporary issues, Elasticsearch <a class="xref" href="delayed-allocation.html" title="Delaying allocation when a node leaves">delays
allocation</a> by one minute by default. If you&#8217;ve recovered a node and don’t want
to wait for the delay period, you can call the <a class="xref" href="cluster-reroute.html" title="Cluster reroute API">cluster reroute
API</a> with no arguments to start the allocation process. The process runs
asynchronously in the background.</p>
<a id="0709a38613d2de90d418ce12b36af30e"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">POST _cluster/reroute</pre>
</div>
<div class="console_widget" data-snippet="snippets/1740.console"></div>
<p><span class="strong strong"><strong>Fix allocation settings</strong></span></p>
<p>Misconfigured allocation settings can result in an unassigned primary shard.
These settings include:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="shard-allocation-filtering.html" title="Index-level shard allocation filtering">Shard allocation</a> index settings
</li>
<li class="listitem">
<a class="xref" href="modules-cluster.html#cluster-shard-allocation-filtering" title="Cluster-level shard allocation filtering">Allocation filtering</a> cluster settings
</li>
<li class="listitem">
<a class="xref" href="modules-cluster.html#shard-allocation-awareness" title="Shard allocation awareness">Allocation awareness</a> cluster settings
</li>
</ul>
</div>
<p>To review your allocation settings, use the <a class="xref" href="indices-get-settings.html" title="Get index settings API">get index
settings</a> and <a class="xref" href="cluster-get-settings.html" title="Cluster get settings API">cluster get settings</a> APIs.</p>
<a id="674bb755111c6fbaa4c5ac759395c122"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET my-index/_settings?flat_settings=true&amp;include_defaults=true

GET _cluster/settings?flat_settings=true&amp;include_defaults=true</pre>
</div>
<div class="console_widget" data-snippet="snippets/1741.console"></div>
<p>You can change the settings using the <a class="xref" href="indices-update-settings.html" title="Update index settings API">update index
settings</a> and <a class="xref" href="cluster-update-settings.html" title="Cluster update settings API">cluster update settings</a> APIs.</p>
<p><span class="strong strong"><strong>Allocate or reduce replicas</strong></span></p>
<p>To protect against hardware failure, Elasticsearch will not assign a replica to the same
node as its primary shard. If no other data nodes are available to host the
replica, it remains unassigned. To fix this, you can:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Add a data node to the same tier to host the replica.
</li>
<li class="listitem">
Change the <code class="literal">index.number_of_replicas</code> index setting to reduce the number of
replicas for each primary shard. We recommend keeping at least one replica per
primary.
</li>
</ul>
</div>
<a id="cdfd4fef983c1c0fe8d7417f67d01eae"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT _settings
{
  "index.number_of_replicas": 1
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/1742.console"></div>
<p><span class="strong strong"><strong>Free up or increase disk space</strong></span></p>
<p>Elasticsearch uses a <a class="xref" href="modules-cluster.html#disk-based-shard-allocation" title="Disk-based shard allocation settings">low disk watermark</a> to ensure data
nodes have enough disk space for incoming shards. By default, Elasticsearch does not
allocate shards to nodes using more than 85% of disk space.</p>
<p>To check the current disk space of your nodes, use the <a class="xref" href="cat-allocation.html" title="cat allocation API">cat
allocation API</a>.</p>
<a id="b728d6ba226dba719aadcd8b8099cc74"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET _cat/allocation?v=true&amp;h=node,shards,disk.*</pre>
</div>
<div class="console_widget" data-snippet="snippets/1743.console"></div>
<p>If your nodes are running low on disk space, you have a few options:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Upgrade your nodes to increase disk space.
</li>
<li class="listitem">
Delete unneeded indices to free up space. If you use ILM, you can
update your lifecycle policy to use <a class="xref" href="ilm-searchable-snapshot.html" title="Searchable snapshot">searchable
snapshots</a> or add a delete phase. If you no longer need to search the data, you
can use a <a class="xref" href="snapshot-restore.html" title="Snapshot and restore">snapshot</a> to store it off-cluster.
</li>
<li class="listitem">
<p>If you no longer write to an index, use the <a class="xref" href="indices-forcemerge.html" title="Force merge API">force merge
API</a> or ILM&#8217;s <a class="xref" href="ilm-forcemerge.html" title="Force merge">force merge action</a> to merge its
segments into larger ones.</p>
<a id="4c5f0d7af287618062bb627b44ccb23e"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">POST my-index/_forcemerge</pre>
</div>
<div class="console_widget" data-snippet="snippets/1744.console"></div>
</li>
<li class="listitem">
<p>If an index is read-only, use the <a class="xref" href="indices-shrink-index.html" title="Shrink index API">shrink index API</a> or
ILM&#8217;s <a class="xref" href="ilm-shrink.html" title="Shrink">shrink action</a> to reduce its primary shard count.</p>
<a id="44231f7cdd5c3a21025861cdef31e355"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">POST my-index/_shrink/my-shrunken-index</pre>
</div>
<div class="console_widget" data-snippet="snippets/1745.console"></div>
</li>
<li class="listitem">
<p>If your node has a large disk capacity, you can increase the low disk
watermark or set it to an explicit byte value.</p>
<a id="9d47f02a063444da9f098858a1830d28"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT _cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.disk.watermark.low": "30gb"
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/1746.console"></div>
</li>
</ul>
</div>
<p><span class="strong strong"><strong>Reduce JVM memory pressure</strong></span></p>
<p>Shard allocation requires JVM heap memory. High JVM memory pressure can trigger
<a class="xref" href="circuit-breaker.html" title="Circuit breaker settings">circuit breakers</a> that stop allocation and leave shards
unassigned. See <a class="xref" href="fix-common-cluster-issues.html#high-jvm-memory-pressure" title="High JVM memory pressure">High JVM memory pressure</a>.</p>
<p><span class="strong strong"><strong>Recover data for a lost primary shard</strong></span></p>
<p>If a node containing a primary shard is lost, Elasticsearch can typically replace it
using a replica on another node. If you can&#8217;t recover the node and replicas
don&#8217;t exist or are irrecoverable, you&#8217;ll need to re-add the missing data from a
<a class="xref" href="snapshot-restore.html" title="Snapshot and restore">snapshot</a> or the original data source.</p>
<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<p>Only use this option if node recovery is no longer possible. This
process allocates an empty primary shard. If the node later rejoins the cluster,
Elasticsearch will overwrite its primary shard with data from this newer empty shard,
resulting in data loss.</p>
</div>
</div>
<p>Use the <a class="xref" href="cluster-reroute.html" title="Cluster reroute API">cluster reroute API</a> to manually allocate the
unassigned primary shard to another data node in the same tier. Set
<code class="literal">accept_data_loss</code> to <code class="literal">true</code>.</p>
<a id="750ac969f9a05567f5cdf4f93d6244b6"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">POST _cluster/reroute
{
  "commands": [
    {
      "allocate_empty_primary": {
        "index": "my-index",
        "shard": 0,
        "node": "my-node",
        "accept_data_loss": "true"
      }
    }
  ]
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/1747.console"></div>
<p>If you backed up the missing index data to a snapshot, use the
<a class="xref" href="restore-snapshot-api.html" title="Restore snapshot API">restore snapshot API</a> to restore the individual index.
Alternatively, you can index the missing data from the original data source.</p>
<h3><a id="rejected-requests"></a>Rejected requests<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.1/docs/reference/how-to/fix-common-cluster-issues.asciidoc">edit</a></h3>
<p>When Elasticsearch rejects a request, it stops the operation and returns an error with a
<code class="literal">429</code> response code. Rejected requests are commonly caused by:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
A <a class="xref" href="fix-common-cluster-issues.html#high-cpu-usage" title="High CPU usage">depleted thread pool</a>. A depleted <code class="literal">search</code> or <code class="literal">write</code>
thread pool returns a <code class="literal">TOO_MANY_REQUESTS</code> error message.
</li>
<li class="listitem">
A <a class="xref" href="fix-common-cluster-issues.html#circuit-breaker-errors" title="Circuit breaker errors">circuit breaker error</a>.
</li>
<li class="listitem">
High <a class="xref" href="index-modules-indexing-pressure.html" title="Indexing pressure">indexing pressure</a> that exceeds the
<a class="xref" href="index-modules-indexing-pressure.html#memory-limits" title="Memory limits"><code class="literal">indexing_pressure.memory.limit</code></a>.
</li>
</ul>
</div>
<h4><a id="check-rejected-tasks"></a>Check rejected tasks<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.1/docs/reference/how-to/fix-common-cluster-issues.asciidoc">edit</a></h4>
<p>To check the number of rejected tasks for each thread pool, use the
<a class="xref" href="cat-thread-pool.html" title="cat thread pool API">cat thread pool API</a>. A high ratio of <code class="literal">rejected</code> to
<code class="literal">completed</code> tasks, particularly in the <code class="literal">search</code> and <code class="literal">write</code> thread pools, means
Elasticsearch regularly rejects requests.</p>
<a id="4655c3dea0c61935b7ecf1e57441df66"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET /_cat/thread_pool?v=true&amp;h=id,name,active,rejected,completed</pre>
</div>
<div class="console_widget" data-snippet="snippets/1748.console"></div>
<h4><a id="prevent-rejected-requests"></a>Prevent rejected requests<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.1/docs/reference/how-to/fix-common-cluster-issues.asciidoc">edit</a></h4>
<p><span class="strong strong"><strong>Fix high CPU and memory usage</strong></span></p>
<p>If Elasticsearch regularly rejects requests and other tasks, your cluster likely has high
CPU usage or high JVM memory pressure. For tips, see <a class="xref" href="fix-common-cluster-issues.html#high-cpu-usage" title="High CPU usage">High CPU usage</a> and
<a class="xref" href="fix-common-cluster-issues.html#high-jvm-memory-pressure" title="High JVM memory pressure">High JVM memory pressure</a>.</p>
<p><span class="strong strong"><strong>Prevent circuit breaker errors</strong></span></p>
<p>If you regularly trigger circuit breaker errors, see <a class="xref" href="fix-common-cluster-issues.html#circuit-breaker-errors" title="Circuit breaker errors">Circuit breaker errors</a>
for tips on diagnosing and preventing them.</p>
<h3><a id="task-queue-backlog"></a>Task queue backlog<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.1/docs/reference/how-to/fix-common-cluster-issues.asciidoc">edit</a></h3>
<p>A backlogged task queue can prevent tasks from completing and
put the cluster into an unhealthy state.
Resource constraints, a large number of tasks being triggered at once,
and long running tasks can all contribute to a backlogged task queue.</p>
<h4><a id="diagnose-task-queue-backlog"></a>Diagnose a task queue backlog<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.1/docs/reference/how-to/fix-common-cluster-issues.asciidoc">edit</a></h4>
<p><span class="strong strong"><strong>Check the thread pool status</strong></span></p>
<p>A <a class="xref" href="fix-common-cluster-issues.html#high-cpu-usage" title="High CPU usage">depleted thread pool</a> can result in <a class="xref" href="fix-common-cluster-issues.html#rejected-requests" title="Rejected requests">rejected requests</a>.</p>
<p>You can use the <a class="xref" href="cat-thread-pool.html" title="cat thread pool API">cat thread pool API</a> to
see the number of active threads in each thread pool and
how many tasks are queued, how many have been rejected, and how many have completed.</p>
<a id="425eaaf9c7e3b1e77a3474fbab4183b4"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET /_cat/thread_pool?v&amp;s=t,n&amp;h=type,name,node_name,active,queue,rejected,completed</pre>
</div>
<div class="console_widget" data-snippet="snippets/1749.console"></div>
<p><span class="strong strong"><strong>Inspect the hot threads on each node</strong></span></p>
<p>If a particular thread pool queue is backed up,
you can periodically poll the <a class="xref" href="cluster-nodes-hot-threads.html" title="Nodes hot threads API">Nodes hot threads</a> API
to determine if the thread has sufficient
resources to progress and gauge how quickly it is progressing.</p>
<a id="270549e6b062228312c4e7a54a2c2209"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET /_nodes/hot_threads</pre>
</div>
<div class="console_widget" data-snippet="snippets/1750.console"></div>
<p><span class="strong strong"><strong>Look for long running tasks</strong></span></p>
<p>Long-running tasks can also cause a backlog.
You can use the <a class="xref" href="tasks.html" title="Task management API">task management</a> API to get information about the tasks that are running.
Check the <code class="literal">running_time_in_nanos</code> to identify tasks that are taking an excessive amount of time to complete.</p>
<a id="50ddf374cfa8128538ea092ee98b723d"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">GET /_tasks?filter_path=nodes.*.tasks</pre>
</div>
<div class="console_widget" data-snippet="snippets/1751.console"></div>
<h4><a id="resolve-task-queue-backlog"></a>Resolve a task queue backlog<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.1/docs/reference/how-to/fix-common-cluster-issues.asciidoc">edit</a></h4>
<p><span class="strong strong"><strong>Increase available resources</strong></span></p>
<p>If tasks are progressing slowly and the queue is backing up,
you might need to take steps to <a class="xref" href="fix-common-cluster-issues.html#reduce-cpu-usage" title="Reduce CPU usage">Reduce CPU usage</a>.</p>
<p>In some cases, increasing the thread pool size might help.
For example, the <code class="literal">force_merge</code> thread pool defaults to a single thread.
Increasing the size to 2 might help reduce a backlog of force merge requests.</p>
<p><span class="strong strong"><strong>Cancel stuck tasks</strong></span></p>
<p>If you find the active task&#8217;s hot thread isn&#8217;t progressing and there&#8217;s a backlog,
consider canceling the task.</p>
</div>
<div class="navfooter">
<span class="prev">
<a href="tune-for-disk-usage.html">« Tune for disk usage</a>
</span>
<span class="next">
<a href="size-your-shards.html">Size your shards »</a>
</span>
</div>
</div>
</body>
</html>
