<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Cluster-level shard allocation and routing settings | Elasticsearch Guide [8.14] | Elastic</title>
<meta class="elastic" name="content" content="Cluster-level shard allocation and routing settings | Elasticsearch Guide [8.14]">

<link rel="home" href="index.html" title="Elasticsearch Guide [8.14]"/>
<link rel="up" href="settings.html" title="Configuring Elasticsearch"/>
<link rel="prev" href="circuit-breaker.html" title="Circuit breaker settings"/>
<link rel="next" href="misc-cluster-settings.html" title="Miscellaneous cluster settings"/>
<meta class="elastic" name="product_version" content="8.14"/>
<meta class="elastic" name="product_name" content="Elasticsearch"/>
<meta class="elastic" name="website_area" content="documentation"/>
<meta name="DC.type" content="Learn/Docs/Elasticsearch/Reference/8.14"/>
<meta name="DC.subject" content="Elasticsearch"/>
<meta name="DC.identifier" content="8.14"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body>
<div class="page_header">
<strong>IMPORTANT</strong>: No additional bug fixes or documentation updates
will be released for this version. For the latest information, see the
<a href="../current/index.html">current release documentation</a>.
</div>
<div class="navheader">
<span class="prev">
<a href="circuit-breaker.html">« Circuit breaker settings</a>
</span>
<span class="next">
<a href="misc-cluster-settings.html">Miscellaneous cluster settings »</a>
</span>
</div>
<div class="book" lang="en">
<div class="titlepage">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="index.html">Elasticsearch Guide [8.14]</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="setup.html">Set up Elasticsearch</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="settings.html">Configuring Elasticsearch</a></span>
</div>
<div>
<div><h1 class="title"><a id="id-1"></a>Cluster-level shard allocation and routing settings</h1><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.14/docs/reference/modules/cluster.asciidoc">edit</a></div>
</div>
<!--EXTRA-->
</div>
<div id="content">
<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="modules-cluster"></a>Cluster-level shard allocation and routing settings<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.14/docs/reference/modules/cluster.asciidoc">edit</a></h2>
</div></div></div>
<p><em>Shard allocation</em> is the process of allocating shards to nodes. This can
happen during initial recovery, replica allocation, rebalancing, or
when nodes are added or removed.</p>
<p>One of the main roles of the master is to decide which shards to allocate to
which nodes, and when to move shards between nodes in order to rebalance the
cluster.</p>
<p>There are a number of settings available to control the shard allocation process:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="modules-cluster.html#cluster-shard-allocation-settings" title="Cluster-level shard allocation settings">Cluster-level shard allocation settings</a> control allocation and
rebalancing operations.
</li>
<li class="listitem">
<a class="xref" href="modules-cluster.html#disk-based-shard-allocation" title="Disk-based shard allocation settings">Disk-based shard allocation settings</a> explains how Elasticsearch takes available
disk space into account, and the related settings.
</li>
<li class="listitem">
<a class="xref" href="modules-cluster.html#shard-allocation-awareness" title="Shard allocation awareness">Shard allocation awareness</a> and <a class="xref" href="modules-cluster.html#forced-awareness" title="Forced awareness">Forced awareness</a> control how shards
can be distributed across different racks or availability zones.
</li>
<li class="listitem">
<a class="xref" href="modules-cluster.html#cluster-shard-allocation-filtering" title="Cluster-level shard allocation filtering">Cluster-level shard allocation filtering</a> allows certain nodes or groups of
nodes excluded from allocation so that they can be decommissioned.
</li>
</ul>
</div>
<p>Besides these, there are a few other <a class="xref" href="misc-cluster-settings.html" title="Miscellaneous cluster settings">miscellaneous cluster-level settings</a>.</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="cluster-shard-allocation-settings"></a>Cluster-level shard allocation settings<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.14/docs/reference/modules/cluster/shards_allocation.asciidoc">edit</a></h3>
</div></div></div>
<p>You can use the following settings to control shard allocation and recovery:</p>
<div class="variablelist">
<a id="cluster-routing-allocation-enable"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.enable</code>
</span>
</dt>
<dd>
<p>(<a class="xref" href="settings.html#dynamic-cluster-setting">Dynamic</a>)
Enable or disable allocation for specific kinds of shards:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">all</code> -             (default) Allows shard allocation for all kinds of shards.
</li>
<li class="listitem">
<code class="literal">primaries</code> -       Allows shard allocation only for primary shards.
</li>
<li class="listitem">
<code class="literal">new_primaries</code> -   Allows shard allocation only for primary shards for new indices.
</li>
<li class="listitem">
<code class="literal">none</code> -            No shard allocations of any kind are allowed for any indices.
</li>
</ul>
</div>
<p>This setting does not affect the recovery of local primary shards when
restarting a node. A restarted node that has a copy of an unassigned primary
shard will recover that primary immediately, assuming that its allocation id matches
one of the active allocation ids in the cluster state.</p>
</dd>
</dl>
</div>
<div class="variablelist">
<a id="cluster-routing-allocation-same-shard-host"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.same_shard.host</code>
</span>
</dt>
<dd>
(<a class="xref" href="settings.html#dynamic-cluster-setting">Dynamic</a>)
If <code class="literal">true</code>, forbids multiple copies of a shard from being allocated to
distinct nodes on the same host, i.e. which have the same network
address. Defaults to <code class="literal">false</code>, meaning that copies of a shard may
sometimes be allocated to nodes on the same host. This setting is only
relevant if you run multiple nodes on each host.
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.node_concurrent_incoming_recoveries</code>
</span>
</dt>
<dd>
(<a class="xref" href="settings.html#dynamic-cluster-setting">Dynamic</a>)
How many concurrent incoming shard recoveries are allowed to happen on a
node. Incoming recoveries are the recoveries where the target shard (most
likely the replica unless a shard is relocating) is allocated on the node.
Defaults to <code class="literal">2</code>. Increasing this setting may cause shard movements to have
a performance impact on other activity in your cluster, but may not make
shard movements complete noticeably sooner. We do not recommend adjusting
this setting from its default of <code class="literal">2</code>.
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.node_concurrent_outgoing_recoveries</code>
</span>
</dt>
<dd>
(<a class="xref" href="settings.html#dynamic-cluster-setting">Dynamic</a>)
How many concurrent outgoing shard recoveries are allowed to happen on a
node. Outgoing recoveries are the recoveries where the source shard (most
likely the primary unless a shard is relocating) is allocated on the node.
Defaults to <code class="literal">2</code>. Increasing this setting may cause shard movements to have
a performance impact on other activity in your cluster, but may not make
shard movements complete noticeably sooner. We do not recommend adjusting
this setting from its default of <code class="literal">2</code>.
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.node_concurrent_recoveries</code>
</span>
</dt>
<dd>
(<a class="xref" href="settings.html#dynamic-cluster-setting">Dynamic</a>)
A shortcut to set both
<code class="literal">cluster.routing.allocation.node_concurrent_incoming_recoveries</code> and
<code class="literal">cluster.routing.allocation.node_concurrent_outgoing_recoveries</code>. The
value of this setting takes effect only when the more specific setting is
not configured.  Defaults to <code class="literal">2</code>. Increasing this setting may cause shard
movements to have a performance impact on other activity in your cluster,
but may not make shard movements complete noticeably sooner. We do not
recommend adjusting this setting from its default of <code class="literal">2</code>.
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.node_initial_primaries_recoveries</code>
</span>
</dt>
<dd>
(<a class="xref" href="settings.html#dynamic-cluster-setting">Dynamic</a>)
While the recovery of replicas happens over the network, the recovery of
an unassigned primary after node restart uses data from the local disk.
These should be fast so more initial primary recoveries can happen in
parallel on each node. Defaults to <code class="literal">4</code>. Increasing this setting may cause
shard recoveries to have a performance impact on other activity in your
cluster, but may not make shard recoveries complete noticeably sooner. We
do not recommend adjusting this setting from its default of <code class="literal">4</code>.
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="shards-rebalancing-settings"></a>Shard rebalancing settings<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.14/docs/reference/modules/cluster/shards_allocation.asciidoc">edit</a></h3>
</div></div></div>
<p>A cluster is <em>balanced</em> when it has an equal number of shards on each node, with
all nodes needing equal resources, without having a concentration of shards from
any index on any node. Elasticsearch runs an automatic process called <em>rebalancing</em> which
moves shards between the nodes in your cluster to improve its balance.
Rebalancing obeys all other shard allocation rules such as
<a class="xref" href="modules-cluster.html#cluster-shard-allocation-filtering" title="Cluster-level shard allocation filtering">allocation filtering</a> and
<a class="xref" href="modules-cluster.html#forced-awareness" title="Forced awareness">forced awareness</a> which may prevent it from completely
balancing the cluster. In that case, rebalancing strives to achieve the most
balanced cluster possible within the rules you have configured. If you are using
<a class="xref" href="data-tiers.html" title="Data tiers">data tiers</a> then Elasticsearch automatically applies allocation filtering
rules to place each shard within the appropriate tier. These rules mean that the
balancer works independently within each tier.</p>
<p>You can use the following settings to control the rebalancing of shards across
the cluster:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.allow_rebalance</code>
</span>
</dt>
<dd>
<p>(<a class="xref" href="settings.html#dynamic-cluster-setting">Dynamic</a>)
Specify when shard rebalancing is allowed:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">always</code> -                    Always allow rebalancing.
</li>
<li class="listitem">
<code class="literal">indices_primaries_active</code> -  Only when all primaries in the cluster are allocated.
</li>
<li class="listitem">
<code class="literal">indices_all_active</code> -        (default) Only when all shards (primaries and replicas) in the cluster are allocated.
</li>
</ul>
</div>
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.rebalance.enable</code>
</span>
</dt>
<dd>
<p>(<a class="xref" href="settings.html#dynamic-cluster-setting">Dynamic</a>)
Enable or disable rebalancing for specific kinds of shards:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">all</code> -         (default) Allows shard balancing for all kinds of shards.
</li>
<li class="listitem">
<code class="literal">primaries</code> -   Allows shard balancing only for primary shards.
</li>
<li class="listitem">
<code class="literal">replicas</code> -    Allows shard balancing only for replica shards.
</li>
<li class="listitem">
<code class="literal">none</code> -        No shard balancing of any kind are allowed for any indices.
</li>
</ul>
</div>
<p>Rebalancing is important to ensure the cluster returns to a healthy and fully
resilient state after a disruption. If you adjust this setting, remember to set
it back to <code class="literal">all</code> as soon as possible.</p>
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.cluster_concurrent_rebalance</code>
</span>
</dt>
<dd>
(<a class="xref" href="settings.html#dynamic-cluster-setting">Dynamic</a>)
Defines the number of concurrent shard rebalances are allowed across the whole
cluster. Defaults to <code class="literal">2</code>. Note that this setting only controls the number of
concurrent shard relocations due to imbalances in the cluster. This setting
does not limit shard relocations due to
<a class="xref" href="modules-cluster.html#cluster-shard-allocation-filtering" title="Cluster-level shard allocation filtering">allocation filtering</a> or
<a class="xref" href="modules-cluster.html#forced-awareness" title="Forced awareness">forced awareness</a>. Increasing this setting may cause the
cluster to use additional resources moving shards between nodes, so we
generally do not recommend adjusting this setting from its default of <code class="literal">2</code>.
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.type</code>
</span>
</dt>
<dd>
<p>Selects the algorithm used for computing the cluster balance. Defaults to
<code class="literal">desired_balance</code> which selects the <em>desired balance allocator</em>. This allocator
runs a background task which computes the desired balance of shards in the
cluster. Once this background task completes, Elasticsearch moves shards to their
desired locations.</p>
<p><span class="Admonishment Admonishment--change">
[<span class="Admonishment-version u-mono u-strikethrough">8.8</span>]
<span class="Admonishment-detail">
Deprecated in 8.8. The <code class="literal">balanced</code> allocator type is deprecated and no longer recommended
</span>
</span>
May also be set to <code class="literal">balanced</code> to select the legacy <em>balanced allocator</em>. This
allocator was the default allocator in versions of Elasticsearch before 8.6.0. It runs
in the foreground, preventing the master from doing other work in parallel. It
works by selecting a small number of shard movements which immediately improve
the balance of the cluster, and when those shard movements complete it runs
again and selects another few shards to move. Since this allocator makes its
decisions based only on the current state of the cluster, it will sometimes
move a shard several times while balancing the cluster.</p>
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="shards-rebalancing-heuristics"></a>Shard balancing heuristics settings<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.14/docs/reference/modules/cluster/shards_allocation.asciidoc">edit</a></h3>
</div></div></div>
<p>Rebalancing works by computing a <em>weight</em> for each node based on its allocation
of shards, and then moving shards between nodes to reduce the weight of the
heavier nodes and increase the weight of the lighter ones. The cluster is
balanced when there is no possible shard movement that can bring the weight of
any node closer to the weight of any other node by more than a configurable
threshold.</p>
<p>The weight of a node depends on the number of shards it holds and on the total
estimated resource usage of those shards expressed in terms of the size of the
shard on disk and the number of threads needed to support write traffic to the
shard. Elasticsearch estimates the resource usage of shards belonging to data streams
when they are created by a rollover. The estimated disk size of the new shard
is the mean size of the other shards in the data stream. The estimated write
load of the new shard is a weighted average of the actual write loads of recent
shards in the data stream. Shards that do not belong to the write index of a
data stream have an estimated write load of zero.</p>
<p>The following settings control how Elasticsearch combines these values into an overall
measure of each node&#8217;s weight.</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.balance.threshold</code>
</span>
</dt>
<dd>
(float, <a class="xref" href="settings.html#dynamic-cluster-setting">Dynamic</a>)
The minimum improvement in weight which triggers a rebalancing shard movement.
Defaults to <code class="literal">1.0f</code>. Raising this value will cause Elasticsearch to stop rebalancing
shards sooner, leaving the cluster in a more unbalanced state.
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.balance.shard</code>
</span>
</dt>
<dd>
(float, <a class="xref" href="settings.html#dynamic-cluster-setting">Dynamic</a>)
Defines the weight factor for the total number of shards allocated to each node.
Defaults to <code class="literal">0.45f</code>. Raising this value increases the tendency of Elasticsearch to
equalize the total number of shards across nodes ahead of the other balancing
variables.
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.balance.index</code>
</span>
</dt>
<dd>
(float, <a class="xref" href="settings.html#dynamic-cluster-setting">Dynamic</a>)
Defines the weight factor for the number of shards per index allocated to each
node. Defaults to <code class="literal">0.55f</code>. Raising this value increases the tendency of Elasticsearch to
equalize the number of shards of each index across nodes ahead of the other
balancing variables.
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.balance.disk_usage</code>
</span>
</dt>
<dd>
(float, <a class="xref" href="settings.html#dynamic-cluster-setting">Dynamic</a>)
Defines the weight factor for balancing shards according to their predicted disk
size in bytes. Defaults to <code class="literal">2e-11f</code>. Raising this value increases the tendency
of Elasticsearch to equalize the total disk usage across nodes ahead of the other
balancing variables.
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.balance.write_load</code>
</span>
</dt>
<dd>
(float, <a class="xref" href="settings.html#dynamic-cluster-setting">Dynamic</a>)
Defines the weight factor for the write load of each shard, in terms of the
estimated number of indexing threads needed by the shard. Defaults to <code class="literal">10.0f</code>.
Raising this value increases the tendency of Elasticsearch to equalize the total write
load across nodes ahead of the other balancing variables.
</dd>
</dl>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
If you have a large cluster, it may be unnecessary to keep it in
a perfectly balanced state at all times. It is less resource-intensive for the
cluster to operate in a somewhat unbalanced state rather than to perform all
the shard movements needed to achieve the perfect balance. If so, increase the
value of <code class="literal">cluster.routing.allocation.balance.threshold</code> to define the
acceptable imbalance between nodes. For instance, if you have an average of 500
shards per node and can accept a difference of 5% (25 typical shards) between
nodes, set <code class="literal">cluster.routing.allocation.balance.threshold</code> to <code class="literal">25</code>.
</li>
<li class="listitem">
We do not recommend adjusting the values of the heuristic weight factor
settings. The default values work well in all reasonable clusters. Although
different values may improve the current balance in some ways, it is possible
that they will create unexpected problems in the future or prevent it from
gracefully handling an unexpected disruption.
</li>
<li class="listitem">
Regardless of the result of the balancing algorithm, rebalancing might
not be allowed due to allocation rules such as forced awareness and allocation
filtering. Use the <a class="xref" href="cluster-allocation-explain.html" title="Cluster allocation explain API">Cluster allocation explain</a> API to explain the current
allocation of shards.
</li>
</ul>
</div>
</div>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="disk-based-shard-allocation"></a>Disk-based shard allocation settings<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.14/docs/reference/modules/cluster/disk_allocator.asciidoc">edit</a></h3>
</div></div></div>
<p><a id="disk-based-shard-allocation-description"></a>The disk-based shard allocator ensures that all nodes have enough disk space
without performing more shard movements than necessary. It allocates shards
based on a pair of thresholds known as the <em>low watermark</em> and the <em>high
watermark</em>. Its primary goal is to ensure that no node exceeds the high
watermark, or at least that any such overage is only temporary. If a node
exceeds the high watermark then Elasticsearch will solve this by moving some of its
shards onto other nodes in the cluster.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>It is normal for nodes to temporarily exceed the high watermark from time
to time.</p>
</div>
</div>
<p>The allocator also tries to keep nodes clear of the high watermark by
forbidding the allocation of more shards to a node that exceeds the low
watermark. Importantly, if all of your nodes have exceeded the low watermark
then no new shards can be allocated and Elasticsearch will not be able to move any
shards between nodes in order to keep the disk usage below the high watermark.
You must ensure that your cluster has enough disk space in total and that there
are always some nodes below the low watermark.</p>
<p>Shard movements triggered by the disk-based shard allocator must also satisfy
all other shard allocation rules such as
<a class="xref" href="modules-cluster.html#cluster-shard-allocation-filtering" title="Cluster-level shard allocation filtering">allocation filtering</a> and
<a class="xref" href="modules-cluster.html#forced-awareness" title="Forced awareness">forced awareness</a>. If these rules are too strict then they
can also prevent the shard movements needed to keep the nodes' disk usage under
control. If you are using <a class="xref" href="data-tiers.html" title="Data tiers">data tiers</a> then Elasticsearch automatically
configures allocation filtering rules to place shards within the appropriate
tier, which means that the disk-based shard allocator works independently
within each tier.</p>
<p>If a node is filling up its disk faster than Elasticsearch can move shards elsewhere
then there is a risk that the disk will completely fill up. To prevent this, as
a last resort, once the disk usage reaches the <em>flood-stage</em> watermark Elasticsearch
will block writes to indices with a shard on the affected node. It will also
continue to move shards onto the other nodes in the cluster. When disk usage
on the affected node drops below the high watermark, Elasticsearch automatically removes
the write block. Refer to <a class="xref" href="fix-watermark-errors.html" title="Fix watermark errors">Fix watermark errors</a> to
resolve persistent watermark errors.</p>
<div class="tip admon">
<div class="icon"></div>
<div class="admon_content">
<a id="disk-based-shard-allocation-does-not-balance"></a>
<p>It is normal for the nodes in your cluster to be using very different amounts
of disk space. The <a class="xref" href="modules-cluster.html#shards-rebalancing-settings" title="Shard rebalancing settings">balance</a> of the cluster
depends on a combination of factors which includes the number of shards on each
node, the indices to which those shards belong, and the resource needs of each
shard in terms of its size on disk and its CPU usage. Elasticsearch must trade off all
of these factors against each other, and a cluster which is balanced when
looking at the combination of all of these factors may not appear to be
balanced if you focus attention on just one of them.</p>
</div>
</div>
<p>You can use the following settings to control disk-based allocation:</p>
<div class="variablelist">
<a id="cluster-routing-disk-threshold"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.disk.threshold_enabled</code>
</span>
</dt>
<dd>
(<a class="xref" href="settings.html#dynamic-cluster-setting">Dynamic</a>)
Defaults to <code class="literal">true</code>. Set to <code class="literal">false</code> to disable the disk allocation decider. Upon disabling, it will also remove any existing <code class="literal">index.blocks.read_only_allow_delete</code> index blocks.
</dd>
</dl>
</div>
<div class="variablelist">
<a id="cluster-routing-watermark-low"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.disk.watermark.low</code> <span class="image"><a class="image" href="https://www.elastic.co/cloud/elasticsearch-service/signup?page=docs&amp;placement=docs-body"><img src="https://doc-icons.s3.us-east-2.amazonaws.com/logo_cloud.svg" alt="logo cloud" title="Supported on Elasticsearch Service"></a></span>
</span>
</dt>
<dd>
(<a class="xref" href="settings.html#dynamic-cluster-setting">Dynamic</a>)
Controls the low watermark for disk usage. It defaults to <code class="literal">85%</code>, meaning that Elasticsearch will not allocate shards to nodes that have more than 85% disk used. It can alternatively be set to a ratio value, e.g., <code class="literal">0.85</code>. It can also be set to an absolute byte value (like <code class="literal">500mb</code>) to prevent Elasticsearch from allocating shards if less than the specified amount of space is available. This setting has no effect on the primary shards of newly-created indices but will prevent their replicas from being allocated.
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.disk.watermark.low.max_headroom</code>
</span>
</dt>
<dd>
(<a class="xref" href="settings.html#dynamic-cluster-setting">Dynamic</a>) Controls the max headroom for the low watermark (in case of a percentage/ratio value).
Defaults to 200GB when <code class="literal">cluster.routing.allocation.disk.watermark.low</code> is not explicitly set.
This caps the amount of free space required.
</dd>
</dl>
</div>
<div class="variablelist">
<a id="cluster-routing-watermark-high"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.disk.watermark.high</code> <span class="image"><a class="image" href="https://www.elastic.co/cloud/elasticsearch-service/signup?page=docs&amp;placement=docs-body"><img src="https://doc-icons.s3.us-east-2.amazonaws.com/logo_cloud.svg" alt="logo cloud" title="Supported on Elasticsearch Service"></a></span>
</span>
</dt>
<dd>
(<a class="xref" href="settings.html#dynamic-cluster-setting">Dynamic</a>)
Controls the high watermark. It defaults to <code class="literal">90%</code>, meaning that Elasticsearch will attempt to relocate shards away from a node whose disk usage is above 90%. It can alternatively be set to a ratio value, e.g., <code class="literal">0.9</code>. It can also be set to an absolute byte value (similarly to the low watermark) to relocate shards away from a node if it has less than the specified amount of free space. This setting affects the allocation of all shards, whether previously allocated or not.
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.disk.watermark.high.max_headroom</code>
</span>
</dt>
<dd>
(<a class="xref" href="settings.html#dynamic-cluster-setting">Dynamic</a>) Controls the max headroom for the high watermark (in case of a percentage/ratio value).
Defaults to 150GB when <code class="literal">cluster.routing.allocation.disk.watermark.high</code> is not explicitly set.
This caps the amount of free space required.
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.disk.watermark.enable_for_single_data_node</code>
</span>
</dt>
<dd>
    (<a class="xref" href="settings.html#static-cluster-setting">Static</a>)
In earlier releases, the default behaviour was to disregard disk watermarks for a single
data node cluster when making an allocation decision. This is deprecated behavior
since 7.14 and has been removed in 8.0. The only valid value for this setting
is now <code class="literal">true</code>. The setting will be removed in a future release.
</dd>
</dl>
</div>
<div class="variablelist">
<a id="cluster-routing-flood-stage"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.disk.watermark.flood_stage</code> <span class="image"><a class="image" href="https://www.elastic.co/cloud/elasticsearch-service/signup?page=docs&amp;placement=docs-body"><img src="https://doc-icons.s3.us-east-2.amazonaws.com/logo_cloud.svg" alt="logo cloud" title="Supported on Elasticsearch Service"></a></span>
</span>
</dt>
<dd>
<p>(<a class="xref" href="settings.html#dynamic-cluster-setting">Dynamic</a>)
Controls the flood stage watermark, which defaults to 95%. Elasticsearch enforces a read-only index block (<code class="literal">index.blocks.read_only_allow_delete</code>) on every index that has one or more shards allocated on the node, and that has at least one disk exceeding the flood stage. This setting is a last resort to prevent nodes from running out of disk space. The index block is automatically released when the disk utilization falls below the high watermark. Similarly to the low and high watermark values, it can alternatively be set to a ratio value, e.g., <code class="literal">0.95</code>, or an absolute byte value.</p>
<p>An example of resetting the read-only index block on the <code class="literal">my-index-000001</code> index:</p>
<div class="pre_wrapper lang-ruby alternative">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-ruby alternative">response = client.indices.put_settings(
  index: 'my-index-000001',
  body: {
    'index.blocks.read_only_allow_delete' =&gt; nil
  }
)
puts response</pre>
</div>
<a id="e20037f66bf54bcac7d10f536f031f34"></a>
<div class="pre_wrapper lang-console default has-ruby">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console default has-ruby">PUT /my-index-000001/_settings
{
  "index.blocks.read_only_allow_delete": null
}</pre>
</div>
<div class="console_widget has-ruby" data-snippet="snippets/10.console"></div>
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.disk.watermark.flood_stage.max_headroom</code>
</span>
</dt>
<dd>
(<a class="xref" href="settings.html#dynamic-cluster-setting">Dynamic</a>) Controls the max headroom for the flood stage watermark (in case of a percentage/ratio value).
Defaults to 100GB when
<code class="literal">cluster.routing.allocation.disk.watermark.flood_stage</code> is not explicitly set.
This caps the amount of free space required.
</dd>
</dl>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>You cannot mix the usage of percentage/ratio values and byte values across
the <code class="literal">cluster.routing.allocation.disk.watermark.low</code>, <code class="literal">cluster.routing.allocation.disk.watermark.high</code>,
and <code class="literal">cluster.routing.allocation.disk.watermark.flood_stage</code> settings. Either all values
are set to percentage/ratio values, or all are set to byte values. This enforcement is
so that Elasticsearch can validate that the settings are internally consistent, ensuring that the
low disk threshold is less than the high disk threshold, and the high disk threshold is
less than the flood stage threshold. A similar comparison check is done for the max
headroom values.</p>
</div>
</div>
<div class="variablelist">
<a id="cluster-routing-flood-stage-frozen"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.disk.watermark.flood_stage.frozen</code> <span class="image"><a class="image" href="https://www.elastic.co/cloud/elasticsearch-service/signup?page=docs&amp;placement=docs-body"><img src="https://doc-icons.s3.us-east-2.amazonaws.com/logo_cloud.svg" alt="logo cloud" title="Supported on Elasticsearch Service"></a></span>
</span>
</dt>
<dd>
(<a class="xref" href="settings.html#dynamic-cluster-setting">Dynamic</a>)
Controls the flood stage watermark for dedicated frozen nodes, which defaults to
95%.
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.disk.watermark.flood_stage.frozen.max_headroom</code> <span class="image"><a class="image" href="https://www.elastic.co/cloud/elasticsearch-service/signup?page=docs&amp;placement=docs-body"><img src="https://doc-icons.s3.us-east-2.amazonaws.com/logo_cloud.svg" alt="logo cloud" title="Supported on Elasticsearch Service"></a></span>
</span>
</dt>
<dd>
(<a class="xref" href="settings.html#dynamic-cluster-setting">Dynamic</a>)
Controls the max headroom for the flood stage watermark (in case of a
percentage/ratio value) for dedicated frozen nodes. Defaults to 20GB when
<code class="literal">cluster.routing.allocation.disk.watermark.flood_stage.frozen</code> is not explicitly
set. This caps the amount of free space required on dedicated frozen nodes.
</dd>
<dt>
<span class="term">
<code class="literal">cluster.info.update.interval</code>
</span>
</dt>
<dd>
(<a class="xref" href="settings.html#dynamic-cluster-setting">Dynamic</a>)
How often Elasticsearch should check on disk usage for each node in the
cluster. Defaults to <code class="literal">30s</code>.
</dd>
</dl>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>Percentage values refer to used disk space, while byte values refer to
free disk space. This can be confusing, since it flips the meaning of high and
low. For example, it makes sense to set the low watermark to 10gb and the high
watermark to 5gb, but not the other way around.</p>
</div>
</div>
<p>An example of updating the low watermark to at least 100 gigabytes free, a high
watermark of at least 50 gigabytes free, and a flood stage watermark of 10
gigabytes free, and updating the information about the cluster every minute:</p>
<div class="pre_wrapper lang-ruby alternative">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-ruby alternative">response = client.cluster.put_settings(
  body: {
    persistent: {
      'cluster.routing.allocation.disk.watermark.low' =&gt; '100gb',
      'cluster.routing.allocation.disk.watermark.high' =&gt; '50gb',
      'cluster.routing.allocation.disk.watermark.flood_stage' =&gt; '10gb',
      'cluster.info.update.interval' =&gt; '1m'
    }
  }
)
puts response</pre>
</div>
<a id="0e83f140237d75469a428ff403564bb5"></a>
<div class="pre_wrapper lang-console default has-ruby">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console default has-ruby">PUT _cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.disk.watermark.low": "100gb",
    "cluster.routing.allocation.disk.watermark.high": "50gb",
    "cluster.routing.allocation.disk.watermark.flood_stage": "10gb",
    "cluster.info.update.interval": "1m"
  }
}</pre>
</div>
<div class="console_widget has-ruby" data-snippet="snippets/11.console"></div>
<p>Concerning the max headroom settings for the watermarks, please note
that these apply only in the case that the watermark settings are percentages/ratios.
The aim of a max headroom value is to cap the required free disk space before hitting
the respective watermark. This is especially useful for servers with larger
disks, where a percentage/ratio watermark could translate to a big free disk space requirement,
and the max headroom can be used to cap the required free disk space amount.
As an example, let us take the default settings for the flood watermark.
It has a 95% default value, and the flood max headroom setting has a default value of 100GB.
This means that:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
For a smaller disk, e.g., of 100GB, the flood watermark will hit at 95%, meaning at 5GB
of free space, since 5GB is smaller than the 100GB max headroom value.
</li>
<li class="listitem">
For a larger disk, e.g., of 100TB, the flood watermark will hit at 100GB of free space.
That is because the 95% flood watermark alone would require 5TB of free disk space, but
that is capped by the max headroom setting to 100GB.
</li>
</ul>
</div>
<p>Finally, the max headroom settings have their default values only if their respective watermark
settings are not explicitly set (thus, they have their default percentage values).
If watermarks are explicitly set, then the max headroom settings do not have their default values,
and would need to be explicitly set if they are desired.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="shard-allocation-awareness"></a>Shard allocation awareness<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.14/docs/reference/modules/cluster/allocation_awareness.asciidoc">edit</a></h3>
</div></div></div>
<p>You can use custom node attributes as <em>awareness attributes</em> to enable Elasticsearch
to take your physical hardware configuration into account when allocating shards.
If Elasticsearch knows which nodes are on the same physical server, in the same rack, or
in the same zone, it can distribute the primary shard and its replica shards to
minimize the risk of losing all shard copies in the event of a failure.</p>
<p>When shard allocation awareness is enabled with the
<a class="xref" href="settings.html#dynamic-cluster-setting">dynamic</a>
<code class="literal">cluster.routing.allocation.awareness.attributes</code> setting, shards are only
allocated to nodes that have values set for the specified awareness attributes.
If you use multiple awareness attributes, Elasticsearch considers each attribute
separately when allocating shards.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>The number of attribute values determines how many shard copies are
allocated in each location. If the number of nodes in each location is
unbalanced and there are a lot of replicas, replica shards might be left
unassigned.</p>
</div>
</div>
<div class="tip admon">
<div class="icon"></div>
<div class="admon_content">
<p>Learn more about <a class="xref" href="high-availability-cluster-design-large-clusters.html" title="Resilience in larger clusters">designing resilient clusters</a>.</p>
</div>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="enabling-awareness"></a>Enabling shard allocation awareness<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.14/docs/reference/modules/cluster/allocation_awareness.asciidoc">edit</a></h4>
</div></div></div>
<p>To enable shard allocation awareness:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
<p>Specify the location of each node with a custom node attribute. For example,
if you want Elasticsearch to distribute shards across different racks, you might
use an awareness attribute called <code class="literal">rack_id</code>.</p>
<p>You can set custom attributes in two ways:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<p>By editing the <code class="literal">elasticsearch.yml</code> config file:</p>
<div class="pre_wrapper lang-yaml">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-yaml">node.attr.rack_id: rack_one</pre>
</div>
</li>
<li class="listitem">
<p>Using the <code class="literal">-E</code> command line argument when you start a node:</p>
<div class="pre_wrapper lang-sh">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-sh">./bin/elasticsearch -Enode.attr.rack_id=rack_one</pre>
</div>
</li>
</ul>
</div>
</li>
<li class="listitem">
<p>Tell Elasticsearch to take one or more awareness attributes into account when
allocating shards by setting
<code class="literal">cluster.routing.allocation.awareness.attributes</code> in <span class="strong strong"><strong>every</strong></span> master-eligible
node&#8217;s <code class="literal">elasticsearch.yml</code> config file.</p>
<div class="pre_wrapper lang-yaml">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-yaml">cluster.routing.allocation.awareness.attributes: rack_id <a id="CO13-1"></a><i class="conum" data-value="1"></i></pre>
</div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO13-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>Specify multiple attributes as a comma-separated list.</p>
</td>
</tr>
</table>
</div>
<p>You can also use the
<a class="xref" href="cluster-update-settings.html" title="Cluster update settings API">cluster-update-settings</a> API to set or update
a cluster&#8217;s awareness attributes:</p>
<div class="pre_wrapper lang-ruby alternative">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-ruby alternative">response = client.cluster.put_settings(
  body: {
    persistent: {
      'cluster.routing.allocation.awareness.attributes' =&gt; 'rack_id'
    }
  }
)
puts response</pre>
</div>
<a id="4479e8c63a04fa22207a6a8803eadcad"></a>
<div class="pre_wrapper lang-console default has-ruby">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console default has-ruby">PUT /_cluster/settings
{
  "persistent" : {
    "cluster.routing.allocation.awareness.attributes" : "rack_id"
  }
}</pre>
</div>
<div class="console_widget has-ruby" data-snippet="snippets/12.console"></div>
</li>
</ol>
</div>
<p>With this example configuration, if you start two nodes with
<code class="literal">node.attr.rack_id</code> set to <code class="literal">rack_one</code> and create an index with 5 primary
shards and 1 replica of each primary, all primaries and replicas are
allocated across the two node.</p>
<div class="imageblock">
<div class="content">
<img src="images/shard-allocation/shard-allocation-awareness-one-rack.png" alt="All primaries and replicas are allocated across two nodes in the same rack">
</div>
<div class="title">Figure 1. All primaries and replicas allocated across two nodes in the same rack</div>
</div>
<p>If you add two nodes with <code class="literal">node.attr.rack_id</code> set to <code class="literal">rack_two</code>,
Elasticsearch moves shards to the new nodes, ensuring (if possible)
that no two copies of the same shard are in the same rack.</p>
<div class="imageblock">
<div class="content">
<img src="images/shard-allocation/shard-allocation-awareness-two-racks.png" alt="Primaries and replicas are allocated across four nodes in two racks with no two copies of the same shard in the same rack">
</div>
<div class="title">Figure 2. Primaries and replicas allocated across four nodes in two racks, with no two copies of the same shard in the same rack</div>
</div>
<p>If <code class="literal">rack_two</code> fails and takes down both its nodes, by default Elasticsearch
allocates the lost shard copies to nodes in <code class="literal">rack_one</code>. To prevent multiple
copies of a particular shard from being allocated in the same location, you can
enable forced awareness.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="forced-awareness"></a>Forced awareness<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.14/docs/reference/modules/cluster/allocation_awareness.asciidoc">edit</a></h4>
</div></div></div>
<p>By default, if one location fails, Elasticsearch spreads its shards across the remaining
locations. This might be undesirable if the cluster does not have sufficient
resources to host all its shards when one location is missing.</p>
<p>To prevent the remaining locations from being overloaded in the event of a
whole-location failure, specify the attribute values that should exist with the
<code class="literal">cluster.routing.allocation.awareness.force.*</code> settings. This will mean that
Elasticsearch will prefer to leave some replicas unassigned in the event of a
whole-location failure instead of overloading the nodes in the remaining
locations.</p>
<p>For example, if you have an awareness attribute called <code class="literal">zone</code> and configure
nodes in <code class="literal">zone1</code> and <code class="literal">zone2</code>, you can use forced awareness to make Elasticsearch leave
half of your shard copies unassigned if only one zone is available:</p>
<div class="pre_wrapper lang-yaml">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-yaml">cluster.routing.allocation.awareness.attributes: zone
cluster.routing.allocation.awareness.force.zone.values: zone1,zone2 <a id="CO14-1"></a><i class="conum" data-value="1"></i></pre>
</div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO14-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>Specify all possible <code class="literal">zone</code> attribute values.</p>
</td>
</tr>
</table>
</div>
<p>With this example configuration, if you have two nodes with <code class="literal">node.attr.zone</code>
set to <code class="literal">zone1</code> and an index with <code class="literal">number_of_replicas</code> set to <code class="literal">1</code>, Elasticsearch
allocates all the primary shards but none of the replicas. It will assign the
replica shards once nodes with a different value for <code class="literal">node.attr.zone</code> join the
cluster. In contrast, if you do not configure forced awareness, Elasticsearch will
allocate all primaries and replicas to the two nodes even though they are in
the same zone.</p>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="cluster-shard-allocation-filtering"></a>Cluster-level shard allocation filtering<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.14/docs/reference/modules/cluster/allocation_filtering.asciidoc">edit</a></h3>
</div></div></div>
<p>You can use cluster-level shard allocation filters to control where Elasticsearch
allocates shards from any index. These cluster wide filters are applied in
conjunction with <a class="xref" href="shard-allocation-filtering.html" title="Index-level shard allocation filtering">per-index allocation filtering</a>
and <a class="xref" href="modules-cluster.html#shard-allocation-awareness" title="Shard allocation awareness">allocation awareness</a>.</p>
<p>Shard allocation filters can be based on custom node attributes or the built-in
<code class="literal">_name</code>, <code class="literal">_host_ip</code>, <code class="literal">_publish_ip</code>, <code class="literal">_ip</code>, <code class="literal">_host</code>, <code class="literal">_id</code> and <code class="literal">_tier</code> attributes.</p>
<p>The <code class="literal">cluster.routing.allocation</code> settings are <a class="xref" href="settings.html#dynamic-cluster-setting">dynamic</a>, enabling live indices to
be moved from one set of nodes to another. Shards are only relocated if it is
possible to do so without breaking another routing constraint, such as never
allocating a primary and replica shard on the same node.</p>
<p>The most common use case for cluster-level shard allocation filtering is when
you want to decommission a node. To move shards off of a node prior to shutting
it down, you could create a filter that excludes the node by its IP address:</p>
<div class="pre_wrapper lang-ruby alternative">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-ruby alternative">response = client.cluster.put_settings(
  body: {
    persistent: {
      'cluster.routing.allocation.exclude._ip' =&gt; '10.0.0.1'
    }
  }
)
puts response</pre>
</div>
<a id="0f3a78296825d507dda6771f7ceb9d61"></a>
<div class="pre_wrapper lang-console default has-ruby">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console default has-ruby">PUT _cluster/settings
{
  "persistent" : {
    "cluster.routing.allocation.exclude._ip" : "10.0.0.1"
  }
}</pre>
</div>
<div class="console_widget has-ruby" data-snippet="snippets/13.console"></div>
<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="cluster-routing-settings"></a>Cluster routing settings<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.14/docs/reference/modules/cluster/allocation_filtering.asciidoc">edit</a></h4>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.include.{attribute}</code>
</span>
</dt>
<dd>
(<a class="xref" href="settings.html#dynamic-cluster-setting">Dynamic</a>)
Allocate shards to a node whose <code class="literal">{attribute}</code> has at least one of the
comma-separated values.
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.require.{attribute}</code>
</span>
</dt>
<dd>
(<a class="xref" href="settings.html#dynamic-cluster-setting">Dynamic</a>)
Only allocate shards to a node whose <code class="literal">{attribute}</code> has <em>all</em> of the
comma-separated values.
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.exclude.{attribute}</code>
</span>
</dt>
<dd>
(<a class="xref" href="settings.html#dynamic-cluster-setting">Dynamic</a>)
Do not allocate shards to a node whose <code class="literal">{attribute}</code> has <em>any</em> of the
comma-separated values.
</dd>
</dl>
</div>
<p>The cluster allocation settings support the following built-in attributes:</p>
<div class="informaltable">
<table border="0" cellpadding="4px">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody valign="top">
<tr>
<td valign="top">
<p>
<code class="literal">_name</code>
</p>
</td>
<td valign="top">
<p>
Match nodes by node name
</p>
</td>
</tr>
<tr>
<td valign="top">
<p>
<code class="literal">_host_ip</code>
</p>
</td>
<td valign="top">
<p>
Match nodes by host IP address (IP associated with hostname)
</p>
</td>
</tr>
<tr>
<td valign="top">
<p>
<code class="literal">_publish_ip</code>
</p>
</td>
<td valign="top">
<p>
Match nodes by publish IP address
</p>
</td>
</tr>
<tr>
<td valign="top">
<p>
<code class="literal">_ip</code>
</p>
</td>
<td valign="top">
<p>
Match either <code class="literal">_host_ip</code> or <code class="literal">_publish_ip</code>
</p>
</td>
</tr>
<tr>
<td valign="top">
<p>
<code class="literal">_host</code>
</p>
</td>
<td valign="top">
<p>
Match nodes by hostname
</p>
</td>
</tr>
<tr>
<td valign="top">
<p>
<code class="literal">_id</code>
</p>
</td>
<td valign="top">
<p>
Match nodes by node id
</p>
</td>
</tr>
<tr>
<td valign="top">
<p>
<code class="literal">_tier</code>
</p>
</td>
<td valign="top">
<p>
Match nodes by the node&#8217;s <a class="xref" href="data-tiers.html" title="Data tiers">data tier</a> role
</p>
</td>
</tr>
</tbody>
</table>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p><code class="literal">_tier</code> filtering is based on <a class="xref" href="modules-node.html" title="Nodes">node</a> roles. Only
a subset of roles are <a class="xref" href="data-tiers.html" title="Data tiers">data tier</a> roles, and the generic
<a class="xref" href="modules-node.html#data-node" title="Data nodes">data role</a> will match any tier filtering.
a subset of roles that are <a class="xref" href="data-tiers.html" title="Data tiers">data tier</a> roles, but the generic
<a class="xref" href="modules-node.html#data-node" title="Data nodes">data role</a> will match any tier filtering.</p>
</div>
</div>
<p>You can use wildcards when specifying attribute values, for example:</p>
<div class="pre_wrapper lang-ruby alternative">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-ruby alternative">response = client.cluster.put_settings(
  body: {
    persistent: {
      'cluster.routing.allocation.exclude._ip' =&gt; '192.168.2.*'
    }
  }
)
puts response</pre>
</div>
<a id="6af9dc1c3240aa8e623ff3622bcb1b48"></a>
<div class="pre_wrapper lang-console default has-ruby">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console default has-ruby">PUT _cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.exclude._ip": "192.168.2.*"
  }
}</pre>
</div>
<div class="console_widget has-ruby" data-snippet="snippets/14.console"></div>
</div>

</div>

</div>
</div>
</div><div class="navfooter">
<span class="prev">
<a href="circuit-breaker.html">« Circuit breaker settings</a>
</span>
<span class="next">
<a href="misc-cluster-settings.html">Miscellaneous cluster settings »</a>
</span>
</div>
</body>
</html>
