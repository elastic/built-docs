<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="keywords" content="hot-spotting, hotspot, hot-spot, hot spot, hotspots, hotspotting">
<title>Troubleshooting discovery | Elasticsearch Guide [8.15] | Elastic</title>
<meta class="elastic" name="content" content="Troubleshooting discovery | Elasticsearch Guide [8.15]">

<link rel="home" href="index.html" title="Elasticsearch Guide [8.15]"/>
<link rel="up" href="troubleshooting.html" title="Troubleshooting"/>
<link rel="prev" href="troubleshooting-unstable-cluster.html" title="Troubleshooting an unstable cluster"/>
<link rel="next" href="monitoring-troubleshooting.html" title="Troubleshooting monitoring"/>
<meta class="elastic" name="product_version" content="8.15"/>
<meta class="elastic" name="product_name" content="Elasticsearch"/>
<meta class="elastic" name="website_area" content="documentation"/>
<meta name="DC.type" content="Learn/Docs/Elasticsearch/Reference/8.15"/>
<meta name="DC.subject" content="Elasticsearch"/>
<meta name="DC.identifier" content="8.15"/>
</head>
<body>
<div class="navheader">
<span class="prev">
<a href="troubleshooting-unstable-cluster.html">« Troubleshooting an unstable cluster</a>
</span>
<span class="next">
<a href="monitoring-troubleshooting.html">Troubleshooting monitoring »</a>
</span>
</div>
<div class="book" lang="en">
<div class="titlepage">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="index.html">Elasticsearch Guide [8.15]</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="troubleshooting.html">Troubleshooting</a></span>
</div>
<div>
<div><h1 class="title"><a id="id-1"></a>Troubleshooting discovery</h1><a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.15/docs/reference/troubleshooting/discovery-issues.asciidoc">edit</a></div>
</div>
<!--EXTRA-->
</div>
<div id="content">
<div class="chapter">
<div class="titlepage"><div><div>
<h2 class="title"><a id="discovery-troubleshooting"></a>Troubleshooting discovery<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.15/docs/reference/troubleshooting/discovery-issues.asciidoc">edit</a></h2>
</div></div></div>
<p>In most cases, the discovery and election process completes quickly, and the
master node remains elected for a long period of time.</p>
<p>If your cluster doesn&#8217;t have a stable master, many of its features won&#8217;t work
correctly and Elasticsearch will report errors to clients and in its logs. You must fix
the master node&#8217;s instability before addressing these other issues. It will not
be possible to solve any other issues while there is no elected master node or
the elected master node is unstable.</p>
<p>If your cluster has a stable master but some nodes can&#8217;t discover or join it,
these nodes will report errors to clients and in their logs. You must address
the obstacles preventing these nodes from joining the cluster before addressing
other issues. It will not be possible to solve any other issues reported by
these nodes while they are unable to join the cluster.</p>
<p>If the cluster has no elected master node for more than a few seconds, the
master is unstable, or some nodes are unable to discover or join a stable
master, then Elasticsearch will record information in its logs explaining why. If the
problems persist for more than a few minutes, Elasticsearch will record additional
information in its logs. To properly troubleshoot discovery and election
problems, collect and analyse logs covering at least five minutes from all
nodes.</p>
<p>The following sections describe some common discovery and election problems.</p>
<h3><a id="discovery-no-master"></a>No master is elected<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.15/docs/reference/troubleshooting/discovery-issues.asciidoc">edit</a></h3>
<p>When a node wins the master election, it logs a message containing
<code class="literal">elected-as-master</code> and all nodes log a message containing
<code class="literal">master node changed</code> identifying the new elected master node.</p>
<p>If there is no elected master node and no node can win an election, all
nodes will repeatedly log messages about the problem using a logger called
<code class="literal">org.elasticsearch.cluster.coordination.ClusterFormationFailureHelper</code>. By
default, this happens every 10 seconds.</p>
<p>Master elections only involve master-eligible nodes, so focus your attention on
the master-eligible nodes in this situation. These nodes' logs will indicate
the requirements for a master election, such as the discovery of a certain set
of nodes. The <a class="xref" href="health-api.html" title="Health API">Health</a> API on these nodes will also provide useful
information about the situation.</p>
<p>If the logs or the health report indicate that Elasticsearch can&#8217;t discover enough nodes
to form a quorum, you must address the reasons preventing Elasticsearch from discovering
the missing nodes. The missing nodes are needed to reconstruct the cluster
metadata. Without the cluster metadata, the data in your cluster is
meaningless. The cluster metadata is stored on a subset of the master-eligible
nodes in the cluster. If a quorum can&#8217;t be discovered, the missing nodes were
the ones holding the cluster metadata.</p>
<p>Ensure there are enough nodes running to form a quorum and that every node can
communicate with every other node over the network. Elasticsearch will report additional
details about network connectivity if the election problems persist for more
than a few minutes. If you can&#8217;t start enough nodes to form a quorum, start a
new cluster and restore data from a recent snapshot. Refer to
<a class="xref" href="modules-discovery-quorums.html" title="Quorum-based decision making">Quorum-based decision making</a> for more information.</p>
<p>If the logs or the health report indicate that Elasticsearch <em>has</em> discovered a possible
quorum of nodes, the typical reason that the cluster can&#8217;t elect a master is
that one of the other nodes can&#8217;t discover a quorum. Inspect the logs on the
other master-eligible nodes and ensure that they have all discovered enough
nodes to form a quorum.</p>
<p>If the logs suggest that discovery or master elections are failing due to
timeouts or network-related issues then narrow down the problem as follows.</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
GC pauses are recorded in the GC logs that Elasticsearch emits by default, and also
usually by the <code class="literal">JvmMonitorService</code> in the main node logs. Use these logs to
confirm whether or not the node is experiencing high heap usage with long GC
pauses. If so, <a class="xref" href="high-jvm-memory-pressure.html" title="High JVM memory pressure">the troubleshooting guide for high
heap usage</a> has some suggestions for further investigation but typically you
will need to capture a heap dump and the <a class="xref" href="important-settings.html#gc-logging" title="GC logging settings">garbage collector logs</a>
during a time of high heap usage to fully understand the problem.
</li>
<li class="listitem">
VM pauses also affect other processes on the same host. A VM pause also
typically causes a discontinuity in the system clock, which Elasticsearch will report in
its logs. If you see evidence of other processes pausing at the same time, or
unexpected clock discontinuities, investigate the infrastructure on which you
are running Elasticsearch.
</li>
<li class="listitem">
Packet captures will reveal system-level and network-level faults, especially
if you capture the network traffic simultaneously at all relevant nodes. You
should be able to observe any retransmissions, packet loss, or other delays on
the connections between the nodes.
</li>
<li class="listitem">
<p>Long waits for particular threads to be available can be identified by taking
stack dumps of the main Elasticsearch process (for example, using <code class="literal">jstack</code>) or a
profiling trace (for example, using Java Flight Recorder) in the few seconds
leading up to the relevant log message.</p>
<p>The <a class="xref" href="cluster-nodes-hot-threads.html" title="Nodes hot threads API">Nodes hot threads</a> API sometimes yields useful information, but
bear in mind that this API also requires a number of <code class="literal">transport_worker</code> and
<code class="literal">generic</code> threads across all the nodes in the cluster. The API may be affected
by the very problem you&#8217;re trying to diagnose. <code class="literal">jstack</code> is much more reliable
since it doesn&#8217;t require any JVM threads.</p>
<p>The threads involved in discovery and cluster membership are mainly
<code class="literal">transport_worker</code> and <code class="literal">cluster_coordination</code> threads, for which there should
never be a long wait. There may also be evidence of long waits for threads in
the Elasticsearch logs, particularly looking at warning logs from
<code class="literal">org.elasticsearch.transport.InboundHandler</code>. See
<a class="xref" href="modules-network.html#modules-network-threading-model" title="Networking threading model">Networking threading model</a> for more information.</p>
</li>
</ul>
</div>
<h3><a id="discovery-master-unstable"></a>Master is elected but unstable<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.15/docs/reference/troubleshooting/discovery-issues.asciidoc">edit</a></h3>
<p>When a node wins the master election, it logs a message containing
<code class="literal">elected-as-master</code>. If this happens repeatedly, the elected master node is
unstable. In this situation, focus on the logs from the master-eligible nodes
to understand why the election winner stops being the master and triggers
another election. If the logs suggest that the master is unstable due to
timeouts or network-related issues then narrow down the problem as follows.</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
GC pauses are recorded in the GC logs that Elasticsearch emits by default, and also
usually by the <code class="literal">JvmMonitorService</code> in the main node logs. Use these logs to
confirm whether or not the node is experiencing high heap usage with long GC
pauses. If so, <a class="xref" href="high-jvm-memory-pressure.html" title="High JVM memory pressure">the troubleshooting guide for high
heap usage</a> has some suggestions for further investigation but typically you
will need to capture a heap dump and the <a class="xref" href="important-settings.html#gc-logging" title="GC logging settings">garbage collector logs</a>
during a time of high heap usage to fully understand the problem.
</li>
<li class="listitem">
VM pauses also affect other processes on the same host. A VM pause also
typically causes a discontinuity in the system clock, which Elasticsearch will report in
its logs. If you see evidence of other processes pausing at the same time, or
unexpected clock discontinuities, investigate the infrastructure on which you
are running Elasticsearch.
</li>
<li class="listitem">
Packet captures will reveal system-level and network-level faults, especially
if you capture the network traffic simultaneously at all relevant nodes. You
should be able to observe any retransmissions, packet loss, or other delays on
the connections between the nodes.
</li>
<li class="listitem">
<p>Long waits for particular threads to be available can be identified by taking
stack dumps of the main Elasticsearch process (for example, using <code class="literal">jstack</code>) or a
profiling trace (for example, using Java Flight Recorder) in the few seconds
leading up to the relevant log message.</p>
<p>The <a class="xref" href="cluster-nodes-hot-threads.html" title="Nodes hot threads API">Nodes hot threads</a> API sometimes yields useful information, but
bear in mind that this API also requires a number of <code class="literal">transport_worker</code> and
<code class="literal">generic</code> threads across all the nodes in the cluster. The API may be affected
by the very problem you&#8217;re trying to diagnose. <code class="literal">jstack</code> is much more reliable
since it doesn&#8217;t require any JVM threads.</p>
<p>The threads involved in discovery and cluster membership are mainly
<code class="literal">transport_worker</code> and <code class="literal">cluster_coordination</code> threads, for which there should
never be a long wait. There may also be evidence of long waits for threads in
the Elasticsearch logs, particularly looking at warning logs from
<code class="literal">org.elasticsearch.transport.InboundHandler</code>. See
<a class="xref" href="modules-network.html#modules-network-threading-model" title="Networking threading model">Networking threading model</a> for more information.</p>
</li>
</ul>
</div>
<h3><a id="discovery-cannot-join-master"></a>Node cannot discover or join stable master<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.15/docs/reference/troubleshooting/discovery-issues.asciidoc">edit</a></h3>
<p>If there is a stable elected master but a node can&#8217;t discover or join its
cluster, it will repeatedly log messages about the problem using the
<code class="literal">ClusterFormationFailureHelper</code> logger. The <a class="xref" href="health-api.html" title="Health API">Health</a> API on the affected
node will also provide useful information about the situation. Other log
messages on the affected node and the elected master may provide additional
information about the problem. If the logs suggest that the node cannot
discover or join the cluster due to timeouts or network-related issues then
narrow down the problem as follows.</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
GC pauses are recorded in the GC logs that Elasticsearch emits by default, and also
usually by the <code class="literal">JvmMonitorService</code> in the main node logs. Use these logs to
confirm whether or not the node is experiencing high heap usage with long GC
pauses. If so, <a class="xref" href="high-jvm-memory-pressure.html" title="High JVM memory pressure">the troubleshooting guide for high
heap usage</a> has some suggestions for further investigation but typically you
will need to capture a heap dump and the <a class="xref" href="important-settings.html#gc-logging" title="GC logging settings">garbage collector logs</a>
during a time of high heap usage to fully understand the problem.
</li>
<li class="listitem">
VM pauses also affect other processes on the same host. A VM pause also
typically causes a discontinuity in the system clock, which Elasticsearch will report in
its logs. If you see evidence of other processes pausing at the same time, or
unexpected clock discontinuities, investigate the infrastructure on which you
are running Elasticsearch.
</li>
<li class="listitem">
Packet captures will reveal system-level and network-level faults, especially
if you capture the network traffic simultaneously at all relevant nodes. You
should be able to observe any retransmissions, packet loss, or other delays on
the connections between the nodes.
</li>
<li class="listitem">
<p>Long waits for particular threads to be available can be identified by taking
stack dumps of the main Elasticsearch process (for example, using <code class="literal">jstack</code>) or a
profiling trace (for example, using Java Flight Recorder) in the few seconds
leading up to the relevant log message.</p>
<p>The <a class="xref" href="cluster-nodes-hot-threads.html" title="Nodes hot threads API">Nodes hot threads</a> API sometimes yields useful information, but
bear in mind that this API also requires a number of <code class="literal">transport_worker</code> and
<code class="literal">generic</code> threads across all the nodes in the cluster. The API may be affected
by the very problem you&#8217;re trying to diagnose. <code class="literal">jstack</code> is much more reliable
since it doesn&#8217;t require any JVM threads.</p>
<p>The threads involved in discovery and cluster membership are mainly
<code class="literal">transport_worker</code> and <code class="literal">cluster_coordination</code> threads, for which there should
never be a long wait. There may also be evidence of long waits for threads in
the Elasticsearch logs, particularly looking at warning logs from
<code class="literal">org.elasticsearch.transport.InboundHandler</code>. See
<a class="xref" href="modules-network.html#modules-network-threading-model" title="Networking threading model">Networking threading model</a> for more information.</p>
</li>
</ul>
</div>
<h3><a id="discovery-node-leaves"></a>Node joins cluster and leaves again<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/8.15/docs/reference/troubleshooting/discovery-issues.asciidoc">edit</a></h3>
<p>If a node joins the cluster but Elasticsearch determines it to be faulty then it will be
removed from the cluster again. See <a class="xref" href="cluster-fault-detection.html#cluster-fault-detection-troubleshooting" title="Troubleshooting an unstable cluster">Troubleshooting an unstable cluster</a>
for more information.</p>
</div>
</div>
</div><div class="navfooter">
<span class="prev">
<a href="troubleshooting-unstable-cluster.html">« Troubleshooting an unstable cluster</a>
</span>
<span class="next">
<a href="monitoring-troubleshooting.html">Troubleshooting monitoring »</a>
</span>
</div>
</body>
</html>
