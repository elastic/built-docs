<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Infer trained model API | Elasticsearch Guide [8.3] | Elastic</title>
<link rel="home" href="index.html" title="Elasticsearch Guide [8.3]"/>
<link rel="up" href="ml-df-trained-models-apis.html" title="Machine learning trained model APIs"/>
<link rel="prev" href="get-trained-models-stats.html" title="Get trained models statistics API"/>
<link rel="next" href="start-trained-model-deployment.html" title="Start trained model deployment API"/>
<meta name="DC.type" content="Learn/Docs/Elasticsearch/Reference/8.3"/>
<meta name="DC.subject" content="Elasticsearch"/>
<meta name="DC.identifier" content="8.3"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body><div class="page_header">
<strong>IMPORTANT</strong>: No additional bug fixes or documentation updates
will be released for this version. For the latest information, see the
<a href="../current/index.html">current release documentation</a>.
</div>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="index.html">Elasticsearch Guide [8.3]</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="rest-apis.html">REST APIs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="ml-df-trained-models-apis.html">Machine learning trained model APIs</a></span>
</div>
<div class="navheader">
<span class="prev">
<a href="get-trained-models-stats.html">« Get trained models statistics API</a>
</span>
<span class="next">
<a href="start-trained-model-deployment.html">Start trained model deployment API »</a>
</span>
</div>
<div class="section xpack">
<div class="titlepage"><div><div>
<h2 class="title"><a id="infer-trained-model"></a>Infer trained model API<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.3/docs/reference/ml/trained-models/apis/infer-trained-model.asciidoc">edit</a></h2>
</div></div></div>

<p>Evaluates a trained model. The model may be any supervised model either trained by data frame analytics or imported.</p>
<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<p>This functionality is in technical preview and may be changed or removed in a future release. Elastic will apply best effort to fix any issues, but features in technical preview are not subject to the support SLA of official GA features.</p>
</div>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="infer-trained-model-request"></a>Request<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.3/docs/reference/ml/trained-models/apis/infer-trained-model.asciidoc">edit</a></h3>
</div></div></div>
<p><code class="literal">POST _ml/trained_models/&lt;model_id&gt;/_infer</code></p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="infer-trained-model-path-params"></a>Path parameters<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.3/docs/reference/ml/trained-models/apis/infer-trained-model.asciidoc">edit</a></h3>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">&lt;model_id&gt;</code>
</span>
</dt>
<dd>
(Required, string)
The unique identifier of the trained model.
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="infer-trained-model-query-params"></a>Query parameters<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.3/docs/reference/ml/trained-models/apis/infer-trained-model.asciidoc">edit</a></h3>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">timeout</code>
</span>
</dt>
<dd>
(Optional, time)
Controls the amount of time to wait for inference results. Defaults to 10 seconds.
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="infer-trained-model-request-body"></a>Request body<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.3/docs/reference/ml/trained-models/apis/infer-trained-model.asciidoc">edit</a></h3>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">docs</code>
</span>
</dt>
<dd>
(Required, array)
An array of objects to pass to the model for inference. The objects should
contain the fields matching your configured trained model input. Typically for
NLP models, the field name is <code class="literal">text_field</code>. Currently for NLP models, only a
single value is allowed. For data frame analytics or imported classification or
regression models, more than one value is allowed.
</dd>
</dl>
</div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">inference_config</code>
</span>
</dt>
<dd>
<p>
(Required, object)
The default configuration for inference. This can be: <code class="literal">regression</code>,
<code class="literal">classification</code>, <code class="literal">fill_mask</code>, <code class="literal">ner</code>, <code class="literal">question_answering</code>,
<code class="literal">text_classification</code>, <code class="literal">text_embedding</code> or <code class="literal">zero_shot_classification</code>.
If <code class="literal">regression</code> or <code class="literal">classification</code>, it must match the <code class="literal">target_type</code> of the
underlying <code class="literal">definition.trained_model</code>. If <code class="literal">fill_mask</code>, <code class="literal">ner</code>,
<code class="literal">question_answering</code>, <code class="literal">text_classification</code>, or <code class="literal">text_embedding</code>; the
<code class="literal">model_type</code> must be <code class="literal">pytorch</code>.
</p>
<details open>
<summary class="title">Properties of <code class="literal">inference_config</code></summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">classification</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
Classification configuration for inference.
</p>
<details open>
<summary class="title">Properties of classification inference</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">num_top_classes</code>
</span>
</dt>
<dd>
(Optional, integer)
Specifies the number of top class predictions to return. Defaults to 0.
</dd>
<dt>
<span class="term">
<code class="literal">num_top_feature_importance_values</code>
</span>
</dt>
<dd>
(Optional, integer)
Specifies the maximum number of
<a href="/guide/en/machine-learning/8.3/ml-feature-importance.html" class="ulink" target="_top">feature importance</a> values per document. Defaults
to 0 which means no feature importance calculation occurs.
</dd>
<dt>
<span class="term">
<code class="literal">prediction_field_type</code>
</span>
</dt>
<dd>
(Optional, string)
Specifies the type of the predicted field to write.
Valid values are: <code class="literal">string</code>, <code class="literal">number</code>, <code class="literal">boolean</code>. When <code class="literal">boolean</code> is provided
<code class="literal">1.0</code> is transformed to <code class="literal">true</code> and <code class="literal">0.0</code> to <code class="literal">false</code>.
</dd>
<dt>
<span class="term">
<code class="literal">results_field</code>
</span>
</dt>
<dd>
(Optional, string)
The field that is added to incoming documents to contain the inference
prediction. Defaults to <code class="literal">predicted_value</code>.
</dd>
<dt>
<span class="term">
<code class="literal">top_classes_results_field</code>
</span>
</dt>
<dd>
(Optional, string)
Specifies the field to which the top classes are written. Defaults to
<code class="literal">top_classes</code>.
</dd>
</dl>
</div>
</div>
</details>
</dd>
<dt>
<span class="term">
<code class="literal">fill_mask</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
Configuration for a fill_mask natural language processing (NLP) task. The
fill_mask task works with models optimized for a fill mask action. For example,
for BERT models, the following text may be provided: "The capital of France is
[MASK].". The response indicates the value most likely to replace <code class="literal">[MASK]</code>. In
this instance, the most probable token is <code class="literal">paris</code>.
</p>
<details open>
<summary class="title">Properties of fill_mask inference</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">num_top_classes</code>
</span>
</dt>
<dd>
(Optional, integer)
Number of top predicted tokens to return for replacing the mask token. Defaults to <code class="literal">0</code>.
</dd>
<dt>
<span class="term">
<code class="literal">results_field</code>
</span>
</dt>
<dd>
(Optional, string)
The field that is added to incoming documents to contain the inference
prediction. Defaults to <code class="literal">predicted_value</code>.
</dd>
<dt>
<span class="term">
<code class="literal">tokenization</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
Indicates the tokenization to perform and the desired settings.
The default tokenization configuration is <code class="literal">bert</code>. Valid tokenization
values are
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">bert</code>: Use for BERT-style models
</li>
<li class="listitem">
<code class="literal">mpnet</code>: Use for MPNet-style models
</li>
<li class="listitem">
<code class="literal">roberta</code>: Use for RoBERTa-style and BART-style models
</li>
</ul>
</div>
<details open>
<summary class="title">Properties of tokenization</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">bert</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
BERT-style tokenization is to be performed with the enclosed settings.
</p>
<details open>
<summary class="title">Properties of bert</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">truncate</code>
</span>
</dt>
<dd>
<p>
(Optional, string)
Indicates how tokens are truncated when they exceed <code class="literal">max_sequence_length</code>.
The default value is <code class="literal">first</code>.
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">none</code>: No truncation occurs; the inference request receives an error.
</li>
<li class="listitem">
<code class="literal">first</code>: Only the first sequence is truncated.
</li>
<li class="listitem">
<code class="literal">second</code>: Only the second sequence is truncated. If there is just one sequence,
that sequence is truncated.
</li>
</ul>
</div>
</dd>
</dl>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>For <code class="literal">zero_shot_classification</code>, the hypothesis sequence is always the second
sequence. Therefore, do not use <code class="literal">second</code> in this case.</p>
</div>
</div>
</div>
</details>
</dd>
<dt>
<span class="term">
<code class="literal">roberta</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
RoBERTa-style tokenization is to be performed with the enclosed settings.
</p>
<details open>
<summary class="title">Properties of roberta</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">truncate</code>
</span>
</dt>
<dd>
<p>
(Optional, string)
Indicates how tokens are truncated when they exceed <code class="literal">max_sequence_length</code>.
The default value is <code class="literal">first</code>.
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">none</code>: No truncation occurs; the inference request receives an error.
</li>
<li class="listitem">
<code class="literal">first</code>: Only the first sequence is truncated.
</li>
<li class="listitem">
<code class="literal">second</code>: Only the second sequence is truncated. If there is just one sequence,
that sequence is truncated.
</li>
</ul>
</div>
</dd>
</dl>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>For <code class="literal">zero_shot_classification</code>, the hypothesis sequence is always the second
sequence. Therefore, do not use <code class="literal">second</code> in this case.</p>
</div>
</div>
</div>
</details>
</dd>
<dt>
<span class="term">
<code class="literal">mpnet</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
MPNet-style tokenization is to be performed with the enclosed settings.
</p>
<details open>
<summary class="title">Properties of mpnet</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">truncate</code>
</span>
</dt>
<dd>
<p>
(Optional, string)
Indicates how tokens are truncated when they exceed <code class="literal">max_sequence_length</code>.
The default value is <code class="literal">first</code>.
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">none</code>: No truncation occurs; the inference request receives an error.
</li>
<li class="listitem">
<code class="literal">first</code>: Only the first sequence is truncated.
</li>
<li class="listitem">
<code class="literal">second</code>: Only the second sequence is truncated. If there is just one sequence,
that sequence is truncated.
</li>
</ul>
</div>
</dd>
</dl>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>For <code class="literal">zero_shot_classification</code>, the hypothesis sequence is always the second
sequence. Therefore, do not use <code class="literal">second</code> in this case.</p>
</div>
</div>
</div>
</details>
</dd>
</dl>
</div>
</div>
</details>
</dd>
</dl>
</div>
</div>
</details>
</dd>
<dt>
<span class="term">
<code class="literal">ner</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
Configures a named entity recognition (NER) task. NER is a special case of token
classification. Each token in the sequence is classified according to the
provided classification labels. Currently, the NER task requires the
<code class="literal">classification_labels</code> Inside-Outside-Beginning (IOB) formatted labels. Only
person, organization, location, and miscellaneous are supported.
</p>
<details open>
<summary class="title">Properties of ner inference</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">results_field</code>
</span>
</dt>
<dd>
(Optional, string)
The field that is added to incoming documents to contain the inference
prediction. Defaults to <code class="literal">predicted_value</code>.
</dd>
<dt>
<span class="term">
<code class="literal">tokenization</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
Indicates the tokenization to perform and the desired settings.
The default tokenization configuration is <code class="literal">bert</code>. Valid tokenization
values are
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">bert</code>: Use for BERT-style models
</li>
<li class="listitem">
<code class="literal">mpnet</code>: Use for MPNet-style models
</li>
<li class="listitem">
<code class="literal">roberta</code>: Use for RoBERTa-style and BART-style models
</li>
</ul>
</div>
<details open>
<summary class="title">Properties of tokenization</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">bert</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
BERT-style tokenization is to be performed with the enclosed settings.
</p>
<details open>
<summary class="title">Properties of bert</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">truncate</code>
</span>
</dt>
<dd>
<p>
(Optional, string)
Indicates how tokens are truncated when they exceed <code class="literal">max_sequence_length</code>.
The default value is <code class="literal">first</code>.
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">none</code>: No truncation occurs; the inference request receives an error.
</li>
<li class="listitem">
<code class="literal">first</code>: Only the first sequence is truncated.
</li>
<li class="listitem">
<code class="literal">second</code>: Only the second sequence is truncated. If there is just one sequence,
that sequence is truncated.
</li>
</ul>
</div>
</dd>
</dl>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>For <code class="literal">zero_shot_classification</code>, the hypothesis sequence is always the second
sequence. Therefore, do not use <code class="literal">second</code> in this case.</p>
</div>
</div>
</div>
</details>
</dd>
<dt>
<span class="term">
<code class="literal">roberta</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
RoBERTa-style tokenization is to be performed with the enclosed settings.
</p>
<details open>
<summary class="title">Properties of roberta</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">truncate</code>
</span>
</dt>
<dd>
<p>
(Optional, string)
Indicates how tokens are truncated when they exceed <code class="literal">max_sequence_length</code>.
The default value is <code class="literal">first</code>.
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">none</code>: No truncation occurs; the inference request receives an error.
</li>
<li class="listitem">
<code class="literal">first</code>: Only the first sequence is truncated.
</li>
<li class="listitem">
<code class="literal">second</code>: Only the second sequence is truncated. If there is just one sequence,
that sequence is truncated.
</li>
</ul>
</div>
</dd>
</dl>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>For <code class="literal">zero_shot_classification</code>, the hypothesis sequence is always the second
sequence. Therefore, do not use <code class="literal">second</code> in this case.</p>
</div>
</div>
</div>
</details>
</dd>
<dt>
<span class="term">
<code class="literal">mpnet</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
MPNet-style tokenization is to be performed with the enclosed settings.
</p>
<details open>
<summary class="title">Properties of mpnet</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">truncate</code>
</span>
</dt>
<dd>
<p>
(Optional, string)
Indicates how tokens are truncated when they exceed <code class="literal">max_sequence_length</code>.
The default value is <code class="literal">first</code>.
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">none</code>: No truncation occurs; the inference request receives an error.
</li>
<li class="listitem">
<code class="literal">first</code>: Only the first sequence is truncated.
</li>
<li class="listitem">
<code class="literal">second</code>: Only the second sequence is truncated. If there is just one sequence,
that sequence is truncated.
</li>
</ul>
</div>
</dd>
</dl>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>For <code class="literal">zero_shot_classification</code>, the hypothesis sequence is always the second
sequence. Therefore, do not use <code class="literal">second</code> in this case.</p>
</div>
</div>
</div>
</details>
</dd>
</dl>
</div>
</div>
</details>
</dd>
</dl>
</div>
</div>
</details>
</dd>
<dt>
<span class="term">
<code class="literal">pass_through</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
Configures a <code class="literal">pass_through</code> task. This task is useful for debugging as no
post-processing is done to the inference output and the raw pooling layer
results are returned to the caller.
</p>
<details open>
<summary class="title">Properties of pass_through inference</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">results_field</code>
</span>
</dt>
<dd>
(Optional, string)
The field that is added to incoming documents to contain the inference
prediction. Defaults to <code class="literal">predicted_value</code>.
</dd>
<dt>
<span class="term">
<code class="literal">tokenization</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
Indicates the tokenization to perform and the desired settings.
The default tokenization configuration is <code class="literal">bert</code>. Valid tokenization
values are
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">bert</code>: Use for BERT-style models
</li>
<li class="listitem">
<code class="literal">mpnet</code>: Use for MPNet-style models
</li>
<li class="listitem">
<code class="literal">roberta</code>: Use for RoBERTa-style and BART-style models
</li>
</ul>
</div>
<details open>
<summary class="title">Properties of tokenization</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">bert</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
BERT-style tokenization is to be performed with the enclosed settings.
</p>
<details open>
<summary class="title">Properties of bert</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">truncate</code>
</span>
</dt>
<dd>
<p>
(Optional, string)
Indicates how tokens are truncated when they exceed <code class="literal">max_sequence_length</code>.
The default value is <code class="literal">first</code>.
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">none</code>: No truncation occurs; the inference request receives an error.
</li>
<li class="listitem">
<code class="literal">first</code>: Only the first sequence is truncated.
</li>
<li class="listitem">
<code class="literal">second</code>: Only the second sequence is truncated. If there is just one sequence,
that sequence is truncated.
</li>
</ul>
</div>
</dd>
</dl>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>For <code class="literal">zero_shot_classification</code>, the hypothesis sequence is always the second
sequence. Therefore, do not use <code class="literal">second</code> in this case.</p>
</div>
</div>
</div>
</details>
</dd>
<dt>
<span class="term">
<code class="literal">roberta</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
RoBERTa-style tokenization is to be performed with the enclosed settings.
</p>
<details open>
<summary class="title">Properties of roberta</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">truncate</code>
</span>
</dt>
<dd>
<p>
(Optional, string)
Indicates how tokens are truncated when they exceed <code class="literal">max_sequence_length</code>.
The default value is <code class="literal">first</code>.
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">none</code>: No truncation occurs; the inference request receives an error.
</li>
<li class="listitem">
<code class="literal">first</code>: Only the first sequence is truncated.
</li>
<li class="listitem">
<code class="literal">second</code>: Only the second sequence is truncated. If there is just one sequence,
that sequence is truncated.
</li>
</ul>
</div>
</dd>
</dl>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>For <code class="literal">zero_shot_classification</code>, the hypothesis sequence is always the second
sequence. Therefore, do not use <code class="literal">second</code> in this case.</p>
</div>
</div>
</div>
</details>
</dd>
<dt>
<span class="term">
<code class="literal">mpnet</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
MPNet-style tokenization is to be performed with the enclosed settings.
</p>
<details open>
<summary class="title">Properties of mpnet</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">truncate</code>
</span>
</dt>
<dd>
<p>
(Optional, string)
Indicates how tokens are truncated when they exceed <code class="literal">max_sequence_length</code>.
The default value is <code class="literal">first</code>.
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">none</code>: No truncation occurs; the inference request receives an error.
</li>
<li class="listitem">
<code class="literal">first</code>: Only the first sequence is truncated.
</li>
<li class="listitem">
<code class="literal">second</code>: Only the second sequence is truncated. If there is just one sequence,
that sequence is truncated.
</li>
</ul>
</div>
</dd>
</dl>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>For <code class="literal">zero_shot_classification</code>, the hypothesis sequence is always the second
sequence. Therefore, do not use <code class="literal">second</code> in this case.</p>
</div>
</div>
</div>
</details>
</dd>
</dl>
</div>
</div>
</details>
</dd>
</dl>
</div>
</div>
</details>
</dd>
<dt>
<span class="term">
<code class="literal">question_answering</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
Configures a question answering natural language processing (NLP) task. Question
answering is useful for extracting answers for certain questions from a large
corpus of text.
</p>
<details open>
<summary class="title">Properties of question_answering inference</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">max_answer_length</code>
</span>
</dt>
<dd>
(Optional, integer)
The maximum amount of words in the answer. Defaults to <code class="literal">15</code>.
</dd>
<dt>
<span class="term">
<code class="literal">num_top_classes</code>
</span>
</dt>
<dd>
(Optional, integer)
The number the top found answers to return. Defaults to <code class="literal">0</code>, meaning only the best found answer is returned.
</dd>
<dt>
<span class="term">
<code class="literal">question</code>
</span>
</dt>
<dd>
(Required, string)
The question to use when extracting an answer
</dd>
<dt>
<span class="term">
<code class="literal">results_field</code>
</span>
</dt>
<dd>
(Optional, string)
The field that is added to incoming documents to contain the inference
prediction. Defaults to <code class="literal">predicted_value</code>.
</dd>
<dt>
<span class="term">
<code class="literal">tokenization</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
Indicates the tokenization to perform and the desired settings.
The default tokenization configuration is <code class="literal">bert</code>. Valid tokenization
values are
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">bert</code>: Use for BERT-style models
</li>
<li class="listitem">
<code class="literal">mpnet</code>: Use for MPNet-style models
</li>
<li class="listitem">
<code class="literal">roberta</code>: Use for RoBERTa-style and BART-style models
</li>
</ul>
</div>
<p>Recommended to set <code class="literal">max_sequence_length</code> to <code class="literal">386</code> with <code class="literal">128</code> of <code class="literal">span</code> and set
<code class="literal">truncate</code> to <code class="literal">none</code>.</p>
<details open>
<summary class="title">Properties of tokenization</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">bert</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
BERT-style tokenization is to be performed with the enclosed settings.
</p>
<details open>
<summary class="title">Properties of bert</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">span</code>
</span>
</dt>
<dd>
<p>
(Optional, integer)
When <code class="literal">truncate</code> is <code class="literal">none</code>, you can partition longer text sequences
for inference. The value indicates how many tokens overlap between each
subsequence.
</p>
<p>The default value is <code class="literal">-1</code>, indicating no windowing or spanning occurs.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>When your typical input is just slightly larger than <code class="literal">max_sequence_length</code>, it may be best to simply truncate;
there will be very little information in the second subsequence.</p>
</div>
</div>
</dd>
<dt>
<span class="term">
<code class="literal">truncate</code>
</span>
</dt>
<dd>
<p>
(Optional, string)
Indicates how tokens are truncated when they exceed <code class="literal">max_sequence_length</code>.
The default value is <code class="literal">first</code>.
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">none</code>: No truncation occurs; the inference request receives an error.
</li>
<li class="listitem">
<code class="literal">first</code>: Only the first sequence is truncated.
</li>
<li class="listitem">
<code class="literal">second</code>: Only the second sequence is truncated. If there is just one sequence,
that sequence is truncated.
</li>
</ul>
</div>
</dd>
</dl>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>For <code class="literal">zero_shot_classification</code>, the hypothesis sequence is always the second
sequence. Therefore, do not use <code class="literal">second</code> in this case.</p>
</div>
</div>
</div>
</details>
</dd>
<dt>
<span class="term">
<code class="literal">roberta</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
RoBERTa-style tokenization is to be performed with the enclosed settings.
</p>
<details open>
<summary class="title">Properties of roberta</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">span</code>
</span>
</dt>
<dd>
<p>
(Optional, integer)
When <code class="literal">truncate</code> is <code class="literal">none</code>, you can partition longer text sequences
for inference. The value indicates how many tokens overlap between each
subsequence.
</p>
<p>The default value is <code class="literal">-1</code>, indicating no windowing or spanning occurs.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>When your typical input is just slightly larger than <code class="literal">max_sequence_length</code>, it may be best to simply truncate;
there will be very little information in the second subsequence.</p>
</div>
</div>
</dd>
<dt>
<span class="term">
<code class="literal">truncate</code>
</span>
</dt>
<dd>
<p>
(Optional, string)
Indicates how tokens are truncated when they exceed <code class="literal">max_sequence_length</code>.
The default value is <code class="literal">first</code>.
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">none</code>: No truncation occurs; the inference request receives an error.
</li>
<li class="listitem">
<code class="literal">first</code>: Only the first sequence is truncated.
</li>
<li class="listitem">
<code class="literal">second</code>: Only the second sequence is truncated. If there is just one sequence,
that sequence is truncated.
</li>
</ul>
</div>
</dd>
</dl>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>For <code class="literal">zero_shot_classification</code>, the hypothesis sequence is always the second
sequence. Therefore, do not use <code class="literal">second</code> in this case.</p>
</div>
</div>
</div>
</details>
</dd>
<dt>
<span class="term">
<code class="literal">mpnet</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
MPNet-style tokenization is to be performed with the enclosed settings.
</p>
<details open>
<summary class="title">Properties of mpnet</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">span</code>
</span>
</dt>
<dd>
<p>
(Optional, integer)
When <code class="literal">truncate</code> is <code class="literal">none</code>, you can partition longer text sequences
for inference. The value indicates how many tokens overlap between each
subsequence.
</p>
<p>The default value is <code class="literal">-1</code>, indicating no windowing or spanning occurs.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>When your typical input is just slightly larger than <code class="literal">max_sequence_length</code>, it may be best to simply truncate;
there will be very little information in the second subsequence.</p>
</div>
</div>
</dd>
<dt>
<span class="term">
<code class="literal">truncate</code>
</span>
</dt>
<dd>
<p>
(Optional, string)
Indicates how tokens are truncated when they exceed <code class="literal">max_sequence_length</code>.
The default value is <code class="literal">first</code>.
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">none</code>: No truncation occurs; the inference request receives an error.
</li>
<li class="listitem">
<code class="literal">first</code>: Only the first sequence is truncated.
</li>
<li class="listitem">
<code class="literal">second</code>: Only the second sequence is truncated. If there is just one sequence,
that sequence is truncated.
</li>
</ul>
</div>
</dd>
</dl>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>For <code class="literal">zero_shot_classification</code>, the hypothesis sequence is always the second
sequence. Therefore, do not use <code class="literal">second</code> in this case.</p>
</div>
</div>
</div>
</details>
</dd>
</dl>
</div>
</div>
</details>
</dd>
</dl>
</div>
</div>
</details>
</dd>
<dt>
<span class="term">
<code class="literal">regression</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
Regression configuration for inference.
</p>
<details open>
<summary class="title">Properties of regression inference</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">num_top_feature_importance_values</code>
</span>
</dt>
<dd>
(Optional, integer)
Specifies the maximum number of
<a href="/guide/en/machine-learning/8.3/ml-feature-importance.html" class="ulink" target="_top">feature importance</a> values per document.
By default, it is zero and no feature importance calculation occurs.
</dd>
<dt>
<span class="term">
<code class="literal">results_field</code>
</span>
</dt>
<dd>
(Optional, string)
The field that is added to incoming documents to contain the inference
prediction. Defaults to <code class="literal">predicted_value</code>.
</dd>
</dl>
</div>
</div>
</details>
</dd>
<dt>
<span class="term">
<code class="literal">text_classification</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
A text classification task. Text classification classifies a provided text
sequence into previously known target classes. A specific example of this is
sentiment analysis, which returns the likely target classes indicating text
sentiment, such as "sad", "happy", or "angry".
</p>
<details open>
<summary class="title">Properties of text_classification inference</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">classification_labels</code>
</span>
</dt>
<dd>
(Optional, string) An array of classification labels.
</dd>
<dt>
<span class="term">
<code class="literal">num_top_classes</code>
</span>
</dt>
<dd>
(Optional, integer)
Specifies the number of top class predictions to return. Defaults to all classes (-1).
</dd>
<dt>
<span class="term">
<code class="literal">results_field</code>
</span>
</dt>
<dd>
(Optional, string)
The field that is added to incoming documents to contain the inference
prediction. Defaults to <code class="literal">predicted_value</code>.
</dd>
<dt>
<span class="term">
<code class="literal">tokenization</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
Indicates the tokenization to perform and the desired settings.
The default tokenization configuration is <code class="literal">bert</code>. Valid tokenization
values are
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">bert</code>: Use for BERT-style models
</li>
<li class="listitem">
<code class="literal">mpnet</code>: Use for MPNet-style models
</li>
<li class="listitem">
<code class="literal">roberta</code>: Use for RoBERTa-style and BART-style models
</li>
</ul>
</div>
<details open>
<summary class="title">Properties of tokenization</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">bert</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
BERT-style tokenization is to be performed with the enclosed settings.
</p>
<details open>
<summary class="title">Properties of bert</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">span</code>
</span>
</dt>
<dd>
<p>
(Optional, integer)
When <code class="literal">truncate</code> is <code class="literal">none</code>, you can partition longer text sequences
for inference. The value indicates how many tokens overlap between each
subsequence.
</p>
<p>The default value is <code class="literal">-1</code>, indicating no windowing or spanning occurs.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>When your typical input is just slightly larger than <code class="literal">max_sequence_length</code>, it may be best to simply truncate;
there will be very little information in the second subsequence.</p>
</div>
</div>
</dd>
<dt>
<span class="term">
<code class="literal">truncate</code>
</span>
</dt>
<dd>
<p>
(Optional, string)
Indicates how tokens are truncated when they exceed <code class="literal">max_sequence_length</code>.
The default value is <code class="literal">first</code>.
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">none</code>: No truncation occurs; the inference request receives an error.
</li>
<li class="listitem">
<code class="literal">first</code>: Only the first sequence is truncated.
</li>
<li class="listitem">
<code class="literal">second</code>: Only the second sequence is truncated. If there is just one sequence,
that sequence is truncated.
</li>
</ul>
</div>
</dd>
</dl>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>For <code class="literal">zero_shot_classification</code>, the hypothesis sequence is always the second
sequence. Therefore, do not use <code class="literal">second</code> in this case.</p>
</div>
</div>
</div>
</details>
</dd>
<dt>
<span class="term">
<code class="literal">roberta</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
RoBERTa-style tokenization is to be performed with the enclosed settings.
</p>
<details open>
<summary class="title">Properties of roberta</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">span</code>
</span>
</dt>
<dd>
<p>
(Optional, integer)
When <code class="literal">truncate</code> is <code class="literal">none</code>, you can partition longer text sequences
for inference. The value indicates how many tokens overlap between each
subsequence.
</p>
<p>The default value is <code class="literal">-1</code>, indicating no windowing or spanning occurs.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>When your typical input is just slightly larger than <code class="literal">max_sequence_length</code>, it may be best to simply truncate;
there will be very little information in the second subsequence.</p>
</div>
</div>
</dd>
<dt>
<span class="term">
<code class="literal">truncate</code>
</span>
</dt>
<dd>
<p>
(Optional, string)
Indicates how tokens are truncated when they exceed <code class="literal">max_sequence_length</code>.
The default value is <code class="literal">first</code>.
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">none</code>: No truncation occurs; the inference request receives an error.
</li>
<li class="listitem">
<code class="literal">first</code>: Only the first sequence is truncated.
</li>
<li class="listitem">
<code class="literal">second</code>: Only the second sequence is truncated. If there is just one sequence,
that sequence is truncated.
</li>
</ul>
</div>
</dd>
</dl>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>For <code class="literal">zero_shot_classification</code>, the hypothesis sequence is always the second
sequence. Therefore, do not use <code class="literal">second</code> in this case.</p>
</div>
</div>
</div>
</details>
</dd>
<dt>
<span class="term">
<code class="literal">mpnet</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
MPNet-style tokenization is to be performed with the enclosed settings.
</p>
<details open>
<summary class="title">Properties of mpnet</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">span</code>
</span>
</dt>
<dd>
<p>
(Optional, integer)
When <code class="literal">truncate</code> is <code class="literal">none</code>, you can partition longer text sequences
for inference. The value indicates how many tokens overlap between each
subsequence.
</p>
<p>The default value is <code class="literal">-1</code>, indicating no windowing or spanning occurs.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>When your typical input is just slightly larger than <code class="literal">max_sequence_length</code>, it may be best to simply truncate;
there will be very little information in the second subsequence.</p>
</div>
</div>
</dd>
<dt>
<span class="term">
<code class="literal">truncate</code>
</span>
</dt>
<dd>
<p>
(Optional, string)
Indicates how tokens are truncated when they exceed <code class="literal">max_sequence_length</code>.
The default value is <code class="literal">first</code>.
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">none</code>: No truncation occurs; the inference request receives an error.
</li>
<li class="listitem">
<code class="literal">first</code>: Only the first sequence is truncated.
</li>
<li class="listitem">
<code class="literal">second</code>: Only the second sequence is truncated. If there is just one sequence,
that sequence is truncated.
</li>
</ul>
</div>
</dd>
</dl>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>For <code class="literal">zero_shot_classification</code>, the hypothesis sequence is always the second
sequence. Therefore, do not use <code class="literal">second</code> in this case.</p>
</div>
</div>
</div>
</details>
</dd>
</dl>
</div>
</div>
</details>
</dd>
</dl>
</div>
</div>
</details>
</dd>
<dt>
<span class="term">
<code class="literal">text_embedding</code>
</span>
</dt>
<dd>
<p>
(Object, optional)
Text embedding takes an input sequence and transforms it into a vector of
numbers. These embeddings capture not simply tokens, but semantic meanings and
context. These embeddings can be used in a <a class="xref" href="dense-vector.html" title="Dense vector field type">dense vector</a> field
for powerful insights.
</p>
<details open>
<summary class="title">Properties of text_embedding inference</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">results_field</code>
</span>
</dt>
<dd>
(Optional, string)
The field that is added to incoming documents to contain the inference
prediction. Defaults to <code class="literal">predicted_value</code>.
</dd>
<dt>
<span class="term">
<code class="literal">tokenization</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
Indicates the tokenization to perform and the desired settings.
The default tokenization configuration is <code class="literal">bert</code>. Valid tokenization
values are
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">bert</code>: Use for BERT-style models
</li>
<li class="listitem">
<code class="literal">mpnet</code>: Use for MPNet-style models
</li>
<li class="listitem">
<code class="literal">roberta</code>: Use for RoBERTa-style and BART-style models
</li>
</ul>
</div>
<details open>
<summary class="title">Properties of tokenization</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">bert</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
BERT-style tokenization is to be performed with the enclosed settings.
</p>
<details open>
<summary class="title">Properties of bert</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">truncate</code>
</span>
</dt>
<dd>
<p>
(Optional, string)
Indicates how tokens are truncated when they exceed <code class="literal">max_sequence_length</code>.
The default value is <code class="literal">first</code>.
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">none</code>: No truncation occurs; the inference request receives an error.
</li>
<li class="listitem">
<code class="literal">first</code>: Only the first sequence is truncated.
</li>
<li class="listitem">
<code class="literal">second</code>: Only the second sequence is truncated. If there is just one sequence,
that sequence is truncated.
</li>
</ul>
</div>
</dd>
</dl>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>For <code class="literal">zero_shot_classification</code>, the hypothesis sequence is always the second
sequence. Therefore, do not use <code class="literal">second</code> in this case.</p>
</div>
</div>
</div>
</details>
</dd>
<dt>
<span class="term">
<code class="literal">roberta</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
RoBERTa-style tokenization is to be performed with the enclosed settings.
</p>
<details open>
<summary class="title">Properties of roberta</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">truncate</code>
</span>
</dt>
<dd>
<p>
(Optional, string)
Indicates how tokens are truncated when they exceed <code class="literal">max_sequence_length</code>.
The default value is <code class="literal">first</code>.
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">none</code>: No truncation occurs; the inference request receives an error.
</li>
<li class="listitem">
<code class="literal">first</code>: Only the first sequence is truncated.
</li>
<li class="listitem">
<code class="literal">second</code>: Only the second sequence is truncated. If there is just one sequence,
that sequence is truncated.
</li>
</ul>
</div>
</dd>
</dl>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>For <code class="literal">zero_shot_classification</code>, the hypothesis sequence is always the second
sequence. Therefore, do not use <code class="literal">second</code> in this case.</p>
</div>
</div>
</div>
</details>
</dd>
<dt>
<span class="term">
<code class="literal">mpnet</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
MPNet-style tokenization is to be performed with the enclosed settings.
</p>
<details open>
<summary class="title">Properties of mpnet</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">truncate</code>
</span>
</dt>
<dd>
<p>
(Optional, string)
Indicates how tokens are truncated when they exceed <code class="literal">max_sequence_length</code>.
The default value is <code class="literal">first</code>.
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">none</code>: No truncation occurs; the inference request receives an error.
</li>
<li class="listitem">
<code class="literal">first</code>: Only the first sequence is truncated.
</li>
<li class="listitem">
<code class="literal">second</code>: Only the second sequence is truncated. If there is just one sequence,
that sequence is truncated.
</li>
</ul>
</div>
</dd>
</dl>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>For <code class="literal">zero_shot_classification</code>, the hypothesis sequence is always the second
sequence. Therefore, do not use <code class="literal">second</code> in this case.</p>
</div>
</div>
</div>
</details>
</dd>
</dl>
</div>
</div>
</details>
</dd>
</dl>
</div>
</div>
</details>
</dd>
<dt>
<span class="term">
<code class="literal">zero_shot_classification</code>
</span>
</dt>
<dd>
<p>
(Object, optional)
Configures a zero-shot classification task. Zero-shot classification allows for
text classification to occur without pre-determined labels. At inference time,
it is possible to adjust the labels to classify. This makes this type of model
and task exceptionally flexible.
</p>
<p>If consistently classifying the same labels, it may be better to use a
fine-tuned text classification model.</p>
<details open>
<summary class="title">Properties of zero_shot_classification inference</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">labels</code>
</span>
</dt>
<dd>
(Optional, array)
The labels to classify. Can be set at creation for default labels, and
then updated during inference.
</dd>
<dt>
<span class="term">
<code class="literal">multi_label</code>
</span>
</dt>
<dd>
(Optional, boolean)
Indicates if more than one <code class="literal">true</code> label is possible given the input.
This is useful when labeling text that could pertain to more than one of the
input labels. Defaults to <code class="literal">false</code>.
</dd>
<dt>
<span class="term">
<code class="literal">results_field</code>
</span>
</dt>
<dd>
(Optional, string)
The field that is added to incoming documents to contain the inference
prediction. Defaults to <code class="literal">predicted_value</code>.
</dd>
<dt>
<span class="term">
<code class="literal">tokenization</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
Indicates the tokenization to perform and the desired settings.
The default tokenization configuration is <code class="literal">bert</code>. Valid tokenization
values are
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">bert</code>: Use for BERT-style models
</li>
<li class="listitem">
<code class="literal">mpnet</code>: Use for MPNet-style models
</li>
<li class="listitem">
<code class="literal">roberta</code>: Use for RoBERTa-style and BART-style models
</li>
</ul>
</div>
<details open>
<summary class="title">Properties of tokenization</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">bert</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
BERT-style tokenization is to be performed with the enclosed settings.
</p>
<details open>
<summary class="title">Properties of bert</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">truncate</code>
</span>
</dt>
<dd>
<p>
(Optional, string)
Indicates how tokens are truncated when they exceed <code class="literal">max_sequence_length</code>.
The default value is <code class="literal">first</code>.
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">none</code>: No truncation occurs; the inference request receives an error.
</li>
<li class="listitem">
<code class="literal">first</code>: Only the first sequence is truncated.
</li>
<li class="listitem">
<code class="literal">second</code>: Only the second sequence is truncated. If there is just one sequence,
that sequence is truncated.
</li>
</ul>
</div>
</dd>
</dl>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>For <code class="literal">zero_shot_classification</code>, the hypothesis sequence is always the second
sequence. Therefore, do not use <code class="literal">second</code> in this case.</p>
</div>
</div>
</div>
</details>
</dd>
<dt>
<span class="term">
<code class="literal">roberta</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
RoBERTa-style tokenization is to be performed with the enclosed settings.
</p>
<details open>
<summary class="title">Properties of roberta</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">truncate</code>
</span>
</dt>
<dd>
<p>
(Optional, string)
Indicates how tokens are truncated when they exceed <code class="literal">max_sequence_length</code>.
The default value is <code class="literal">first</code>.
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">none</code>: No truncation occurs; the inference request receives an error.
</li>
<li class="listitem">
<code class="literal">first</code>: Only the first sequence is truncated.
</li>
<li class="listitem">
<code class="literal">second</code>: Only the second sequence is truncated. If there is just one sequence,
that sequence is truncated.
</li>
</ul>
</div>
</dd>
</dl>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>For <code class="literal">zero_shot_classification</code>, the hypothesis sequence is always the second
sequence. Therefore, do not use <code class="literal">second</code> in this case.</p>
</div>
</div>
</div>
</details>
</dd>
<dt>
<span class="term">
<code class="literal">mpnet</code>
</span>
</dt>
<dd>
<p>
(Optional, object)
MPNet-style tokenization is to be performed with the enclosed settings.
</p>
<details open>
<summary class="title">Properties of mpnet</summary>
<div class="content">
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">truncate</code>
</span>
</dt>
<dd>
<p>
(Optional, string)
Indicates how tokens are truncated when they exceed <code class="literal">max_sequence_length</code>.
The default value is <code class="literal">first</code>.
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">none</code>: No truncation occurs; the inference request receives an error.
</li>
<li class="listitem">
<code class="literal">first</code>: Only the first sequence is truncated.
</li>
<li class="listitem">
<code class="literal">second</code>: Only the second sequence is truncated. If there is just one sequence,
that sequence is truncated.
</li>
</ul>
</div>
</dd>
</dl>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>For <code class="literal">zero_shot_classification</code>, the hypothesis sequence is always the second
sequence. Therefore, do not use <code class="literal">second</code> in this case.</p>
</div>
</div>
</div>
</details>
</dd>
</dl>
</div>
</div>
</details>
</dd>
</dl>
</div>
</div>
</details>
</dd>
</dl>
</div>
</div>
</details>
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="infer-trained-model-example"></a>Examples<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.3/docs/reference/ml/trained-models/apis/infer-trained-model.asciidoc">edit</a></h3>
</div></div></div>
<p>The response depends on the kind of model.</p>
<p>For example, for language identification the response is the predicted language and the score:</p>
<a id="35a272df8c919a12d7c3106a18245748"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">POST _ml/trained_models/lang_ident_model_1/_infer
{
  "docs":[{"text": "The fool doth think he is wise, but the wise man knows himself to be a fool."}]
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/2462.console"></div>
<p>Here are the results predicting english with a high probability.</p>
<a id="9dba1d3030959a81d193dde87a60af07"></a>
<div class="pre_wrapper lang-console-result">
<pre class="programlisting prettyprint lang-console-result">{
  "inference_results": [
    {
      "predicted_value": "en",
      "prediction_probability": 0.9999658805366392,
      "prediction_score": 0.9999658805366392
    }
  ]
}</pre>
</div>
<p>When it is a text classification model, the response is the score and predicted classification.</p>
<p>For example:</p>
<a id="776b553df0e507c96dbdbaedecaca0cc"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">POST _ml/trained_models/model2/_infer
{
	"docs": [{"text_field": "The movie was awesome!!"}]
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/2463.console"></div>
<p>The API returns the predicted label and the confidence.</p>
<a id="880db3183fcaa977d129d62a679f5c2c"></a>
<div class="pre_wrapper lang-console-result">
<pre class="programlisting prettyprint lang-console-result">{
  "inference_results": [{
    "predicted_value" : "POSITIVE",
    "prediction_probability" : 0.9998667964092964
  }]
}</pre>
</div>
<p>For named entity recognition (NER) models, the response contains the annotated
text output and the recognized entities.</p>
<a id="ff776c0fccf93e1c7050f7cb7efbae0b"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">POST _ml/trained_models/model2/_infer
{
	"docs": [{"text_field": "Hi my name is Josh and I live in Berlin"}]
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/2464.console"></div>
<p>The API returns in this case:</p>
<a id="4f645597e1009e893b7fa0ddba3e7207"></a>
<div class="pre_wrapper lang-console-result">
<pre class="programlisting prettyprint lang-console-result">{
  "inference_results": [{
    "predicted_value" : "Hi my name is [Josh](PER&amp;Josh) and I live in [Berlin](LOC&amp;Berlin)",
    "entities" : [
      {
        "entity" : "Josh",
        "class_name" : "PER",
        "class_probability" : 0.9977303419824,
        "start_pos" : 14,
        "end_pos" : 18
      },
      {
        "entity" : "Berlin",
        "class_name" : "LOC",
        "class_probability" : 0.9992474323902818,
        "start_pos" : 33,
        "end_pos" : 39
      }
    ]
  }]
}</pre>
</div>
<p>Zero-shot classification models require extra configuration defining the class
labels. These labels are passed in the zero-shot inference config.</p>
<a id="9cbb097e5498a9fde39e3b1d3b62a4d2"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">POST _ml/trained_models/model2/_infer
{
  "docs": [
    {
      "text_field": "This is a very happy person"
    }
  ],
  "inference_config": {
    "zero_shot_classification": {
      "labels": [
        "glad",
        "sad",
        "bad",
        "rad"
      ],
      "multi_label": false
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/2465.console"></div>
<p>The API returns the predicted label and the confidence, as well as the top
classes:</p>
<a id="0a7cd6c39c2f26b0feffe4cf3db6c5db"></a>
<div class="pre_wrapper lang-console-result">
<pre class="programlisting prettyprint lang-console-result">{
  "inference_results": [{
    "predicted_value" : "glad",
    "top_classes" : [
      {
        "class_name" : "glad",
        "class_probability" : 0.8061155063386439,
        "class_score" : 0.8061155063386439
      },
      {
        "class_name" : "rad",
        "class_probability" : 0.18218006158387956,
        "class_score" : 0.18218006158387956
      },
      {
        "class_name" : "bad",
        "class_probability" : 0.006325615787634201,
        "class_score" : 0.006325615787634201
      },
      {
        "class_name" : "sad",
        "class_probability" : 0.0053788162898424545,
        "class_score" : 0.0053788162898424545
      }
    ],
    "prediction_probability" : 0.8061155063386439
  }]
}</pre>
</div>
<p>Question answering models require extra configuration defining the question to
answer.</p>
<a id="3c345feb7c52fd54bcb5d5505fd8bc3b"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">POST _ml/trained_models/model2/_infer
{
  "docs": [
    {
      "text_field": "&lt;long text to extract answer&gt;"
    }
  ],
  "inference_config": {
    "question_answering": {
      "question": "&lt;question to be answered&gt;"
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/2466.console"></div>
<p>The API returns a response similar to the following:</p>
<a id="326297e76ab2f4f081295145a30d12ea"></a>
<div class="pre_wrapper lang-console-result">
<pre class="programlisting prettyprint lang-console-result">{
    "predicted_value": &lt;string subsection of the text that is the answer&gt;,
    "start_offset": &lt;character offset in document to start&gt;,
    "end_offset": &lt;character offset end of the answer,
    "prediction_probability": &lt;prediction score&gt;
}</pre>
</div>
<p>The tokenization truncate option can be overridden when calling the API:</p>
<a id="cee491dd0a8d10ed0cb11a2faa0c99f0"></a>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">POST _ml/trained_models/model2/_infer
{
  "docs": [{"text_field": "The Amazon rainforest covers most of the Amazon basin in South America"}],
  "inference_config": {
    "ner": {
      "tokenization": {
        "bert": {
          "truncate": "first"
        }
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/2467.console"></div>
<p>When the input has been truncated due to the limit imposed by the model&#8217;s <code class="literal">max_sequence_length</code>
the <code class="literal">is_truncated</code> field appears in the response.</p>
<a id="778547ddac52cb030de0f44bb5c5bbdc"></a>
<div class="pre_wrapper lang-console-result">
<pre class="programlisting prettyprint lang-console-result">{
  "inference_results": [{
    "predicted_value" : "The [Amazon](LOC&amp;Amazon) rainforest covers most of the [Amazon](LOC&amp;Amazon) basin in [South America](LOC&amp;South+America)",
    "entities" : [
      {
        "entity" : "Amazon",
        "class_name" : "LOC",
        "class_probability" : 0.9505460915724254,
        "start_pos" : 4,
        "end_pos" : 10
      },
      {
        "entity" : "Amazon",
        "class_name" : "LOC",
        "class_probability" : 0.9969992804311777,
        "start_pos" : 41,
        "end_pos" : 47
      }
    ],
    "is_truncated" : true
  }]
}</pre>
</div>
</div>

</div>
<div class="navfooter">
<span class="prev">
<a href="get-trained-models-stats.html">« Get trained models statistics API</a>
</span>
<span class="next">
<a href="start-trained-model-deployment.html">Start trained model deployment API »</a>
</span>
</div>
</div>
</body>
</html>
