<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Inference APIs | Elasticsearch Guide [8.x] | Elastic</title>
<meta class="elastic" name="content" content="Inference APIs | Elasticsearch Guide [8.x]">

<link rel="home" href="index.html" title="Elasticsearch Guide [8.x]"/>
<link rel="up" href="rest-apis.html" title="REST APIs"/>
<link rel="prev" href="ilm-migrate-to-data-tiers.html" title="Migrate to data tiers routing API"/>
<link rel="next" href="delete-inference-api.html" title="Delete inference API"/>
<meta class="elastic" name="product_version" content="8.x"/>
<meta class="elastic" name="product_name" content="Elasticsearch"/>
<meta class="elastic" name="website_area" content="documentation"/>
<meta name="DC.type" content="Learn/Docs/Elasticsearch/Reference/8.x"/>
<meta name="DC.subject" content="Elasticsearch"/>
<meta name="DC.identifier" content="8.x"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body>
<div class="page_header">
This documentation contains work-in-progress information for future Elastic Stack and Cloud releases. Use the version selector to view supported release docs. It also contains some Elastic Cloud serverless information. Check out our <a href="https://www.elastic.co/docs/current/serverless">serverless docs</a> for more details.
</div>
<div class="navheader">
<span class="prev">
<a href="ilm-migrate-to-data-tiers.html">« Migrate to data tiers routing API</a>
</span>
<span class="next">
<a href="delete-inference-api.html">Delete inference API »</a>
</span>
</div>
<div class="book" lang="en">
<div class="titlepage">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="index.html">Elasticsearch Guide [8.x]</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="rest-apis.html">REST APIs</a></span>
</div>
<div>
<div><h1 class="title"><a id="id-1"></a>Inference APIs</h1><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.x/docs/reference/inference/inference-apis.asciidoc">edit</a></div>
</div>
<!--EXTRA-->
</div>
<div id="content">
<div class="chapter xpack">
<div class="titlepage"><div><div>
<div class="position-relative"><h2 class="title"><a id="inference-apis"></a>Inference APIs</h2><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.x/docs/reference/inference/inference-apis.asciidoc">edit</a></div>
</div></div></div>
<div class="important admon">
<div class="icon"></div>
<div class="admon_content">
<p>The inference APIs enable you to use certain services, such as built-in
machine learning models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Azure,
Google AI Studio or Hugging Face. For built-in models and models uploaded
through Eland, the inference APIs offer an alternative way to use and manage
trained models. However, if you do not plan to use the inference APIs to use these
models or if you want to use non-NLP models, use the
<a class="xref" href="ml-df-trained-models-apis.html" title="Machine learning trained model APIs"><em>Machine learning trained model APIs</em></a>.</p>
</div>
</div>
<div class="sidebar">
<div class="titlepage"><div><div>
<p class="title"><strong>New API reference</strong></p>
</div></div></div>
<p>For the most up-to-date API details, refer to <a href="/docs/api/doc/elasticsearch/v8/group/endpoint-inference" class="ulink" target="_top">Inference APIs</a>.</p>
</div>
<p>The inference APIs enable you to create inference endpoints and integrate with machine learning models of different services - such as Amazon Bedrock, Anthropic, Azure AI Studio, Cohere, Google AI, Mistral, OpenAI, or HuggingFace.
Use the following APIs to manage inference models and perform inference:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="delete-inference-api.html" title="Delete inference API">Delete inference API</a>
</li>
<li class="listitem">
<a class="xref" href="get-inference-api.html" title="Get inference API">Get inference API</a>
</li>
<li class="listitem">
<a class="xref" href="post-inference-api.html" title="Perform inference API">Perform inference API</a>
</li>
<li class="listitem">
<a class="xref" href="put-inference-api.html" title="Create inference API">Create inference API</a>
</li>
<li class="listitem">
<a class="xref" href="stream-inference-api.html" title="Stream inference API">Stream inference API</a>
</li>
<li class="listitem">
<a class="xref" href="chat-completion-inference-api.html" title="Chat completion inference API">Chat completion inference API</a>
</li>
<li class="listitem">
<a class="xref" href="update-inference-api.html" title="Update inference API">Update inference API</a>
</li>
</ul>
</div>
<div id="inference-landscape" class="imageblock text-center">
<div class="content">
<img src="images/inference-landscape.jpg" alt="A representation of the Elastic inference landscape">
</div>
<div class="title">Figure 16. A representation of the Elastic inference landscape</div>
</div>
<p>An inference endpoint enables you to use the corresponding machine learning model without
manual deployment and apply it to your data at ingestion time through
<a class="xref" href="semantic-search-semantic-text.html" title="Tutorial: semantic search with semantic_text">semantic text</a>.</p>
<p>Choose a model from your service or use ELSER – a retrieval model trained by Elastic –, then create an inference endpoint by the <a class="xref" href="put-inference-api.html" title="Create inference API">Create inference API</a>.
Now use <a class="xref" href="semantic-search-semantic-text.html" title="Tutorial: semantic search with semantic_text">semantic text</a> to perform <a class="xref" href="semantic-search.html" title="Semantic search">semantic search</a> on your data.</p>
<div class="position-relative"><h3><a id="adaptive-allocations"></a>Adaptive allocations</h3><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.x/docs/reference/inference/inference-apis.asciidoc">edit</a></div>
<p>Adaptive allocations allow inference services to dynamically adjust the number of model allocations based on the current load.</p>
<p>When adaptive allocations are enabled:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<p>The number of allocations scales up automatically when the load increases.</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Allocations scale down to a minimum of 0 when the load decreases, saving resources.
</li>
</ul>
</div>
</li>
</ul>
</div>
<p>For more information about adaptive allocations and resources, refer to the <a href="/guide/en/machine-learning/8.x/ml-nlp-auto-scale.html" class="ulink" target="_top">trained model autoscaling</a> documentation.</p>
<div class="position-relative"><h3><a id="default-enpoints"></a>Default inference endpoints</h3><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.x/docs/reference/inference/inference-apis.asciidoc">edit</a></div>
<p>Your Elasticsearch deployment contains preconfigured inference endpoints which makes them easier to use when defining <code class="literal">semantic_text</code> fields or using inference processors.
The following list contains the default inference endpoints listed by <code class="literal">inference_id</code>:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">.elser-2-elasticsearch</code>: uses the <a href="/guide/en/machine-learning/8.x/ml-nlp-elser.html" class="ulink" target="_top">ELSER</a> built-in trained model for <code class="literal">sparse_embedding</code> tasks (recommended for English language texts)
</li>
<li class="listitem">
<code class="literal">.multilingual-e5-small-elasticsearch</code>: uses the <a href="/guide/en/machine-learning/8.x/ml-nlp-e5.html" class="ulink" target="_top">E5</a> built-in trained model for <code class="literal">text_embedding</code> tasks (recommended for non-English language texts)
</li>
</ul>
</div>
<p>Use the <code class="literal">inference_id</code> of the endpoint in a <a class="xref" href="semantic-text.html" title="Semantic text field type"><code class="literal">semantic_text</code></a> field definition or when creating an <a class="xref" href="inference-processor.html" title="Inference processor">inference processor</a>.
The API call will automatically download and deploy the model which might take a couple of minutes.
Default inference enpoints have <a href="/guide/en/machine-learning/8.x/ml-nlp-auto-scale.html#nlp-model-adaptive-allocations" class="ulink" target="_top">adaptive allocations</a> enabled.
For these models, the minimum number of allocations is <code class="literal">0</code>.
If there is no inference activity that uses the endpoint, the number of allocations will scale down to <code class="literal">0</code> automatically after 15 minutes.</p>
<div class="position-relative"><h3><a id="infer-chunking-config"></a>Configuring chunking</h3><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.x/docs/reference/inference/inference-apis.asciidoc">edit</a></div>
<p>Inference endpoints have a limit on the amount of text they can process at once, determined by the model&#8217;s input capacity.
Chunking is the process of splitting the input text into pieces that remain within these limits.
It occurs when ingesting documents into <a class="xref" href="semantic-text.html" title="Semantic text field type"><code class="literal">semantic_text</code> fields</a>.
Chunking also helps produce sections that are digestible for humans.
Returning a long document in search results is less useful than providing the most relevant chunk of text.</p>
<p>Each chunk will include the text subpassage and the corresponding embedding generated from it.</p>
<p>By default, documents are split into sentences and grouped in sections up to 250 words with 1 sentence overlap so that each chunk shares a sentence with the previous chunk.
Overlapping ensures continuity and prevents vital contextual information in the input text from being lost by a hard break.</p>
<p>Elasticsearch uses the <a href="https://unicode-org.github.io/icu-docs/" class="ulink" target="_top">ICU4J</a> library to detect word and sentence boundaries for chunking.
<a href="https://unicode-org.github.io/icu/userguide/boundaryanalysis/#word-boundary" class="ulink" target="_top">Word boundaries</a> are identified by following a series of rules, not just the presence of a whitespace character.
For written languages that do use whitespace such as Chinese or Japanese dictionary lookups are used to detect word boundaries.</p>
<div class="position-relative"><h4><a id="_chunking_strategies"></a>Chunking strategies</h4><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.x/docs/reference/inference/inference-apis.asciidoc">edit</a></div>
<p>Two strategies are available for chunking: <code class="literal">sentence</code> and <code class="literal">word</code>.</p>
<p>The <code class="literal">sentence</code> strategy splits the input text at sentence boundaries.
Each chunk contains one or more complete sentences ensuring that the integrity of sentence-level context is preserved, except when a sentence causes a chunk to exceed a word count of <code class="literal">max_chunk_size</code>, in which case it will be split across chunks.
The <code class="literal">sentence_overlap</code> option defines the number of sentences from the previous chunk to include in the current chunk which is either <code class="literal">0</code> or <code class="literal">1</code>.</p>
<p>The <code class="literal">word</code> strategy splits the input text on individual words up to the <code class="literal">max_chunk_size</code> limit.
The <code class="literal">overlap</code> option is the number of words from the previous chunk to include in the current chunk.</p>
<p>The default chunking strategy is <code class="literal">sentence</code>.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>The default chunking strategy for inference endpoints created before 8.16 is <code class="literal">word</code>.</p>
</div>
</div>
<div class="position-relative"><h4><a id="_example_of_configuring_the_chunking_behavior"></a>Example of configuring the chunking behavior</h4><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/8.x/docs/reference/inference/inference-apis.asciidoc">edit</a></div>
<p>The following example creates an inference endpoint with the <code class="literal">elasticsearch</code> service that deploys the ELSER model by default and configures the chunking behavior.</p>
<div class="pre_wrapper lang-python alternative">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-python alternative">resp = client.inference.put(
    task_type="sparse_embedding",
    inference_id="small_chunk_size",
    inference_config={
        "service": "elasticsearch",
        "service_settings": {
            "num_allocations": 1,
            "num_threads": 1
        },
        "chunking_settings": {
            "strategy": "sentence",
            "max_chunk_size": 100,
            "sentence_overlap": 0
        }
    },
)
print(resp)</pre>
</div>
<div class="pre_wrapper lang-js alternative">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-js alternative">const response = await client.inference.put({
  task_type: "sparse_embedding",
  inference_id: "small_chunk_size",
  inference_config: {
    service: "elasticsearch",
    service_settings: {
      num_allocations: 1,
      num_threads: 1,
    },
    chunking_settings: {
      strategy: "sentence",
      max_chunk_size: 100,
      sentence_overlap: 0,
    },
  },
});
console.log(response);</pre>
</div>
<a id="5ceb734e3affe00e2cdc29af748d95bf"></a>
<div class="pre_wrapper lang-console default has-python has-js">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console default has-python has-js">PUT _inference/sparse_embedding/small_chunk_size
{
  "service": "elasticsearch",
  "service_settings": {
    "num_allocations": 1,
    "num_threads": 1
  },
  "chunking_settings": {
    "strategy": "sentence",
    "max_chunk_size": 100,
    "sentence_overlap": 0
  }
}</pre>
</div>
<div class="console_widget has-python has-js" data-snippet="snippets/2915.console"></div>






















</div>
</div>
</div><div class="navfooter">
<span class="prev">
<a href="ilm-migrate-to-data-tiers.html">« Migrate to data tiers routing API</a>
</span>
<span class="next">
<a href="delete-inference-api.html">Delete inference API »</a>
</span>
</div>
</body>
</html>
