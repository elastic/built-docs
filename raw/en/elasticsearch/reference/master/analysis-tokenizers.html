<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Tokenizer reference | Elasticsearch Guide [master] | Elastic</title>
<meta class="elastic" name="content" content="Tokenizer reference | Elasticsearch Guide [master]">

<link rel="home" href="index.html" title="Elasticsearch Guide [master]"/>
<link rel="up" href="analysis.html" title="Text analysis"/>
<link rel="prev" href="analysis-whitespace-analyzer.html" title="Whitespace analyzer"/>
<link rel="next" href="analysis-chargroup-tokenizer.html" title="Character group tokenizer"/>
<meta class="elastic" name="product_version" content="master"/>
<meta class="elastic" name="product_name" content="Elasticsearch"/>
<meta class="elastic" name="website_area" content="documentation"/>
<meta name="DC.type" content="Learn/Docs/Elasticsearch/Reference/master"/>
<meta name="DC.subject" content="Elasticsearch"/>
<meta name="DC.identifier" content="master"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body>
<div class="page_header">
This documentation contains work-in-progress information for future Elastic Stack and Cloud releases. Use the version selector to view supported release docs. It also contains some Elastic Cloud serverless information. Check out our <a href="https://www.elastic.co/docs/current/serverless">serverless docs</a> for more details.
</div>
<div class="navheader">
<span class="prev">
<a href="analysis-whitespace-analyzer.html">« Whitespace analyzer</a>
</span>
<span class="next">
<a href="analysis-chargroup-tokenizer.html">Character group tokenizer »</a>
</span>
</div>
<div class="book" lang="en">
<div class="titlepage">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="index.html">Elasticsearch Guide [master]</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="analysis.html">Text analysis</a></span>
</div>
<div>
<div><h1 class="title"><a id="id-1"></a>Tokenizer reference</h1><a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/main/docs/reference/analysis/tokenizers.asciidoc">edit</a></div>
</div>
<!--EXTRA-->
</div>
<div id="content">
<div class="chapter">
<div class="titlepage"><div><div>
<h2 class="title"><a id="analysis-tokenizers"></a>Tokenizer reference<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/main/docs/reference/analysis/tokenizers.asciidoc">edit</a></h2>
</div></div></div>
<p>A <em>tokenizer</em> receives a stream of characters, breaks it up into individual
<em>tokens</em> (usually individual words), and outputs a stream of <em>tokens</em>. For
instance, a <a class="xref" href="analysis-whitespace-tokenizer.html" title="Whitespace tokenizer"><code class="literal">whitespace</code></a> tokenizer breaks
text into tokens whenever it sees any whitespace. It would convert the text
<code class="literal">"Quick brown fox!"</code> into the terms <code class="literal">[Quick, brown, fox!]</code>.</p>
<p>The tokenizer is also responsible for recording the following:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Order or <em>position</em> of each term (used for phrase and word proximity queries)
</li>
<li class="listitem">
Start and end <em>character offsets</em> of the original word which the term
represents (used for highlighting search snippets).
</li>
<li class="listitem">
<em>Token type</em>, a classification of each term produced, such as <code class="literal">&lt;ALPHANUM&gt;</code>,
<code class="literal">&lt;HANGUL&gt;</code>, or <code class="literal">&lt;NUM&gt;</code>. Simpler analyzers only produce the <code class="literal">word</code> token type.
</li>
</ul>
</div>
<p>Elasticsearch has a number of built in tokenizers which can be used to build
<a class="xref" href="analysis-custom-analyzer.html" title="Create a custom analyzer">custom analyzers</a>.</p>
<h3><a id="_word_oriented_tokenizers"></a>Word Oriented Tokenizers<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/main/docs/reference/analysis/tokenizers.asciidoc">edit</a></h3>
<p>The following tokenizers are usually used for tokenizing full text into
individual words:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<a class="xref" href="analysis-standard-tokenizer.html" title="Standard tokenizer">Standard Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">standard</code> tokenizer divides text into terms on word boundaries, as
defined by the Unicode Text Segmentation algorithm. It removes most
punctuation symbols. It is the best choice for most languages.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-letter-tokenizer.html" title="Letter tokenizer">Letter Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">letter</code> tokenizer divides text into terms whenever it encounters a
character which is not a letter.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-lowercase-tokenizer.html" title="Lowercase tokenizer">Lowercase Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">lowercase</code> tokenizer, like the <code class="literal">letter</code> tokenizer,  divides text into
terms whenever it encounters a character which is not a letter, but it also
lowercases all terms.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-whitespace-tokenizer.html" title="Whitespace tokenizer">Whitespace Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">whitespace</code> tokenizer divides text into terms whenever it encounters any
whitespace character.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-uaxurlemail-tokenizer.html" title="UAX URL email tokenizer">UAX URL Email Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">uax_url_email</code> tokenizer is like the <code class="literal">standard</code> tokenizer except that it
recognises URLs and email addresses as single tokens.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-classic-tokenizer.html" title="Classic tokenizer">Classic Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">classic</code> tokenizer is a grammar based tokenizer for the English Language.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-thai-tokenizer.html" title="Thai tokenizer">Thai Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">thai</code> tokenizer segments Thai text into words.
</dd>
</dl>
</div>
<h3><a id="_partial_word_tokenizers"></a>Partial Word Tokenizers<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/main/docs/reference/analysis/tokenizers.asciidoc">edit</a></h3>
<p>These tokenizers break up text or words into small fragments, for partial word
matching:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<a class="xref" href="analysis-ngram-tokenizer.html" title="N-gram tokenizer">N-Gram Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">ngram</code> tokenizer can break up text into words when it encounters any of
a list of specified characters (e.g. whitespace or punctuation), then it returns
n-grams of each word: a sliding window of continuous letters, e.g. <code class="literal">quick</code> &#8594;
<code class="literal">[qu, ui, ic, ck]</code>.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-edgengram-tokenizer.html" title="Edge n-gram tokenizer">Edge N-Gram Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">edge_ngram</code> tokenizer can break up text into words when it encounters any of
a list of specified characters (e.g. whitespace or punctuation), then it returns
n-grams of each word which are anchored to the start of the word, e.g. <code class="literal">quick</code> &#8594;
<code class="literal">[q, qu, qui, quic, quick]</code>.
</dd>
</dl>
</div>
<h3><a id="_structured_text_tokenizers"></a>Structured Text Tokenizers<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/main/docs/reference/analysis/tokenizers.asciidoc">edit</a></h3>
<p>The following tokenizers are usually used with structured text like
identifiers, email addresses, zip codes, and paths, rather than with full
text:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<a class="xref" href="analysis-keyword-tokenizer.html" title="Keyword tokenizer">Keyword Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">keyword</code> tokenizer is a &#8220;noop&#8221; tokenizer that accepts whatever text it
is given and outputs the exact same text as a single term. It can be combined
with token filters like <a class="xref" href="analysis-lowercase-tokenfilter.html" title="Lowercase token filter"><code class="literal">lowercase</code></a> to
normalise the analysed terms.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-pattern-tokenizer.html" title="Pattern tokenizer">Pattern Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">pattern</code> tokenizer uses a regular expression to either split text into
terms whenever it matches a word separator, or to capture matching text as
terms.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-simplepattern-tokenizer.html" title="Simple pattern tokenizer">Simple Pattern Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">simple_pattern</code> tokenizer uses a regular expression to capture matching
text as terms. It uses a restricted subset of regular expression features
and is generally faster than the <code class="literal">pattern</code> tokenizer.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-chargroup-tokenizer.html" title="Character group tokenizer">Char Group Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">char_group</code> tokenizer is configurable through sets of characters to split
on, which is usually less expensive than running regular expressions.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-simplepatternsplit-tokenizer.html" title="Simple pattern split tokenizer">Simple Pattern Split Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">simple_pattern_split</code> tokenizer uses the same restricted regular expression
subset as the <code class="literal">simple_pattern</code> tokenizer, but splits the input at matches rather
than returning the matches as terms.
</dd>
<dt>
<span class="term">
<a class="xref" href="analysis-pathhierarchy-tokenizer.html" title="Path hierarchy tokenizer">Path Tokenizer</a>
</span>
</dt>
<dd>
The <code class="literal">path_hierarchy</code> tokenizer takes a hierarchical value like a filesystem
path, splits on the path separator, and emits a term for each component in the
tree, e.g. <code class="literal">/foo/bar/baz</code> &#8594; <code class="literal">[/foo, /foo/bar, /foo/bar/baz ]</code>.
</dd>
</dl>
</div>















</div>
</div>
</div><div class="navfooter">
<span class="prev">
<a href="analysis-whitespace-analyzer.html">« Whitespace analyzer</a>
</span>
<span class="next">
<a href="analysis-chargroup-tokenizer.html">Character group tokenizer »</a>
</span>
</div>
</body>
</html>
