<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="keywords" content="hot-spotting, hotspot, hot-spot, hot spot, hotspots, hotspotting">
<title>Hot spotting | Elasticsearch Guide [master] | Elastic</title>
<meta class="elastic" name="content" content="Hot spotting | Elasticsearch Guide [master]">

<link rel="home" href="index.html" title="Elasticsearch Guide [master]"/>
<link rel="up" href="fix-common-cluster-issues.html" title="Fix common cluster issues"/>
<link rel="prev" href="mapping-explosion.html" title="Mapping explosion"/>
<link rel="next" href="diagnose-unassigned-shards.html" title="Diagnose unassigned shards"/>
<meta class="elastic" name="product_version" content="master"/>
<meta class="elastic" name="product_name" content="Elasticsearch"/>
<meta class="elastic" name="website_area" content="documentation"/>
<meta name="DC.type" content="Learn/Docs/Elasticsearch/Reference/master"/>
<meta name="DC.subject" content="Elasticsearch"/>
<meta name="DC.identifier" content="master"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body><div class="page_header">
You are looking at preliminary documentation for a future release.
Not what you want? See the
<a href="../current/index.html">current release documentation</a>.
</div>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="index.html">Elasticsearch Guide [master]</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="troubleshooting.html">Troubleshooting</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="fix-common-cluster-issues.html">Fix common cluster issues</a></span>
</div>
<div class="navheader">
<span class="prev">
<a href="mapping-explosion.html">« Mapping explosion</a>
</span>
<span class="next">
<a href="diagnose-unassigned-shards.html">Diagnose unassigned shards »</a>
</span>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="hotspotting"></a>Hot spotting<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/main/docs/reference/troubleshooting/common-issues/hotspotting.asciidoc">edit</a></h2>
</div></div></div>

<p>Computer <a href="https://en.wikipedia.org/wiki/Hot_spot_(computer_programming)" class="ulink" target="_top">hot spotting</a>
may occur in Elasticsearch when resource utilizations are unevenly distributed across
<a class="xref" href="modules-node.html" title="Node">nodes</a>. Temporary spikes are not usually considered problematic, but
ongoing significantly unique utilization may lead to cluster bottlenecks
and should be reviewed.</p>
<h4><a id="detect"></a>Detect hot spotting<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/main/docs/reference/troubleshooting/common-issues/hotspotting.asciidoc">edit</a></h4>
<p>Hot spotting most commonly surfaces as significantly elevated
resource utilization (of <code class="literal">disk.percent</code>, <code class="literal">heap.percent</code>, or <code class="literal">cpu</code>) among a
subset of nodes as reported via <a class="xref" href="cat-nodes.html" title="cat nodes API">cat nodes</a>. Individual spikes aren&#8217;t
necessarily problematic, but if utilization repeatedly spikes or consistently remains
high over time (for example longer than 30 seconds), the resource may be experiencing problematic
hot spotting.</p>
<p>For example, let&#8217;s show case two separate plausible issues using cat nodes:</p>
<div class="pre_wrapper lang-ruby alternative">
<pre class="programlisting prettyprint lang-ruby alternative">response = client.cat.nodes(
  v: true,
  s: 'master,name',
  h: 'name,master,node.role,heap.percent,disk.used_percent,cpu'
)
puts response</pre>
</div>
<a id="1570976f7807b88dc8a046b833be057b"></a>
<div class="pre_wrapper lang-console default has-ruby">
<pre class="programlisting prettyprint lang-console default has-ruby">GET _cat/nodes?v&amp;s=master,name&amp;h=name,master,node.role,heap.percent,disk.used_percent,cpu</pre>
</div>
<div class="console_widget has-ruby" data-snippet="snippets/1945.console"></div>
<p>Pretend this same output pulled twice across five minutes:</p>
<a id="4420b0b1f98f7e824a354112587afad7"></a>
<div class="pre_wrapper lang-console-result">
<pre class="programlisting prettyprint lang-console-result">name   master node.role heap.percent disk.used_percent cpu
node_1 *      hirstm              24                20  95
node_2 -      hirstm              23                18  18
node_3 -      hirstmv             25                90  10</pre>
</div>
<p>Here we see two significantly unique utilizations: where the master node is at
<code class="literal">cpu: 95</code> and a hot node is at <code class="literal">disk.used_percent: 90%</code>. This would indicate
hot spotting was occurring on these two nodes, and not necessarily from the same
root cause.</p>
<h4><a id="causes"></a>Causes<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/main/docs/reference/troubleshooting/common-issues/hotspotting.asciidoc">edit</a></h4>
<p>Historically, clusters experience hot spotting mainly as an effect of hardware,
shard distributions, and/or task load. We&#8217;ll review these sequentially in order
of their potentially impacting scope.</p>
<h4><a id="causes-hardware"></a>Hardware<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/main/docs/reference/troubleshooting/common-issues/hotspotting.asciidoc">edit</a></h4>
<p>Here are some common improper hardware setups which may contribute to hot
spotting:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Resources are allocated non-uniformly. For example, if one hot node is
given half the CPU of its peers. Elasticsearch expects all nodes on a
<a class="xref" href="data-tiers.html" title="Data tiers">data tier</a> to share the same hardware profiles or
specifications.
</li>
<li class="listitem">
Resources are consumed by another service on the host, including other
Elasticsearch nodes. Refer to our <a class="xref" href="setup.html#dedicated-host" title="Use dedicated hosts">dedicated host</a> recommendation.
</li>
<li class="listitem">
Resources experience different network or disk throughputs. For example, if one
node&#8217;s I/O is lower than its peers. Refer to
<a class="xref" href="tune-for-indexing-speed.html" title="Tune for indexing speed">Use faster hardware</a> for more information.
</li>
<li class="listitem">
A JVM that has been configured with a heap larger than 31GB. Refer to <a class="xref" href="advanced-configuration.html#set-jvm-heap-size" title="Set the JVM heap size">Set the JVM heap size</a>
for more information.
</li>
<li class="listitem">
Problematic resources uniquely report <a class="xref" href="setup-configuration-memory.html" title="Disable swapping">memory swapping</a>.
</li>
</ul>
</div>
<h4><a id="causes-shards"></a>Shard distributions<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/main/docs/reference/troubleshooting/common-issues/hotspotting.asciidoc">edit</a></h4>
<p>Elasticsearch indices are divided into one or more <a href="https://en.wikipedia.org/wiki/Shard_(database_architecture)" class="ulink" target="_top">shards</a>
which can sometimes be poorly distributed. Elasticsearch accounts for this by <a class="xref" href="modules-cluster.html" title="Cluster-level shard allocation and routing settings">balancing shard counts</a>
across data nodes. As <a href="/blog/whats-new-elasticsearch-kibana-cloud-8-6-0" class="ulink" target="_top">introduced in version 8.6</a>,
Elasticsearch by default also enables <a class="xref" href="modules-cluster.html" title="Cluster-level shard allocation and routing settings">desired balancing</a> to account for ingest load.
A node may still experience hot spotting either due to write-heavy indices or by the
overall shards it&#8217;s hosting.</p>
<h5><a id="causes-shards-nodes"></a>Node level<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/main/docs/reference/troubleshooting/common-issues/hotspotting.asciidoc">edit</a></h5>
<p>You can check for shard balancing via <a class="xref" href="cat-allocation.html" title="cat allocation API">cat allocation</a>, though as of version
8.6, <a class="xref" href="modules-cluster.html" title="Cluster-level shard allocation and routing settings">desired balancing</a> may no longer fully expect to
balance shards. Kindly note, both methods may temporarily show problematic imbalance during
<a class="xref" href="cluster-fault-detection.html" title="Cluster fault detection">cluster stability issues</a>.</p>
<p>For example, let&#8217;s showcase two separate plausible issues using cat allocation:</p>
<div class="pre_wrapper lang-ruby alternative">
<pre class="programlisting prettyprint lang-ruby alternative">response = client.cat.allocation(
  v: true,
  s: 'node',
  h: 'node,shards,disk.percent,disk.indices,disk.used'
)
puts response</pre>
</div>
<a id="9a05cc10eea1251e23b82a4549913536"></a>
<div class="pre_wrapper lang-console default has-ruby">
<pre class="programlisting prettyprint lang-console default has-ruby">GET _cat/allocation?v&amp;s=node&amp;h=node,shards,disk.percent,disk.indices,disk.used</pre>
</div>
<div class="console_widget has-ruby" data-snippet="snippets/1946.console"></div>
<p>Which could return:</p>
<a id="94cb975a6303de99d6314bfd4f46d5a3"></a>
<div class="pre_wrapper lang-console-result">
<pre class="programlisting prettyprint lang-console-result">node   shards disk.percent disk.indices disk.used
node_1    446           19      154.8gb   173.1gb
node_2     31           52       44.6gb   372.7gb
node_3    445           43      271.5gb   289.4gb</pre>
</div>
<p>Here we see two significantly unique situations. <code class="literal">node_2</code> has recently
restarted, so it has a much lower number of shards than all other nodes. This
also relates to <code class="literal">disk.indices</code> being much smaller than <code class="literal">disk.used</code> while shards
are recovering as seen via <a class="xref" href="cat-recovery.html" title="cat recovery API">cat recovery</a>. While <code class="literal">node_2</code>'s shard
count is low, it may become a write hot spot due to ongoing <a class="xref" href="ilm-rollover.html" title="Rollover">ILM
rollovers</a>. This is a common root cause of write hot spots covered in the next
section.</p>
<p>The second situation is that <code class="literal">node_3</code> has a higher <code class="literal">disk.percent</code> than <code class="literal">node_1</code>,
even though they hold roughly the same number of shards. This occurs when either
shards are not evenly sized (refer to <a class="xref" href="size-your-shards.html#shard-size-recommendation" title="Aim for shard sizes between 10GB and 50GB">Aim for shard sizes between 10GB and 50GB</a>) or when
there are a lot of empty indices.</p>
<p>Cluster rebalancing based on desired balance does much of the heavy lifting
of keeping nodes from hot spotting. It can be limited by either nodes hitting
<a class="xref" href="modules-cluster.html#disk-based-shard-allocation" title="Disk-based shard allocation settings">watermarks</a> (refer to <a class="xref" href="fix-watermark-errors.html" title="Fix watermark errors">fixing disk watermark errors</a>) or by a
write-heavy index&#8217;s total shards being much lower than the written-to nodes.</p>
<p>You can confirm hot spotted nodes via <a class="xref" href="cluster-nodes-stats.html" title="Nodes stats API">the nodes stats API</a>,
potentially polling twice over time to only checking for the stats differences
between them rather than polling once giving you stats for the node&#8217;s
full <a class="xref" href="cluster-nodes-usage.html" title="Nodes feature usage API">node uptime</a>. For example, to check all nodes
indexing stats:</p>
<div class="pre_wrapper lang-ruby alternative">
<pre class="programlisting prettyprint lang-ruby alternative">response = client.nodes.stats(
  human: true,
  filter_path: 'nodes.*.name,nodes.*.indices.indexing'
)
puts response</pre>
</div>
<a id="52b71aa4ae6563abae78cd20ff06d1e9"></a>
<div class="pre_wrapper lang-console default has-ruby">
<pre class="programlisting prettyprint lang-console default has-ruby">GET _nodes/stats?human&amp;filter_path=nodes.*.name,nodes.*.indices.indexing</pre>
</div>
<div class="console_widget has-ruby" data-snippet="snippets/1947.console"></div>
<h5><a id="causes-shards-index"></a>Index level<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/main/docs/reference/troubleshooting/common-issues/hotspotting.asciidoc">edit</a></h5>
<p>Hot spotted nodes frequently surface via <a class="xref" href="cat-thread-pool.html" title="cat thread pool API">cat thread pool</a>'s
<code class="literal">write</code> and <code class="literal">search</code> queue backups. For example:</p>
<div class="pre_wrapper lang-ruby alternative">
<pre class="programlisting prettyprint lang-ruby alternative">response = client.cat.thread_pool(
  thread_pool_patterns: 'write,search',
  v: true,
  s: 'n,nn',
  h: 'n,nn,q,a,r,c'
)
puts response</pre>
</div>
<a id="67bab07fda27ef77e3bc948211051a33"></a>
<div class="pre_wrapper lang-console default has-ruby">
<pre class="programlisting prettyprint lang-console default has-ruby">GET _cat/thread_pool/write,search?v=true&amp;s=n,nn&amp;h=n,nn,q,a,r,c</pre>
</div>
<div class="console_widget has-ruby" data-snippet="snippets/1948.console"></div>
<p>Which could return:</p>
<a id="215617c7efb41f5806f20e2d2ed8f397"></a>
<div class="pre_wrapper lang-console-result">
<pre class="programlisting prettyprint lang-console-result">n      nn       q a r    c
search node_1   3 1 0 1287
search node_2   0 2 0 1159
search node_3   0 1 0 1302
write  node_1 100 3 0 4259
write  node_2   0 4 0  980
write  node_3   1 5 0 8714</pre>
</div>
<p>Here you can see two significantly unique situations. Firstly, <code class="literal">node_1</code> has a
severely backed up write queue compared to other nodes. Secondly, <code class="literal">node_3</code> shows
historically completed writes that are double any other node. These are both
probably due to either poorly distributed write-heavy indices, or to multiple
write-heavy indices allocated to the same node. Since primary and replica writes
are majorly the same amount of cluster work, we usually recommend setting
<a class="xref" href="allocation-total-shards.html#total-shards-per-node"><code class="literal">index.routing.allocation.total_shards_per_node</code></a> to
force index spreading after lining up index shard counts to total nodes.</p>
<p>We normally recommend heavy-write indices have sufficient primary
<code class="literal">number_of_shards</code> and replica <code class="literal">number_of_replicas</code> to evenly spread across
indexing nodes. Alternatively, you can <a class="xref" href="cluster-reroute.html" title="Cluster reroute API">reroute</a> shards to
more quiet nodes to alleviate the nodes with write hot spotting.</p>
<p>If it&#8217;s non-obvious what indices are problematic, you can introspect further via
<a class="xref" href="indices-stats.html" title="Index stats API">the index stats API</a> by running:</p>
<div class="pre_wrapper lang-ruby alternative">
<pre class="programlisting prettyprint lang-ruby alternative">response = client.indices.stats(
  level: 'shards',
  human: true,
  expand_wildcards: 'all',
  filter_path: 'indices.*.total.indexing.index_total'
)
puts response</pre>
</div>
<a id="1a3897cfb4f974c09d0d847baac8aa6d"></a>
<div class="pre_wrapper lang-console default has-ruby">
<pre class="programlisting prettyprint lang-console default has-ruby">GET _stats?level=shards&amp;human&amp;expand_wildcards=all&amp;filter_path=indices.*.total.indexing.index_total</pre>
</div>
<div class="console_widget has-ruby" data-snippet="snippets/1949.console"></div>
<p>For more advanced analysis, you can poll for shard-level stats,
which lets you compare joint index-level and node-level stats. This analysis
wouldn&#8217;t account for node restarts and/or shards rerouting, but serves as
overview:</p>
<div class="pre_wrapper lang-ruby alternative">
<pre class="programlisting prettyprint lang-ruby alternative">response = client.indices.stats(
  metric: 'indexing,search',
  level: 'shards',
  human: true,
  expand_wildcards: 'all'
)
puts response</pre>
</div>
<a id="8e2bbef535fef688d397e60e09aefa7f"></a>
<div class="pre_wrapper lang-console default has-ruby">
<pre class="programlisting prettyprint lang-console default has-ruby">GET _stats/indexing,search?level=shards&amp;human&amp;expand_wildcards=all</pre>
</div>
<div class="console_widget has-ruby" data-snippet="snippets/1950.console"></div>
<p>You can for example use the <a href="https://stedolan.github.io/jq" class="ulink" target="_top">third-party JQ tool</a>,
to process the output saved as <code class="literal">indices_stats.json</code>:</p>
<div class="pre_wrapper lang-sh">
<pre class="programlisting prettyprint lang-sh">cat indices_stats.json | jq -rc ['.indices|to_entries[]|.key as $i|.value.shards|to_entries[]|.key as $s|.value[]|{node:.routing.node[:4], index:$i, shard:$s, primary:.routing.primary, size:.store.size, total_indexing:.indexing.index_total, time_indexing:.indexing.index_time_in_millis, total_query:.search.query_total, time_query:.search.query_time_in_millis } | .+{ avg_indexing: (if .total_indexing&gt;0 then (.time_indexing/.total_indexing|round) else 0 end), avg_search: (if .total_search&gt;0 then (.time_search/.total_search|round) else 0 end) }'] &gt; shard_stats.json

# show top written-to shard simplified stats which contain their index and node references
cat shard_stats.json | jq -rc 'sort_by(-.avg_indexing)[]' | head</pre>
</div>
<h4><a id="causes-tasks"></a>Task loads<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/elasticsearch/edit/main/docs/reference/troubleshooting/common-issues/hotspotting.asciidoc">edit</a></h4>
<p>Shard distribution problems will most-likely surface as task load as seen
above in the <a class="xref" href="cat-thread-pool.html" title="cat thread pool API">cat thread pool</a> example. It is also
possible for tasks to hot spot a node either due to
individual qualitative expensiveness or overall quantitative traffic loads.</p>
<p>For example, if <a class="xref" href="cat-thread-pool.html" title="cat thread pool API">cat thread pool</a> reported a high
queue on the <code class="literal">warmer</code> <a class="xref" href="modules-threadpool.html" title="Thread pools">thread pool</a>, you would
look-up the effected node&#8217;s <a class="xref" href="cluster-nodes-hot-threads.html" title="Nodes hot threads API">hot threads</a>.
Let&#8217;s say it reported <code class="literal">warmer</code> threads at <code class="literal">100% cpu</code> related to
<code class="literal">GlobalOrdinalsBuilder</code>. This would let you know to inspect
<a class="xref" href="eager-global-ordinals.html" title="eager_global_ordinals">field data&#8217;s global ordinals</a>.</p>
<p>Alternatively, let&#8217;s say <a class="xref" href="cat-nodes.html" title="cat nodes API">cat nodes</a> shows a hot spotted master node
and <a class="xref" href="cat-thread-pool.html" title="cat thread pool API">cat thread pool</a> shows general queuing across nodes.
This would suggest the master node is overwhelmed. To resolve
this, first ensure <a class="xref" href="high-availability-cluster-small-clusters.html" title="Resilience in small clusters">hardware high availability</a>
setup and then look to ephemeral causes. In this example,
<a class="xref" href="cluster-nodes-hot-threads.html" title="Nodes hot threads API">the nodes hot threads API</a> reports multiple threads in
<code class="literal">other</code> which indicates they&#8217;re waiting on or blocked by either garbage collection
or I/O.</p>
<p>For either of these example situations, a good way to confirm the problematic tasks
is to look at longest running non-continuous (designated <code class="literal">[c]</code>) tasks via
<a class="xref" href="cat-tasks.html" title="cat task management API">cat task management</a>. This can be supplemented checking longest
running cluster sync tasks via <a class="xref" href="cat-pending-tasks.html" title="cat pending tasks API">cat pending tasks</a>. Using
a third example,</p>
<div class="pre_wrapper lang-ruby alternative">
<pre class="programlisting prettyprint lang-ruby alternative">response = client.cat.tasks(
  v: true,
  s: 'time:desc',
  h: 'type,action,running_time,node,cancellable'
)
puts response</pre>
</div>
<a id="146bd22fd0e7be2345619e8f11d3a4cb"></a>
<div class="pre_wrapper lang-console default has-ruby">
<pre class="programlisting prettyprint lang-console default has-ruby">GET _cat/tasks?v&amp;s=time:desc&amp;h=type,action,running_time,node,cancellable</pre>
</div>
<div class="console_widget has-ruby" data-snippet="snippets/1951.console"></div>
<p>This could return:</p>
<a id="285446914709d09dcb5ca7e7418d288d"></a>
<div class="pre_wrapper lang-console-result">
<pre class="programlisting prettyprint lang-console-result">type   action                running_time  node    cancellable
direct indices:data/read/eql 10m           node_1  true
...</pre>
</div>
<p>This surfaces a problematic <a class="xref" href="eql-search-api.html" title="EQL search API">EQL query</a>. We can gain
further insight on it via <a class="xref" href="tasks.html" title="Task management API">the task management API</a>. Its response
contains a <code class="literal">description</code> that reports this query:</p>
<div class="pre_wrapper lang-eql">
<pre class="programlisting prettyprint lang-eql">indices[winlogbeat-*,logs-window*], sequence by winlog.computer_name with maxspan=1m\n\n[authentication where host.os.type == "windows" and event.action:"logged-in" and\n event.outcome == "success" and process.name == "svchost.exe" ] by winlog.event_data.TargetLogonId</pre>
</div>
<p>This lets you know which indices to check (<code class="literal">winlogbeat-*,logs-window*</code>), as well
as the <a class="xref" href="eql-search-api.html" title="EQL search API">EQL search</a> request body. Most likely this is
<a href="/guide/en/security/master/es-overview.html" class="ulink" target="_top">SIEM related</a>.
You can combine this with <a class="xref" href="enable-audit-logging.html" title="Enable audit logging">audit logging</a> as needed to
trace the request source.</p>
</div>
<div class="navfooter">
<span class="prev">
<a href="mapping-explosion.html">« Mapping explosion</a>
</span>
<span class="next">
<a href="diagnose-unassigned-shards.html">Diagnose unassigned shards »</a>
</span>
</div>
</div>
</body>
</html>
