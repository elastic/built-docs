<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Semantic search | Elasticsearch Guide [master] | Elastic</title>
<meta class="elastic" name="content" content="Semantic search | Elasticsearch Guide [master]">

<link rel="home" href="index.html" title="Elasticsearch Guide [master]"/>
<link rel="up" href="search-with-elasticsearch.html" title="Search your data"/>
<link rel="prev" href="knn-search.html" title="k-nearest neighbor (kNN) search"/>
<link rel="next" href="semantic-search-elser.html" title="Tutorial: semantic search with ELSER"/>
<meta class="elastic" name="product_version" content="master"/>
<meta class="elastic" name="product_name" content="Elasticsearch"/>
<meta class="elastic" name="website_area" content="documentation"/>
<meta name="DC.type" content="Learn/Docs/Elasticsearch/Reference/master"/>
<meta name="DC.subject" content="Elasticsearch"/>
<meta name="DC.identifier" content="master"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body>
<div class="page_header">
This documentation contains work-in-progress information for future Elastic Stack and Cloud releases. Use the version selector to view supported release docs. It also contains some Elastic Cloud serverless information. Check out our <a href="https://www.elastic.co/docs/current/serverless">serverless docs</a> for more details.
</div>
<div class="navheader">
<span class="prev">
<a href="knn-search.html">« k-nearest neighbor (kNN) search</a>
</span>
<span class="next">
<a href="semantic-search-elser.html">Tutorial: semantic search with ELSER »</a>
</span>
</div>
<div class="book" lang="en">
<div class="titlepage">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="index.html">Elasticsearch Guide [master]</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="search-with-elasticsearch.html">Search your data</a></span>
</div>
<div>
<div><h1 class="title"><a id="id-1"></a>Semantic search</h1><a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/main/docs/reference/search/search-your-data/semantic-search.asciidoc">edit</a></div>
</div>
<!--EXTRA-->
</div>
<div id="content">
<div class="chapter">
<div class="titlepage"><div><div>
<h2 class="title"><a id="semantic-search"></a>Semantic search<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/main/docs/reference/search/search-your-data/semantic-search.asciidoc">edit</a></h2>
</div></div></div>
<p>Semantic search is a search method that helps you find data based on the intent and contextual meaning of a search query, instead of a match on query terms (lexical search).</p>
<p>Elasticsearch provides various semantic search capabilities using <a href="/guide/en/machine-learning/master/ml-nlp.html" class="ulink" target="_top">natural language processing (NLP)</a> and vector search.
Using an NLP model enables you to extract text embeddings out of text.
Embeddings are vectors that provide a numeric representation of a text.
Pieces of content with similar meaning have similar representations.
NLP models can be used in the Elastic Stack various ways, you can:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
deploy models in Elasticsearch
</li>
<li class="listitem">
use the <a class="xref" href="semantic-search-semantic-text.html" title="Tutorial: semantic search with semantic_text"><code class="literal">semantic_text</code> workflow</a> (recommended)
</li>
<li class="listitem">
use the <a class="xref" href="semantic-search-inference.html" title="Tutorial: semantic search with the inference API">inference API workflow</a>
</li>
</ul>
</div>
<div id="semantic-search-diagram" class="imageblock text-center">
<div class="content">
<img src="images/search/vector-search-oversimplification.png" alt="A simplified representation of encoding textual concepts as vectors">
</div>
<div class="title">Figure 8. A simplified representation of encoding textual concepts as vectors</div>
</div>
<p>At query time, Elasticsearch can use the same NLP model to convert a query into embeddings, enabling you to find documents with similar text embeddings.</p>
<p>This guide shows you how to implement semantic search with Elasticsearch: From selecting an NLP model, to writing queries.</p>
<div class="important admon">
<div class="icon"></div>
<div class="admon_content">
<p>For the easiest way to perform semantic search in the Elastic Stack, refer to the <a class="xref" href="semantic-search-semantic-text.html" title="Tutorial: semantic search with semantic_text"><code class="literal">semantic_text</code></a> end-to-end tutorial.</p>
</div>
</div>
<h3><a id="semantic-search-select-nlp-model"></a>Select an NLP model<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/main/docs/reference/search/search-your-data/semantic-search.asciidoc">edit</a></h3>
<p>Elasticsearch offers the usage of a
<a href="/guide/en/machine-learning/master/ml-nlp-model-ref.html#ml-nlp-model-ref-text-embedding" class="ulink" target="_top">wide range of NLP models</a>, including both dense and sparse vector models.
Your choice of the language model is critical for implementing semantic search successfully.</p>
<p>While it is possible to bring your own text embedding model, achieving good search results through model tuning is challenging.
Selecting an appropriate model from our third-party model list is the first step.
Training the model on your own data is essential to ensure better search results than using only BM25.
However, the model training process requires a team of data scientists and ML experts, making it expensive and time-consuming.</p>
<p>To address this issue, Elastic provides a pre-trained representational model called <a href="/guide/en/machine-learning/master/ml-nlp-elser.html" class="ulink" target="_top">Elastic Learned Sparse EncodeR (ELSER)</a>.
ELSER, currently available only for English, is an out-of-domain sparse vector model that does not require fine-tuning.
This adaptability makes it suitable for various NLP use cases out of the box.
Unless you have a team of ML specialists, it is highly recommended to use the ELSER model.</p>
<p>In the case of sparse vector representation, the vectors mostly consist of zero values, with only a small subset containing non-zero values.
This representation is commonly used for textual data.
In the case of ELSER, each document in an index and the query text itself are represented by high-dimensional sparse vectors.
Each non-zero element of the vector corresponds to a term in the model vocabulary.
The ELSER vocabulary contains around 30000 terms, so the sparse vectors created by ELSER contain about 30000 values, the majority of which are zero.
Effectively the ELSER model is replacing the terms in the original query with other terms that have been learnt to exist in the documents that best match the original search terms in a training dataset, and weights to control how important each is.</p>
<h3><a id="semantic-search-deploy-nlp-model"></a>Deploy the model<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/main/docs/reference/search/search-your-data/semantic-search.asciidoc">edit</a></h3>
<p>After you decide which model you want to use for implementing semantic search, you need to deploy the model in Elasticsearch.</p>
<div class="tabs" data-tab-group="model">
  <div role="tablist" aria-label="model">
    <button role="tab"
            aria-selected="true"
            aria-controls="elser-tab-deploy-nlp-model"
            id="elser-deploy-nlp-model">
      ELSER
    </button>
    <button role="tab"
            aria-selected="false"
            aria-controls="dense-vector-tab-deploy-nlp-model"
            id="dense-vector-deploy-nlp-model">
      Dense vector models
    </button>
  </div>
  <div tabindex="0"
       role="tabpanel"
       id="elser-tab-deploy-nlp-model"
       aria-labelledby="elser-deploy-nlp-model">
<p>To deploy ELSER, refer to
<a href="/guide/en/machine-learning/master/ml-nlp-elser.html#download-deploy-elser" class="ulink" target="_top">Download and deploy ELSER</a>.</p>
  </div>
  <div tabindex="0"
       role="tabpanel"
       id="dense-vector-tab-deploy-nlp-model"
       aria-labelledby="dense-vector-deploy-nlp-model"
       hidden="">
<p>To deploy a third-party text embedding model, refer to
<a href="/guide/en/machine-learning/master/ml-nlp-text-emb-vector-search-example.html#ex-te-vs-deploy" class="ulink" target="_top">Deploy a text embedding model</a>.</p>
  </div>
</div>
<h3><a id="semantic-search-field-mappings"></a>Map a field for the text embeddings<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/main/docs/reference/search/search-your-data/semantic-search.asciidoc">edit</a></h3>
<p>Before you start using the deployed model to generate embeddings based on your input text, you need to prepare your index mapping first.
The mapping of the index depends on the type of model.</p>
<div class="tabs" data-tab-group="model">
  <div role="tablist" aria-label="model">
    <button role="tab"
            aria-selected="true"
            aria-controls="elser-tab-field-mappings"
            id="elser-field-mappings">
      ELSER
    </button>
    <button role="tab"
            aria-selected="false"
            aria-controls="dense-vector-tab-field-mappings"
            id="dense-vector-field-mappings">
      Dense vector models
    </button>
  </div>
  <div tabindex="0"
       role="tabpanel"
       id="elser-tab-field-mappings"
       aria-labelledby="elser-field-mappings">
<p>ELSER produces token-weight pairs as output from the input text and the query.
The Elasticsearch <a class="xref" href="sparse-vector.html" title="Sparse vector field type"><code class="literal">sparse_vector</code></a> field type can store these
token-weight pairs as numeric feature vectors. The index must have a field with
the <code class="literal">sparse_vector</code> field type to index the tokens that ELSER generates.</p>
<p>To create a mapping for your ELSER index, refer to the
<a class="xref" href="semantic-search-elser.html#elser-mappings" title="Create the index mapping">Create the index mapping section</a> of the tutorial. The example
shows how to create an index mapping for <code class="literal">my-index</code> that defines the
<code class="literal">my_embeddings.tokens</code> field - which will contain the ELSER output - as a
<code class="literal">sparse_vector</code> field.</p>
<div class="pre_wrapper lang-ruby alternative">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-ruby alternative">response = client.indices.create(
  index: 'my-index',
  body: {
    mappings: {
      properties: {
        my_tokens: {
          type: 'sparse_vector'
        },
        my_text_field: {
          type: 'text'
        }
      }
    }
  }
)
puts response</pre>
</div>
<a id="31a79a57b242713edec6795599ba0d5d"></a>
<div class="pre_wrapper lang-console default has-ruby">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console default has-ruby">PUT my-index
{
  "mappings": {
    "properties": {
      "my_tokens": { <a id="CO231-1"></a><i class="conum" data-value="1"></i>
        "type": "sparse_vector" <a id="CO231-2"></a><i class="conum" data-value="2"></i>
      },
      "my_text_field": { <a id="CO231-3"></a><i class="conum" data-value="3"></i>
        "type": "text" <a id="CO231-4"></a><i class="conum" data-value="4"></i>
      }
    }
  }
}</pre>
</div>
<div class="console_widget has-ruby" data-snippet="snippets/1014.console"></div>
<div class="calloutlist default has-ruby lang-console">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO231-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>The name of the field that will contain the tokens generated by ELSER.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO231-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p>The field that contains the tokens must be a <code class="literal">sparse_vector</code> field.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO231-3"><i class="conum" data-value="3"></i></a></p>
</td>
<td align="left" valign="top">
<p>The name of the field from which to create the sparse vector representation.
In this example, the name of the field is <code class="literal">my_text_field</code>.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO231-4"><i class="conum" data-value="4"></i></a></p>
</td>
<td align="left" valign="top">
<p>The field type is <code class="literal">text</code> in this example.</p>
</td>
</tr>
</table>
</div>
  </div>
  <div tabindex="0"
       role="tabpanel"
       id="dense-vector-tab-field-mappings"
       aria-labelledby="dense-vector-field-mappings"
       hidden="">
<p>The models compatible with Elasticsearch NLP generate dense vectors as output. The
<a class="xref" href="dense-vector.html" title="Dense vector field type"><code class="literal">dense_vector</code></a> field type is suitable for storing dense vectors
of numeric values. The index must have a field with the <code class="literal">dense_vector</code> field
type to index the embeddings that the supported third-party model that you
selected generates. Keep in mind that the model produces embeddings with a
certain number of dimensions. The <code class="literal">dense_vector</code> field must be configured with
the same number of dimensions using the <code class="literal">dims</code> option. Refer to the respective
model documentation to get information about the number of dimensions of the
embeddings.</p>
<p>To review a mapping of an index for an NLP model, refer to the mapping code
snippet in the
<a href="/guide/en/machine-learning/master/ml-nlp-text-emb-vector-search-example.html#ex-text-emb-ingest" class="ulink" target="_top">Add the text embedding model to an ingest inference pipeline</a>
section of the tutorial. The example shows how to create an index mapping that
defines the <code class="literal">my_embeddings.predicted_value</code> field - which will contain the model
output - as a <code class="literal">dense_vector</code> field.</p>
<div class="pre_wrapper lang-ruby alternative">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-ruby alternative">response = client.indices.create(
  index: 'my-index',
  body: {
    mappings: {
      properties: {
        'my_embeddings.predicted_value' =&gt; {
          type: 'dense_vector',
          dims: 384
        },
        my_text_field: {
          type: 'text'
        }
      }
    }
  }
)
puts response</pre>
</div>
<a id="a6b2815d54df34b6b8d00226e9a1af0c"></a>
<div class="pre_wrapper lang-console default has-ruby">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console default has-ruby">PUT my-index
{
  "mappings": {
    "properties": {
      "my_embeddings.predicted_value": { <a id="CO232-1"></a><i class="conum" data-value="1"></i>
        "type": "dense_vector", <a id="CO232-2"></a><i class="conum" data-value="2"></i>
        "dims": 384 <a id="CO232-3"></a><i class="conum" data-value="3"></i>
      },
      "my_text_field": { <a id="CO232-4"></a><i class="conum" data-value="4"></i>
        "type": "text" <a id="CO232-5"></a><i class="conum" data-value="5"></i>
      }
    }
  }
}</pre>
</div>
<div class="console_widget has-ruby" data-snippet="snippets/1015.console"></div>
<div class="calloutlist default has-ruby lang-console">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO232-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>The name of the field that will contain the embeddings generated by the
model.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO232-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p>The field that contains the embeddings must be a <code class="literal">dense_vector</code> field.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO232-3"><i class="conum" data-value="3"></i></a></p>
</td>
<td align="left" valign="top">
<p>The model produces embeddings with a certain number of dimensions. The
<code class="literal">dense_vector</code> field must be configured with the same number of dimensions by
the <code class="literal">dims</code> option. Refer to the respective model documentation to get
information about the number of dimensions of the embeddings.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO232-4"><i class="conum" data-value="4"></i></a></p>
</td>
<td align="left" valign="top">
<p>The name of the field from which to create the dense vector representation.
In this example, the name of the field is <code class="literal">my_text_field</code>.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO232-5"><i class="conum" data-value="5"></i></a></p>
</td>
<td align="left" valign="top">
<p>The field type is <code class="literal">text</code> in this example.</p>
</td>
</tr>
</table>
</div>
  </div>
</div>
<h3><a id="semantic-search-generate-embeddings"></a>Generate text embeddings<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/main/docs/reference/search/search-your-data/semantic-search.asciidoc">edit</a></h3>
<p>Once you have created the mappings for the index, you can generate text embeddings from your input text.
This can be done by using an
<a class="xref" href="ingest.html" title="Ingest pipelines">ingest pipeline</a> with an <a class="xref" href="inference-processor.html" title="Inference processor">inference processor</a>.
The ingest pipeline processes the input data and indexes it into the destination index.
At index time, the inference ingest processor uses the trained model to infer against the data ingested through the pipeline.
After you created the ingest pipeline with the inference processor, you can ingest your data through it to generate the model output.</p>
<div class="tabs" data-tab-group="model">
  <div role="tablist" aria-label="model">
    <button role="tab"
            aria-selected="true"
            aria-controls="elser-tab-generate-embeddings"
            id="elser-generate-embeddings">
      ELSER
    </button>
    <button role="tab"
            aria-selected="false"
            aria-controls="dense-vector-tab-generate-embeddings"
            id="dense-vector-generate-embeddings">
      Dense vector models
    </button>
  </div>
  <div tabindex="0"
       role="tabpanel"
       id="elser-tab-generate-embeddings"
       aria-labelledby="elser-generate-embeddings">
<p>This is how an ingest pipeline that uses the ELSER model is created:</p>
<div class="pre_wrapper lang-ruby alternative">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-ruby alternative">response = client.ingest.put_pipeline(
  id: 'my-text-embeddings-pipeline',
  body: {
    description: 'Text embedding pipeline',
    processors: [
      {
        inference: {
          model_id: '.elser_model_2',
          input_output: [
            {
              input_field: 'my_text_field',
              output_field: 'my_tokens'
            }
          ]
        }
      }
    ]
  }
)
puts response</pre>
</div>
<a id="8d064eda2199de52e5be9ee68a5b7c68"></a>
<div class="pre_wrapper lang-console default has-ruby">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console default has-ruby">PUT _ingest/pipeline/my-text-embeddings-pipeline
{
  "description": "Text embedding pipeline",
  "processors": [
    {
      "inference": {
        "model_id": ".elser_model_2",
        "input_output": [ <a id="CO233-1"></a><i class="conum" data-value="1"></i>
          {
            "input_field": "my_text_field",
            "output_field": "my_tokens"
          }
        ]
      }
    }
  ]
}</pre>
</div>
<div class="console_widget has-ruby" data-snippet="snippets/1016.console"></div>
<div class="calloutlist default has-ruby lang-console">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO233-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>Configuration object that defines the <code class="literal">input_field</code> for the inference process
and the <code class="literal">output_field</code> that will contain the inference results.</p>
</td>
</tr>
</table>
</div>
<p>To ingest data through the pipeline to generate tokens with ELSER, refer to the
<a class="xref" href="semantic-search-elser.html#reindexing-data-elser" title="Ingest the data through the inference ingest pipeline">Ingest the data through the inference ingest pipeline</a> section of the tutorial. After you successfully
ingested documents by using the pipeline, your index will contain the tokens
generated by ELSER. Tokens are learned associations capturing relevance, they
are not synonyms. To learn more about what tokens are, refer to
<a href="/guide/en/machine-learning/master/ml-nlp-elser.html#elser-tokens" class="ulink" target="_top">this page</a>.</p>
  </div>
  <div tabindex="0"
       role="tabpanel"
       id="dense-vector-tab-generate-embeddings"
       aria-labelledby="dense-vector-generate-embeddings"
       hidden="">
<p>This is how an ingest pipeline that uses a text embedding model is created:</p>
<div class="pre_wrapper lang-ruby alternative">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-ruby alternative">response = client.ingest.put_pipeline(
  id: 'my-text-embeddings-pipeline',
  body: {
    description: 'Text embedding pipeline',
    processors: [
      {
        inference: {
          model_id: 'sentence-transformers__msmarco-minilm-l-12-v3',
          target_field: 'my_embeddings',
          field_map: {
            my_text_field: 'text_field'
          }
        }
      }
    ]
  }
)
puts response</pre>
</div>
<a id="256eba7a77c8890a43afeda8ce8a3225"></a>
<div class="pre_wrapper lang-console default has-ruby">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console default has-ruby">PUT _ingest/pipeline/my-text-embeddings-pipeline
{
  "description": "Text embedding pipeline",
  "processors": [
    {
      "inference": {
        "model_id": "sentence-transformers__msmarco-minilm-l-12-v3", <a id="CO234-1"></a><i class="conum" data-value="1"></i>
        "target_field": "my_embeddings",
        "field_map": { <a id="CO234-2"></a><i class="conum" data-value="2"></i>
          "my_text_field": "text_field"
        }
      }
    }
  ]
}</pre>
</div>
<div class="console_widget has-ruby" data-snippet="snippets/1017.console"></div>
<div class="calloutlist default has-ruby lang-console">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO234-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>The model ID of the text embedding model you want to use.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO234-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p>The <code class="literal">field_map</code> object maps the input document field name (which is
<code class="literal">my_text_field</code> in this example) to the name of the field that the model expects
(which is always <code class="literal">text_field</code>).</p>
</td>
</tr>
</table>
</div>
<p>To ingest data through the pipeline to generate text embeddings with your chosen
model, refer to the
<a href="/guide/en/machine-learning/master/ml-nlp-text-emb-vector-search-example.html#ex-text-emb-ingest" class="ulink" target="_top">Add the text embedding model to an inference ingest pipeline</a>
section. The example shows how to create the pipeline with the inference
processor and reindex your data through the pipeline. After you successfully
ingested documents by using the pipeline, your index will contain the text
embeddings generated by the model.</p>
  </div>
</div>
<p>Now it is time to perform semantic search!</p>
<h3><a id="semantic-search-search"></a>Search the data<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/main/docs/reference/search/search-your-data/semantic-search.asciidoc">edit</a></h3>
<p>Depending on the type of model you have deployed, you can query rank features with a <a class="xref" href="query-dsl-sparse-vector-query.html" title="Sparse vector query">sparse vector</a> query, or dense vectors with a kNN search.</p>
<div class="tabs" data-tab-group="model">
  <div role="tablist" aria-label="model">
    <button role="tab"
            aria-selected="true"
            aria-controls="elser-tab-search"
            id="elser-search">
      ELSER
    </button>
    <button role="tab"
            aria-selected="false"
            aria-controls="dense-vector-tab-search"
            id="dense-vector-search">
      Dense vector models
    </button>
  </div>
  <div tabindex="0"
       role="tabpanel"
       id="elser-tab-search"
       aria-labelledby="elser-search">
<p>ELSER text embeddings can be queried using a
<a class="xref" href="query-dsl-sparse-vector-query.html" title="Sparse vector query">sparse vector query</a>. The sparse vector
query enables you to query a <a class="xref" href="sparse-vector.html" title="Sparse vector field type">sparse vector</a> field, by
providing the inference ID associated with the NLP model you want to use, and the query text:</p>
<a id="5ddc26da6e163fda54f52d33b5157051"></a>
<div class="pre_wrapper lang-console">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console">GET my-index/_search
{
   "query":{
    "sparse_vector": {
        "field": "my_tokens",
        "inference_id": "my-elser-endpoint",
        "query": "the query string"
      }
   }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/1018.console"></div>
  </div>
  <div tabindex="0"
       role="tabpanel"
       id="dense-vector-tab-search"
       aria-labelledby="dense-vector-search"
       hidden="">
<p>Text embeddings produced by dense vector models can be queried using a
<a class="xref" href="knn-search.html#knn-semantic-search" title="Perform semantic search">kNN search</a>. In the <code class="literal">knn</code> clause, provide the name of the
dense vector field, and a <code class="literal">query_vector_builder</code> clause with the model ID and
the query text.</p>
<div class="pre_wrapper lang-ruby alternative">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-ruby alternative">response = client.search(
  index: 'my-index',
  body: {
    knn: {
      field: 'my_embeddings.predicted_value',
      k: 10,
      num_candidates: 100,
      query_vector_builder: {
        text_embedding: {
          model_id: 'sentence-transformers__msmarco-minilm-l-12-v3',
          model_text: 'the query string'
        }
      }
    }
  }
)
puts response</pre>
</div>
<a id="d952ac7c73219d8cabc080679e035514"></a>
<div class="pre_wrapper lang-console default has-ruby">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console default has-ruby">GET my-index/_search
{
  "knn": {
    "field": "my_embeddings.predicted_value",
    "k": 10,
    "num_candidates": 100,
    "query_vector_builder": {
      "text_embedding": {
        "model_id": "sentence-transformers__msmarco-minilm-l-12-v3",
        "model_text": "the query string"
      }
    }
  }
}</pre>
</div>
<div class="console_widget has-ruby" data-snippet="snippets/1019.console"></div>
  </div>
</div>
<h3><a id="semantic-search-hybrid-search"></a>Beyond semantic search with hybrid search<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/main/docs/reference/search/search-your-data/semantic-search.asciidoc">edit</a></h3>
<p>In some situations, lexical search may perform better than semantic search.
For example, when searching for single words or IDs, like product numbers.</p>
<p>Combining semantic and lexical search into one hybrid search request using
<a class="xref" href="rrf.html" title="Reciprocal rank fusion">reciprocal rank fusion</a> provides the best of both worlds.
Not only that, but hybrid search using reciprocal rank fusion <a href="/blog/improving-information-retrieval-elastic-stack-hybrid" class="ulink" target="_top">has been shown to perform better in general</a>.</p>
<div class="tabs" data-tab-group="model">
  <div role="tablist" aria-label="model">
    <button role="tab"
            aria-selected="true"
            aria-controls="elser-tab-hybrid-search"
            id="elser-hybrid-search">
      ELSER
    </button>
    <button role="tab"
            aria-selected="false"
            aria-controls="dense-vector-tab-hybrid-search"
            id="dense-vector-hybrid-search">
      Dense vector models
    </button>
  </div>
  <div tabindex="0"
       role="tabpanel"
       id="elser-tab-hybrid-search"
       aria-labelledby="elser-hybrid-search">
<p>Hybrid search between a semantic and lexical query can be achieved by using an
<a class="xref" href="retriever.html#rrf-retriever" title="RRF Retriever"><code class="literal">rrf</code> retriever</a> as part of your search request. Provide a
<code class="literal">sparse_vector</code> query and a full-text query as
<a class="xref" href="retriever.html#standard-retriever" title="Standard Retriever"><code class="literal">standard</code> retrievers</a> for the <code class="literal">rrf</code> retriever. The <code class="literal">rrf</code>
retriever uses <a class="xref" href="rrf.html" title="Reciprocal rank fusion">reciprocal rank fusion</a> to rank the top documents.</p>
<a id="4e4608ae4ce93c27bd174a9ea078cab2"></a>
<div class="pre_wrapper lang-console">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console">GET my-index/_search
{
  "retriever": {
    "rrf": {
      "retrievers": [
        {
          "standard": {
            "query": {
              "match": {
                "my_text_field": "the query string"
              }
            }
          }
        },
        {
          "standard": {
            "query": {
             "sparse_vector": {
                "field": "my_tokens",
                "inference_id": "my-elser-endpoint",
                "query": "the query string"
              }
            }
          }
        }
      ]
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/1020.console"></div>
  </div>
  <div tabindex="0"
       role="tabpanel"
       id="dense-vector-tab-hybrid-search"
       aria-labelledby="dense-vector-hybrid-search"
       hidden="">
<p>Hybrid search between a semantic and lexical query can be achieved by providing:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
an <code class="literal">rrf</code> retriever to rank top documents using <a class="xref" href="rrf.html" title="Reciprocal rank fusion">reciprocal rank fusion</a>
</li>
<li class="listitem">
a <code class="literal">standard</code> retriever as a child retriever with <code class="literal">query</code> clause for the full-text query
</li>
<li class="listitem">
a <code class="literal">knn</code> retriever as a child retriever with the kNN search that queries the dense vector field
</li>
</ul>
</div>
<a id="fdf7cfdf1c92d21ee710675596eac6fd"></a>
<div class="pre_wrapper lang-console">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console">GET my-index/_search
{
  "retriever": {
    "rrf": {
      "retrievers": [
        {
          "standard": {
            "query": {
              "match": {
                "my_text_field": "the query string"
              }
            }
          }
        },
        {
          "knn": {
            "field": "text_embedding.predicted_value",
            "k": 10,
            "num_candidates": 100,
            "query_vector_builder": {
              "text_embedding": {
                "model_id": "sentence-transformers__msmarco-minilm-l-12-v3",
                "model_text": "the query string"
              }
            }
          }
        }
      ]
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/1021.console"></div>
  </div>
</div>
<h3><a id="semantic-search-read-more"></a>Read more<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/main/docs/reference/search/search-your-data/semantic-search.asciidoc">edit</a></h3>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<p>Tutorials:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="semantic-search-elser.html" title="Tutorial: semantic search with ELSER">Semantic search with ELSER</a>
</li>
<li class="listitem">
<a href="/guide/en/machine-learning/master/ml-nlp-text-emb-vector-search-example.html" class="ulink" target="_top">Semantic search with the msmarco-MiniLM-L-12-v3 sentence-transformer model</a>
</li>
</ul>
</div>
</li>
<li class="listitem">
<p>Blogs:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a href="/blog/may-2023-launch-sparse-encoder-ai-model" class="ulink" target="_top">Introducing Elastic Learned Sparse Encoder: Elastic&#8217;s AI model for semantic search</a>
</li>
<li class="listitem">
<a href="/blog/lexical-ai-powered-search-elastic-vector-database" class="ulink" target="_top">How to get the best of lexical and AI-powered search with Elastic&#8217;s vector database</a>
</li>
<li class="listitem">
<p>Information retrieval blog series:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a href="/blog/improving-information-retrieval-elastic-stack-search-relevance" class="ulink" target="_top">Part 1: Steps to improve search relevance</a>
</li>
<li class="listitem">
<a href="/blog/improving-information-retrieval-elastic-stack-benchmarking-passage-retrieval" class="ulink" target="_top">Part 2: Benchmarking passage retrieval</a>
</li>
<li class="listitem">
<a href="/blog/may-2023-launch-information-retrieval-elasticsearch-ai-model" class="ulink" target="_top">Part 3: Introducing Elastic Learned Sparse Encoder, our new retrieval model</a>
</li>
<li class="listitem">
<a href="/blog/improving-information-retrieval-elastic-stack-hybrid" class="ulink" target="_top">Part 4: Hybrid retrieval</a>
</li>
</ul>
</div>
</li>
</ul>
</div>
</li>
<li class="listitem">
<p>Interactive examples:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
The <a href="https://github.com/elastic/elasticsearch-labs" class="ulink" target="_top"><code class="literal">elasticsearch-labs</code></a> repo contains a number of interactive semantic search examples in the form of executable Python notebooks, using the Elasticsearch Python client
</li>
</ul>
</div>
</li>
</ul>
</div>




</div>
</div>
</div><div class="navfooter">
<span class="prev">
<a href="knn-search.html">« k-nearest neighbor (kNN) search</a>
</span>
<span class="next">
<a href="semantic-search-elser.html">Tutorial: semantic search with ELSER »</a>
</span>
</div>
</body>
</html>
