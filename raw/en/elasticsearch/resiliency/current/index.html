<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Elasticsearch Resiliency Status | Elastic</title>

<meta name="DC.type" content="Learn/Docs/Elasticsearch/Resiliency Status"/>
<meta name="DC.subject" content="Elasticsearch"/>
<meta name="DC.identifier" content="master"/>
</head>
<body>
<div class="book" lang="en">
<div class="titlepage">
<div>
<div><h1 class="title"><a id="id-1"></a>Elasticsearch Resiliency Status</h1></div>
</div>
<hr>
</div>
<div id="content">
<!--START_TOC-->
<div class="toc">
<ul class="toc">
<li><span class="chapter"><a href="#_overview">Overview</a></span>
</li>
<li><span class="chapter"><a href="#_data_store_recommendations">Data Store Recommendations</a></span>
</li>
<li><span class="chapter"><a href="#_work_in_progress">Work in Progress</a></span>
</li>
<li><span class="chapter"><a href="#_completed">Completed</a></span>
</li>
</ul>
</div>
<!--END_TOC-->

<div class="chapter">
<div class="titlepage"><div><div>
<h1 class="title"><a id="_overview"></a>Overview<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h1>
</div></div></div>
<p>The team at Elasticsearch is committed to continuously improving both
Elasticsearch and Apache Lucene to protect your data.  As with any distributed
system, Elasticsearch is complex and has many moving parts, each of which can
encounter edge cases that require proper handling.  Our resiliency project is
an ongoing effort to find and fix these edge cases. If you want to keep up
with all this project on GitHub, see our issues list under the tag
<a href="https://github.com/elastic/elasticsearch/issues?q=label%3Aresiliency" class="ulink" target="_top">resiliency</a>.</p>
<p>While GitHub is great for sharing our work, it can be difficult to get an
overview of the current state of affairs and the previous work that has been
done from an issues list. This page provides an overview of all the
resiliency-related issues that we are aware of, improvements that have already
been made and current in-progress work. We’ve also listed some historical
improvements throughout this page to provide the full context.</p>
<p>If you’re interested in more on how we approach ensuring resiliency in
Elasticsearch, you may be interested in Igor Motov’s talk
<a href="/videos/improving-elasticsearch-resiliency" class="ulink" target="_top">Improving Elasticsearch Resiliency</a>.</p>
<p>You may also be interested in our blog post
<a href="/blog/resiliency-elasticsearch" class="ulink" target="_top">Resiliency in Elasticsearch</a>,
which details our thought processes when addressing resiliency in both
Elasticsearch and the work our developers do upstream in Apache Lucene.</p>
</div>

<div class="chapter">
<div class="titlepage"><div><div>
<h1 class="title"><a id="_data_store_recommendations"></a>Data Store Recommendations<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h1>
</div></div></div>
<p>Some customers use Elasticsearch as a primary datastore, some set-up
comprehensive back-up solutions using features such as our Snapshot and
Restore, while others use Elasticsearch in conjunction with a data storage
system like Hadoop or even flat files. Elasticsearch can be used for so many
different use cases which is why we have created this page to make sure you
are fully informed when you are architecting your system.</p>
</div>

<div class="chapter">
<div class="titlepage"><div><div>
<h1 class="title"><a id="_work_in_progress"></a>Work in Progress<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h1>
</div></div></div>
<h3><a id="_known_unknowns_status_ongoing"></a>Known Unknowns (STATUS: ONGOING)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>We consider this topic to be the most important in our quest for
resiliency. We put a tremendous amount of effort into testing
Elasticsearch to simulate failures and randomize configuration to
produce extreme conditions. In addition, our users are an important
source of information on unexpected edge cases and your bug reports
help us make fixes that ensure that our system continues to be
resilient.</p>
<p>If you encounter an issue, <a href="https://github.com/elastic/elasticsearch/issues" class="ulink" target="_top">please report it</a>!</p>
<p>We are committed to tracking down and fixing all the issues that are posted.</p>
<h4><a id="_jepsen_tests"></a>Jepsen Tests<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h4>
<p>The Jepsen platform is specifically designed to test distributed systems. It is not a single test and is regularly adapted
to create new scenarios. We have currently ported all published Jepsen scenarios that deal with loss of acknowledged writes to our testing
framework. As the Jepsen tests evolve, we will continue porting new scenarios that are not covered yet. We are committed to investigating
all new scenarios and will report issues that we find on this page and in our GitHub repository.</p>
<h3><a id="_better_request_retry_mechanism_when_nodes_are_disconnected_status_ongoing"></a>Better request retry mechanism when nodes are disconnected (STATUS: ONGOING)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>If the node holding a primary shard is disconnected for whatever reason, the
coordinating node retries the request on the same or a new primary shard.  In
certain rare conditions, where the node disconnects and immediately
reconnects, it is possible that the original request has already been
successfully applied but has not been reported, resulting in duplicate
requests. This is particularly true when retrying bulk requests, where some
actions may  have completed and some may not have.</p>
<p>An optimization which disabled the existence check for documents indexed with
auto-generated IDs could result in the creation of duplicate documents. This
optimization has been removed. <a href="https://github.com/elastic/elasticsearch/issues/9468" class="ulink" target="_top">#9468</a> (STATUS: DONE, v1.5.0)</p>
<p>Further issues remain with the retry mechanism:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Unversioned index requests could increment the <code class="literal">_version</code> twice,
obscuring a <code class="literal">created</code> status.
</li>
<li class="listitem">
Versioned index requests could return a conflict exception, even
though they were applied correctly.
</li>
<li class="listitem">
Update requests could be applied twice.
</li>
</ul>
</div>
<p>See <a href="https://github.com/elastic/elasticsearch/issues/9967" class="ulink" target="_top">#9967</a>. (STATUS: ONGOING)</p>
<h3><a id="_oom_resiliency_status_ongoing"></a>OOM resiliency (STATUS: ONGOING)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>The family of circuit breakers has greatly reduced the occurrence of OOM
exceptions, but it is still possible to cause a node to run out of heap
space.  The following issues have been identified:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Set a hard limit on <code class="literal">from</code>/<code class="literal">size</code> parameters <a href="https://github.com/elastic/elasticsearch/issues/9311" class="ulink" target="_top">#9311</a>. (STATUS: DONE, v2.1.0)
</li>
<li class="listitem">
Prevent combinatorial explosion in aggregations from causing OOM <a href="https://github.com/elastic/elasticsearch/issues/8081" class="ulink" target="_top">#8081</a>. (STATUS: DONE, v5.0.0)
</li>
<li class="listitem">
Add the byte size of each hit to the request circuit breaker <a href="https://github.com/elastic/elasticsearch/issues/9310" class="ulink" target="_top">#9310</a>. (STATUS: ONGOING)
</li>
<li class="listitem">
Limit the size of individual requests and also add a circuit breaker for the total memory used by in-flight request objects <a href="https://github.com/elastic/elasticsearch/issues/16011" class="ulink" target="_top">#16011</a>. (STATUS: DONE, v5.0.0)
</li>
</ul>
</div>
<p>Other safeguards are tracked in the meta-issue <a href="https://github.com/elastic/elasticsearch/issues/11511" class="ulink" target="_top">#11511</a>.</p>
<h3><a id="_relocating_shards_omitted_by_reporting_infrastructure_status_ongoing"></a>Relocating shards omitted by reporting infrastructure (STATUS: ONGOING)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>Indices stats and indices segments requests reach out to all nodes that have shards of that index. Shards that have relocated from a node
while the stats request arrives will make that part of the request fail and are just ignored in the overall stats result. <a href="https://github.com/elastic/elasticsearch/issues/13719" class="ulink" target="_top">#13719</a></p>
<h3><a id="_documentation_of_guarantees_and_handling_of_failures_status_ongoing"></a>Documentation of guarantees and handling of failures (STATUS: ONGOING)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>This status page is a start, but we can do a better job of explicitly documenting the processes at work in Elasticsearch and what happens
in the case of each type of failure. The plan is to have a test case that validates each behavior under simulated conditions. Every test
 will document the expected results, the associated test code, and an explicit PASS or FAIL status for each simulated case.</p>
<h3><a id="_run_jepsen_status_ongoing"></a>Run Jepsen (STATUS: ONGOING)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>We have ported the known scenarios in the Jepsen blogs that check loss of acknowledged writes to our testing infrastructure.
The new tests are run continuously in our testing farm and are passing. We are also working on running Jepsen independently to verify
that no failures are found.</p>
</div>

<div class="chapter">
<div class="titlepage"><div><div>
<h1 class="title"><a id="_completed"></a>Completed<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h1>
</div></div></div>
<h3><a id="_documents_indexed_during_a_network_partition_cannot_be_uniquely_identified_status_done_v7_0_0"></a>Documents indexed during a network partition cannot be uniquely identified (STATUS: DONE, v7.0.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>When a primary has been partitioned away from the cluster there is a short
period of time until it detects this. During that time it will continue
indexing writes locally, thereby updating document versions. When it tries
to replicate the operation, however, it will discover that it is partitioned
away. It won&#8217;t acknowledge the write and will wait until the partition is
resolved to negotiate with the master on how to proceed. The master will
decide to either fail any replicas which failed to index the operations on
the primary or tell the primary that it has to step down because a new primary
has been chosen in the meantime. Since the old primary has already written
documents, clients may already have read from the old primary before it shuts
itself down. The <code class="literal">_version</code> field of these reads may not uniquely identify the
document&#8217;s version if the new primary has already accepted writes for the same
document (see <a href="https://github.com/elastic/elasticsearch/issues/19269" class="ulink" target="_top">#19269</a>).</p>
<p>The Sequence numbers infrastructure <a href="https://github.com/elastic/elasticsearch/issues/10708" class="ulink" target="_top">#10708</a> has introduced more
precise ways for tracking primary changes. This new infrastructure therefore
provides a way for uniquely identifying documents using their primary term
and sequence number fields, even in the presence of network partitions, and
has been used to replace the <code class="literal">_version</code> field in operations that require
uniquely identifying the document, such as optimistic concurrency control.</p>
<h3><a id="_replicas_can_fall_out_of_sync_when_a_primary_shard_fails_status_done_v7_0_0"></a>Replicas can fall out of sync when a primary shard fails (STATUS: DONE, v7.0.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>When a primary shard fails, a replica shard will be promoted to be the primary
shard. If there is more than one replica shard, it is possible for the
remaining replicas to be out of sync with the new primary shard. This is caused
by operations that were in-flight when the primary shard failed and may not
have been processed on all replica shards. These discrepancies are not
repaired on primary promotion but instead delayed until replica shards are
relocated (e.g., from hot to cold nodes); this means that the length of time
in which replicas can be out of sync with the primary shard is unbounded.</p>
<p>Sequence numbers <a href="https://github.com/elastic/elasticsearch/issues/10708" class="ulink" target="_top">#10708</a> provide a mechanism for identifying
the discrepancies between shard copies at the document level, which allows
to efficiently sync up the remaining replicas with the newly-promoted primary
shard.</p>
<h3><a id="_repeated_network_partitions_can_cause_cluster_state_updates_to_be_lost_status_done_v7_0_0"></a>Repeated network partitions can cause cluster state updates to be lost (STATUS: DONE, v7.0.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>During a networking partition, cluster state updates (like mapping changes or
shard assignments) are committed if a majority of the master-eligible nodes
received the update correctly. This means that the current master has access to
enough nodes in the cluster to continue to operate correctly. When the network
partition heals, the isolated nodes catch up with the current state and receive
the previously missed changes. However, if a second partition happens while the
cluster is still recovering from the previous one <span class="strong strong"><strong>and</strong></span> the old master falls on
the minority side, it may be that a new master is elected which has not yet
catch up. If that happens, cluster state updates can be lost.</p>
<p>This problem is mostly fixed by <a href="https://github.com/elastic/elasticsearch/issues/20384" class="ulink" target="_top">#20384</a> (v5.0.0), which takes
committed cluster state updates into account during master election. This
considerably reduces the chance of this rare problem occurring but does not
fully mitigate it. If the second partition happens concurrently with a cluster
state update and blocks the cluster state commit message from reaching a
majority of nodes, it may be that the in flight update will be lost. If the
now-isolated master can still acknowledge the cluster state update to the client
this will amount to the loss of an acknowledged change.</p>
<p>Fixing this last scenario was one of the goals of <a href="https://github.com/elastic/elasticsearch/issues/32006" class="ulink" target="_top">#32006</a> and its
sub-issues. See particularly <a href="https://github.com/elastic/elasticsearch/issues/32171" class="ulink" target="_top">#32171</a> and
<a href="https://github.com/elastic/elasticsearch-formal-models/blob/master/ZenWithTerms/tla/ZenWithTerms.tla" class="ulink" target="_top">the
TLA+ formal model</a> used to verify these changes.</p>
<h3><a id="_divergence_between_primary_and_replica_shard_copies_when_documents_deleted_status_done_v6_3_0"></a>Divergence between primary and replica shard copies when documents deleted (STATUS: DONE, V6.3.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>Certain combinations of delays in performing activities related to the deletion
of a document could result in the operations on that document being interpreted
differently on different shard copies. This could lead to a divergence in the
number of documents held in each copy.</p>
<p>Deleting an unacknowledged document that was concurrently being inserted using
an auto-generated ID was erroneously sensitive to the order in which those
operations were processed on each shard copy. Thanks to the introduction of
sequence numbers (<a href="https://github.com/elastic/elasticsearch/issues/10708" class="ulink" target="_top">#10708</a>) it is now possible to detect these
out-of-order operations, and this issue was fixed in <a href="https://github.com/elastic/elasticsearch/issues/28787" class="ulink" target="_top">#28787</a>.</p>
<p>Re-creating a document a specific interval after it was deleted could result in
that document&#8217;s tombstone having being cleaned up on some, but not all, copies
when processing the indexing operation that re-creates it. This resulted in
varying behaviour across the shard copies. The problematic interval was set by
the <code class="literal">index.gc_deletes</code> setting, which is 60 seconds by default. Again, sequence
numbers (<a href="https://github.com/elastic/elasticsearch/issues/10708" class="ulink" target="_top">#10708</a>) gives us the machinery to detect these conflicting
activities, and this issue was fixed in <a href="https://github.com/elastic/elasticsearch/issues/28790" class="ulink" target="_top">#28790</a>.</p>
<p>Under certain rare circumstances a replica might erroneously interpret a stale
tombstone for a document as fresh, resulting in a concurrent indexing operation
for that same document behaving differently on this replica than on the
primary. This is fixed in <a href="https://github.com/elastic/elasticsearch/issues/29619" class="ulink" target="_top">#29619</a>. Triggering this issue required
the following activities all to occur in a short time window, in a specific
order on the primary and a different specific order on the replica:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
a document is deleted twice
</li>
<li class="listitem">
another document is indexed with the same ID as this first document
</li>
<li class="listitem">
another document is indexed with a completely different, auto-generated, ID
</li>
<li class="listitem">
two refreshes
</li>
</ul>
</div>
<p>We found the first two of these issues by empirical testing, and then we built
<a href="https://github.com/elastic/elasticsearch-formal-models/blob/master/ReplicaEngine/tla/ReplicaEngine.tla" class="ulink" target="_top">a
formal model of the replica&#8217;s behaviour</a> using TLA+. Running the TLC model
checker on this model found all three issues. We then applied the proposed
fixes to the model and validated that the fixed design behaved as expected.</p>
<h3><a id="_port_jepsen_tests_dealing_with_loss_of_acknowledged_writes_to_our_testing_framework_status_done_v5_0_0"></a>Port Jepsen tests dealing with loss of acknowledged writes to our testing framework (STATUS: DONE, V5.0.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>We have increased our test coverage to include scenarios tested by Jepsen that demonstrate loss of acknowledged writes, as described in
the Elasticsearch related blogs. We make heavy use of randomization to expand on the scenarios that can be tested and to introduce
new error conditions.
You can view these changes on the <code class="literal">5.0</code> branch of the
<a href="https://github.com/elastic/elasticsearch/blob/5.0/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java" class="ulink" target="_top"><code class="literal">DiscoveryWithServiceDisruptionsIT</code> class</a>,
where the <code class="literal">testAckedIndexing</code> test was specifically added to check that we don&#8217;t lose acknowledged writes in various failure scenarios.</p>
<h3><a id="_loss_of_documents_during_network_partition_status_done_v5_0_0"></a>Loss of documents during network partition (STATUS: DONE, v5.0.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>If a network partition separates a node from the master, there is some window of time before the node detects it. The length of the window is dependent on the type of the partition. This window is extremely small if a socket is broken. More adversarial partitions, for example, silently dropping requests without breaking the socket can take longer (up to 3x30s using current defaults).</p>
<p>If the node hosts a primary shard at the moment of partition, and ends up being isolated from the cluster (which could have resulted in <a href="https://github.com/elastic/elasticsearch/issues/2488" class="ulink" target="_top">split-brain</a> before), some documents that are being indexed into the primary may be lost if they fail to reach one of the allocated replicas (due to the partition) and that replica is later promoted to primary by the master (<a href="https://github.com/elastic/elasticsearch/issues/7572" class="ulink" target="_top">#7572</a>).
To prevent this situation, the primary needs to wait for the master to acknowledge replica shard failures before acknowledging the write to the client. <a href="https://github.com/elastic/elasticsearch/issues/14252" class="ulink" target="_top">#14252</a></p>
<h3><a id="_safe_primary_relocations_status_done_v5_0_0"></a>Safe primary relocations (STATUS: DONE, v5.0.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>When primary relocation completes, a cluster state is propagated that deactivates the old primary and marks the new primary as active. As
cluster state changes are not applied synchronously on all nodes, there can be a time interval where the relocation target has processed the
cluster state and believes to be the active primary and the relocation source has not yet processed the cluster state update and still
believes itself to be the active primary. This means that an index request that gets routed to the new primary does not get replicated to
the old primary (as it has been deactivated from point of view of the new primary). If a subsequent read request gets routed to the old
primary, it cannot see the indexed document. <a href="https://github.com/elastic/elasticsearch/issues/15900" class="ulink" target="_top">#15900</a></p>
<p>In the reverse situation where a cluster state update that completes primary relocation is first applied on the relocation source and then
on the relocation target, each of the nodes believes the other to be the active primary. This leads to the issue of indexing requests
chasing the primary being quickly sent back and forth between the nodes, potentially making them both go OOM. <a href="https://github.com/elastic/elasticsearch/issues/12573" class="ulink" target="_top">#12573</a></p>
<h3><a id="_do_not_allow_stale_shards_to_automatically_be_promoted_to_primary_status_done_v5_0_0"></a>Do not allow stale shards to automatically be promoted to primary (STATUS: DONE, v5.0.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>In some scenarios, after the loss of all valid copies, a stale replica shard can be automatically assigned as a primary, preferring old data
to no data at all (<a href="https://github.com/elastic/elasticsearch/issues/14671" class="ulink" target="_top">#14671</a>). This can lead to a loss of acknowledged writes if the valid copies are not lost but are rather
temporarily unavailable. Allocation IDs (<a href="https://github.com/elastic/elasticsearch/issues/14739" class="ulink" target="_top">#14739</a>) solve this issue by tracking non-stale shard copies in the cluster and using
this tracking information to allocate primary shards. When all shard copies are lost or only stale ones available, Elasticsearch will wait
for one of the good shard copies to reappear. In case where all good copies are lost, a manual override command can be used to allocate a
stale shard copy.</p>
<h3><a id="_make_index_creation_resilient_to_index_closing_and_full_cluster_crashes_status_done_v5_0_0"></a>Make index creation resilient to index closing and full cluster crashes (STATUS: DONE, v5.0.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>Recovering an index requires a quorum (with an exception for 2) of shard copies to be available to allocate a primary. This means that
a primary cannot be assigned if the cluster dies before enough shards have been allocated (<a href="https://github.com/elastic/elasticsearch/issues/9126" class="ulink" target="_top">#9126</a>). The same happens if an index
is closed before enough shard copies were started, making it impossible to reopen the index (<a href="https://github.com/elastic/elasticsearch/issues/15281" class="ulink" target="_top">#15281</a>).
Allocation IDs (<a href="https://github.com/elastic/elasticsearch/issues/14739" class="ulink" target="_top">#14739</a>) solve this issue by tracking allocated shard copies in the cluster. This makes it possible to safely
recover an index in the presence of a single shard copy. Allocation IDs can also distinguish the situation where an index has been created
but none of the shards have been started. If such an index was inadvertently closed before at least one shard could be started, a fresh
shard will be allocated upon reopening the index.</p>
<h3><a id="_use_two_phase_commit_for_cluster_state_publishing_status_done_v5_0_0"></a>Use two phase commit for Cluster State publishing (STATUS: DONE, v5.0.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>A master node in Elasticsearch continuously <a href="/guide/en/elasticsearch/reference/current/cluster-fault-detection.html" class="ulink" target="_top">monitors the cluster nodes</a>
and removes any node from the cluster that doesn&#8217;t respond to its pings in a timely
fashion. If the master is left with too few nodes, it will step down and a new master election will start.</p>
<p>When a network partition causes a master node to lose many followers, there is a short window
in time until the node loss is detected and the master steps down. During that window, the
master may erroneously accept and acknowledge cluster state changes. To avoid this, we introduce
a new phase to cluster state publishing where the proposed cluster state is sent to all nodes
but is not yet committed. Only once enough nodes actively acknowledge
the change, it is committed and commit messages are sent to the nodes. See <a href="https://github.com/elastic/elasticsearch/issues/13062" class="ulink" target="_top">#13062</a>.</p>
<h3><a id="_wait_on_incoming_joins_before_electing_local_node_as_master_status_done_v2_0_0"></a>Wait on incoming joins before electing local node as master (STATUS: DONE, v2.0.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>During master election each node pings in order to discover other nodes and validate the liveness of existing
nodes. Based on this information the node either discovers an existing master or, if enough nodes are found a new master will be elected. Currently, the node that is
elected as master will update the cluster state to indicate the result of the election. Other nodes will submit
a join request to the newly elected master node. Instead of immediately processing the election result, the elected master
node should wait for the incoming joins from other nodes, thus validating that the result of the election is properly applied. As soon as enough
nodes have sent their joins request (based on the <code class="literal">minimum_master_nodes</code> settings) the cluster state is updated.
<a href="https://github.com/elastic/elasticsearch/issues/12161" class="ulink" target="_top">#12161</a></p>
<h3><a id="_mapping_changes_should_be_applied_synchronously_status_done_v2_0_0"></a>Mapping changes should be applied synchronously (STATUS: DONE, v2.0.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>When introducing new fields using dynamic mapping, it is possible that the same
field can be added to different shards with different data types.  Each shard
will operate with its local data type but, if the shard is relocated, the
data type from the cluster state will be applied to the new shard, which
can result in a corrupt shard.  To prevent this, new fields should not
be added to a shard&#8217;s mapping until confirmed by the master.
<a href="https://github.com/elastic/elasticsearch/issues/8688" class="ulink" target="_top">#8688</a> (STATUS: DONE)</p>
<h3><a id="_add_per_segment_and_per_commit_id_to_help_replication_status_done_v2_0_0"></a>Add per-segment and per-commit ID to help replication (STATUS: DONE, v2.0.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p><a href="https://issues.apache.org/jira/browse/LUCENE-5895" class="ulink" target="_top">LUCENE-5895</a> adds a unique ID for each segment and each commit point. File-based replication (as performed by snapshot/restore) can use this ID to know whether the segment/commit on the source and destination machines are the same.  Fixed in Lucene 5.0.</p>
<h3><a id="_write_index_metadata_on_data_nodes_where_shards_allocated_status_done_v2_0_0"></a>Write index metadata on data nodes where shards allocated (STATUS: DONE, v2.0.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>Today, index metadata is written only on nodes that are master-eligible, not on
data-only nodes.  This is not a problem when running with multiple master nodes,
as recommended, as the loss of all but one master node is still recoverable.
However, users running with a single master node are at risk of losing
their index metadata if the master fails.  Instead, this metadata should
also be written on any node where a shard is allocated. <a href="https://github.com/elastic/elasticsearch/issues/8823" class="ulink" target="_top">#8823</a>, <a href="https://github.com/elastic/elasticsearch/issues/9952" class="ulink" target="_top">#9952</a></p>
<h3><a id="_better_file_distribution_with_multiple_data_paths_status_done_v2_0_0"></a>Better file distribution with multiple data paths (STATUS: DONE, v2.0.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>Today, a node configured with multiple data paths distributes writes across
all paths by writing one file to each path in turn.  This can mean that the
failure of a single disk corrupts many shards at once.  Instead, by allocating
an entire shard to a single data path, the extent of the damage can be limited
to just the shards on that disk. <a href="https://github.com/elastic/elasticsearch/issues/9498" class="ulink" target="_top">#9498</a></p>
<h3><a id="_lucene_checksums_phase_3_status_done_v2_0_0"></a>Lucene checksums phase 3 (STATUS: DONE, v2.0.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>Almost all files in Elasticsearch now have checksums which are validated before use.  A few changes remain:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a href="https://github.com/elastic/elasticsearch/issues/7586" class="ulink" target="_top">#7586</a> adds checksums for cluster and index state files. (STATUS: DONE, Fixed in v1.5.0)
</li>
<li class="listitem">
<a href="https://github.com/elastic/elasticsearch/issues/9183" class="ulink" target="_top">#9183</a> supports validating the checksums on all files when starting a node. (STATUS: DONE, Fixed in v2.0.0)
</li>
<li class="listitem">
<a href="https://issues.apache.org/jira/browse/LUCENE-5894" class="ulink" target="_top">LUCENE-5894</a> lays the groundwork for extending more efficient checksum validation to all files during optimized bulk merges. (STATUS: DONE, Fixed in v2.0.0)
</li>
<li class="listitem">
<a href="https://github.com/elastic/elasticsearch/issues/8403" class="ulink" target="_top">#8403</a> to add validation of checksums on Lucene <code class="literal">segments_N</code> files. (STATUS: DONE, v2.0.0)
</li>
</ul>
</div>
<h3><a id="_report_shard_level_statuses_on_write_operations_status_done_v2_0_0"></a>Report shard-level statuses on write operations (STATUS: DONE, v2.0.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>Make write calls return the number of total/successful/missing shards in the same way that we do in search, which ensures transparency in the consistency of write operations. <a href="https://github.com/elastic/elasticsearch/issues/7994" class="ulink" target="_top">#7994</a>. (STATUS: DONE, v2.0.0)</p>
<h3><a id="_take_filter_cache_key_size_into_account_status_done_v2_0_0"></a>Take filter cache key size into account (STATUS: DONE, v2.0.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>Commonly used filters are cached in Elasticsearch. That cache is limited in size
(10% of node&#8217;s memory by default) and is being evicted based on a least recently
used policy. The amount of memory used by the cache depends on two primary
components - the values it stores and the keys associated with them. Calculating
the memory footprint of the values is easy enough but the keys accounting is
trickier to achieve as they are, by default, raw Lucene objects. This is largely
not a problem as the keys are dominated by the values. However, recent
optimizations in Lucene have changed the balance causing the filter cache to
grow beyond it&#8217;s size.</p>
<p>As a temporary solution, we introduced a minimum weight of 1k for each cache entry.
This puts an effective limit on the number of entries in the cache. See <a href="https://github.com/elastic/elasticsearch/issues/8304" class="ulink" target="_top">#8304</a> (STATUS: DONE, fixed in v1.4.0)</p>
<p>The issue has been completely solved by the move to Lucene&#8217;s query cache. See <a href="https://github.com/elastic/elasticsearch/issues/10897" class="ulink" target="_top">#10897</a></p>
<h3><a id="_ensure_shard_state_id_is_incremental_status_done_v1_5_1"></a>Ensure shard state ID is incremental (STATUS: DONE, v1.5.1)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>It is possible in very extreme cases during a complicated full cluster restart,
that the current shard state ID can be reset or even go backwards.
Elasticsearch now ensures that the state ID always moves
forwards, and throws an exception when a legacy ID is higher than the
current ID.  See <a href="https://github.com/elastic/elasticsearch/issues/10316" class="ulink" target="_top">#10316</a> (STATUS: DONE, v1.5.1)</p>
<h3><a id="_verification_of_index_uuids_status_done_v1_5_0"></a>Verification of index UUIDs (STATUS: DONE, v1.5.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>When deleting and recreating indices rapidly, it is possible that cluster state
updates can arrive out of sync and old states can be merged incorrectly.  Instead,
Elasticsearch now checks the index UUID to ensure that cluster state updates
refer to the same index version that is present on the local node.
See <a href="https://github.com/elastic/elasticsearch/issues/9541" class="ulink" target="_top">#9541</a> and <a href="https://github.com/elastic/elasticsearch/issues/10200" class="ulink" target="_top">#10200</a> (STATUS: DONE, Fixed in v1.5.0)</p>
<h3><a id="_disable_recovery_from_known_buggy_versions_status_done_v1_5_0"></a>Disable recovery from known buggy versions (STATUS: DONE, v1.5.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>Corruptions have been known to occur when doing a rolling restart from older, buggy versions.
Now, shards from versions before v1.4.0 are copied over in full and recovery from versions
before v1.3.2 are disabled entirely. See <a href="https://github.com/elastic/elasticsearch/issues/9925" class="ulink" target="_top">#9925</a> (STATUS: DONE, Fixed in v1.5.0)</p>
<h3><a id="_upgrade_3_x_segments_metadata_on_engine_startup_status_done_v1_5_0"></a>Upgrade 3.x segments metadata on engine startup (STATUS: DONE, v1.5.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>Upgrading the metadata of old 3.x segments on node upgrade can be error prone
and can result in corruption when merges are being run concurrently. Instead,
Elasticsearch will now upgrade the metadata of 3.x segments before the engine
starts.  See <a href="https://github.com/elastic/elasticsearch/issues/9899" class="ulink" target="_top">#9899</a> (STATUS; DONE, fixed in v1.5.0)</p>
<h3><a id="_prevent_setting_minimum_master_nodes_to_more_than_the_current_node_count_status_done_v1_5_0"></a>Prevent setting minimum_master_nodes to more than the current node count (STATUS: DONE, v1.5.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>Setting <code class="literal">zen.discovery.minimum_master_nodes</code> to a value higher than the current node count
effectively leaves the cluster without a master and unable to process requests.  The only
way to fix this is to add more master-eligible nodes.  <a href="https://github.com/elastic/elasticsearch/issues/8321" class="ulink" target="_top">#8321</a> adds a mechanism
to validate settings before applying them, and <a href="https://github.com/elastic/elasticsearch/issues/9051" class="ulink" target="_top">#9051</a> extends this validation
support to settings applied during a cluster restore. (STATUS: DONE, Fixed in v1.5.0)</p>
<h3><a id="_simplify_and_harden_shard_recovery_and_allocation_status_done_v1_5_0"></a>Simplify and harden shard recovery and allocation (STATUS: DONE, v1.5.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>Randomized testing combined with chaotic failures has revealed corner cases
where the recovery and allocation of shards in a concurrent manner can result
in shard corruption.  There is an ongoing effort to reduce the complexity of
these operations in order to make them more deterministic.  These include:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Introduce shard level locks to prevent concurrent shard modifications <a href="https://github.com/elastic/elasticsearch/issues/8436" class="ulink" target="_top">#8436</a>. (STATUS: DONE, Fixed in v1.5.0)
</li>
<li class="listitem">
Delete shard contents under a lock <a href="https://github.com/elastic/elasticsearch/issues/9083" class="ulink" target="_top">#9083</a>. (STATUS: DONE, Fixed in v1.5.0)
</li>
<li class="listitem">
Delete shard under a lock <a href="https://github.com/elastic/elasticsearch/issues/8579" class="ulink" target="_top">#8579</a>. (STATUS: DONE, Fixed in v1.5.0)
</li>
<li class="listitem">
Refactor RecoveryTarget state management <a href="https://github.com/elastic/elasticsearch/issues/8092" class="ulink" target="_top">#8092</a>. (STATUS: DONE, Fixed in v1.5.0)
</li>
<li class="listitem">
Cancelling a recovery may leave temporary files behind <a href="https://github.com/elastic/elasticsearch/issues/7893" class="ulink" target="_top">#7893</a>. (STATUS: DONE, Fixed in v1.5.0)
</li>
<li class="listitem">
Quick cluster state processing can result in both shard copies being deleted <a href="https://github.com/elastic/elasticsearch/issues/9503" class="ulink" target="_top">#9503</a>. (STATUS: DONE, Fixed in v1.5.0)
</li>
<li class="listitem">
Rapid creation and deletion of an index can cause reuse of old index metadata <a href="https://github.com/elastic/elasticsearch/issues/9489" class="ulink" target="_top">#9489</a>. (STATUS: DONE, Fixed in v1.5.0)
</li>
<li class="listitem">
Flush immediately after the last concurrent recovery finishes to clear out the translog before a new recovery starts <a href="https://github.com/elastic/elasticsearch/issues/9439" class="ulink" target="_top">#9439</a>. (STATUS: DONE, Fixed in v1.5.0)
</li>
</ul>
</div>
<h3><a id="_prevent_use_of_known_bad_java_versions_status_done_v1_5_0"></a>Prevent use of known-bad Java versions (STATUS: DONE, v1.5.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>Certain versions of the JVM are known to have bugs which can cause index corruption.  <a href="https://github.com/elastic/elasticsearch/issues/7580" class="ulink" target="_top">#7580</a> prevents Elasticsearch startup if known bad versions are in use.</p>
<h3><a id="_make_recovery_be_more_resilient_to_partial_network_partitions_status_done_v1_5_0"></a>Make recovery be more resilient to partial network partitions (STATUS: DONE, v1.5.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>When a node is experience network issues, the master detects it and removes the node from the cluster. That causes all ongoing recoveries from and to that node to be stopped and a new location is found for the relevant shards. However, in the of case partial network partition, where there are connectivity issues between the source and target nodes of a recovery but not between those nodes and the current master things may go wrong. While the nodes successfully restore the connection, the on going recoveries may have encountered issues. In <a href="https://github.com/elastic/elasticsearch/issues/8720" class="ulink" target="_top">#8720</a>, we added test simulations for these and solved several issues that were flagged by them.</p>
<h3><a id="_improving_zen_discovery_status_done_v1_4_0_beta1"></a>Improving Zen Discovery (STATUS: DONE, v1.4.0.Beta1)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>Recovery from failure is a complicated process, especially in an asynchronous distributed system like Elasticsearch. With several processes happening in parallel, it is important to ensure that recovery proceeds swiftly and safely. While fixing the <a href="https://github.com/elastic/elasticsearch/issues/2488" class="ulink" target="_top">split-brain issue</a> we have been hunting down corner cases that were not handled optimally, adding tests to demonstrate the issues, and working on fixes:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Faster &amp; better detection of master &amp; node failures, including not trying to reconnect upon disconnect, fail on disconnect error on ping, verify cluster names in pings. Previously, Elasticsearch had to wait a bit for the node to complete the process required to join the cluster. Recent changes guarantee that a node has fully joined the cluster before we start the fault detection process. Therefore we can do an immediate check causing faster detection of errors and validation of cluster state after a minimum master node breach. <a href="https://github.com/elastic/elasticsearch/issues/6706" class="ulink" target="_top">#6706</a>, <a href="https://github.com/elastic/elasticsearch/issues/7399" class="ulink" target="_top">#7399</a> (STATUS: DONE, v1.4.0.Beta1)
</li>
<li class="listitem">
Broaden Unicast pinging when master fails: When a node loses it’s current master it will start pinging to find a new one. Previously, when using unicast based pinging, the node would ping a set of predefined nodes asking them whether the master had really disappeared or whether there was a network hiccup. Now, we ping all nodes in the cluster to increase coverage. In the case that all unicast hosts are disconnected from the current master during a network failure, this improvement is essential to allow the cluster to reform once the partition is healed. <a href="https://github.com/elastic/elasticsearch/issues/7336" class="ulink" target="_top">#7336</a> (STATUS: DONE, v1.4.0.Beta1)
</li>
<li class="listitem">
After joining a cluster, validate that the join was successful and that the master has been set in the local cluster state. <a href="https://github.com/elastic/elasticsearch/issues/6969" class="ulink" target="_top">#6969</a>. (STATUS: DONE, v1.4.0.Beta1)
</li>
<li class="listitem">
Write additional tests that use the test infrastructure to verify proper behavior during network disconnections and garbage collections. <a href="https://github.com/elastic/elasticsearch/issues/7082" class="ulink" target="_top">#7082</a> (STATUS: DONE, v1.4.0.Beta1)
</li>
</ul>
</div>
<h3><a id="_lucene_checksums_phase_2_statusdone_v1_4_0_beta1"></a>Lucene checksums phase 2 (STATUS:DONE, v1.4.0.Beta1)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>When Lucene opens a segment for reading, it validates the checksum on the smaller segment files&#8201;&#8212;&#8201;those which it reads entirely into memory&#8201;&#8212;&#8201;but not the large files like term frequencies and positions, as this would be very expensive. During merges, term vectors and stored fields are validated, as long the segments being merged come from the same version of Lucene. Checksumming for term vectors and stored fields is important because merging consists of performing optimized byte copies. Term frequencies, term positions, payloads, doc values, and norms are currently not checked during merges, although Lucene provides the option to do so.  These files are less prone to silent corruption as they are actively decoded during merge, and so are more likely to throw exceptions if there is any corruption.</p>
<p>The following changes have been made:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a href="https://github.com/elastic/elasticsearch/issues/7360" class="ulink" target="_top">#7360</a> validates checksums on all segment files during merges. (STATUS: DONE, fixed in v1.4.0.Beta1)
</li>
<li class="listitem">
<a href="https://issues.apache.org/jira/browse/LUCENE-5842" class="ulink" target="_top">LUCENE-5842</a> validates the structure of the checksum footer of the postings lists, doc values, stored fields and term vectors when opening a new segment, to ensure that these files have not been truncated. (STATUS: DONE, Fixed in Lucene 4.10 and v1.4.0.Beta1)
</li>
<li class="listitem">
<a href="https://github.com/elastic/elasticsearch/issues/8407" class="ulink" target="_top">#8407</a> validates Lucene checksums for legacy files. (STATUS: DONE; Fixed in v1.3.6)
</li>
</ul>
</div>
<h3><a id="_dont_allow_unsupported_codecs_status_done_v1_4_0_beta1"></a>Don&#8217;t allow unsupported codecs (STATUS: DONE, v1.4.0.Beta1)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>Lucene 4 added a number of alternative codecs for experimentation purposes, and Elasticsearch exposed the ability to change codecs.  Since then, Lucene has settled on the best choice of codec and provides backwards compatibility only for the default codec.  <a href="https://github.com/elastic/elasticsearch/issues/7566" class="ulink" target="_top">#7566</a> removes the ability to set alternate codecs.</p>
<h3><a id="_use_checksums_to_identify_entire_segments_status_done_v1_4_0_beta1"></a>Use checksums to identify entire segments (STATUS: DONE, v1.4.0.Beta1)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>A hash collision makes it possible for two different files to have the same length and the same checksum. Instead, a segment&#8217;s identity should rely on checksums from all of the files in a single segment, which greatly reduces the chance of a collision. This change has been merged (<a href="https://github.com/elastic/elasticsearch/issues/7351" class="ulink" target="_top">#7351</a>).</p>
<h3><a id="_fix_split_brain_can_occur_even_with_minimum_master_nodes_status_done_v1_4_0_beta1"></a>Fix ''Split Brain can occur even with minimum_master_nodes'' (STATUS: DONE, v1.4.0.Beta1)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>Even when minimum master nodes is set, split brain can still occur under certain conditions, e.g. disconnection between master eligible nodes, which can lead to data loss. The scenario is described in detail in <a href="https://github.com/elastic/elasticsearch/issues/2488" class="ulink" target="_top">issue 2488</a>:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Introduce a new testing infrastructure to simulate different types of node disconnections, including loss of network connection, lost messages, message delays, etc. See <a href="https://github.com/elastic/elasticsearch/issues/5631" class="ulink" target="_top">MockTransportService</a> support and <a href="https://github.com/elastic/elasticsearch/issues/6505" class="ulink" target="_top">service disruption</a> for more details. (STATUS: DONE, v1.4.0.Beta1).
</li>
<li class="listitem">
Added tests that simulated the bug described in issue 2488. You can take a look at the <a href="https://github.com/elastic/elasticsearch/commit/7bf3ffe73c44f1208d1f7a78b0629eb48836e726" class="ulink" target="_top">original commit</a> of a reproduction on master. (STATUS: DONE, v1.2.0)
</li>
<li class="listitem">
The bug described in <a href="https://github.com/elastic/elasticsearch/issues/2488" class="ulink" target="_top">issue 2488</a> is caused by an issue in our zen discovery gossip protocol. This specific issue has been fixed, and work has been done to make the algorithm more resilient. (STATUS: DONE, v1.4.0.Beta1)
</li>
</ul>
</div>
<h3><a id="_translog_entry_checksum_status_done_v1_4_0_beta1"></a>Translog Entry Checksum (STATUS: DONE, v1.4.0.Beta1)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>Each translog entry in Elasticsearch should have its own checksum, and potentially additional information, so that we can properly detect corrupted translog entries and act accordingly. You can find more detail in issue <a href="https://github.com/elastic/elasticsearch/issues/6554" class="ulink" target="_top">#6554</a>.</p>
<p>To start, we will begin by adding checksums to the translog to detect corrupt entries. Once this work has been completed, we will add translog entry markers so that corrupt entries can be skipped in the translog if/when desired.</p>
<h3><a id="_request_level_memory_circuit_breaker_status_done_v1_4_0_beta1"></a>Request-Level Memory Circuit Breaker (STATUS: DONE, v1.4.0.Beta1)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>We are in the process of introducing multiple circuit breakers in Elasticsearch, which can “borrow” space from each other in the event that one runs out of memory. This architecture will allow limits for certain parts of memory, but still allow flexibility in the event that another reserve like field data is not being used. This change includes adding a breaker for the BigArrays internal object used for some aggregations. See issue <a href="https://github.com/elastic/elasticsearch/issues/6739" class="ulink" target="_top">#6739</a> for more details.</p>
<h3><a id="_doc_values_status_done_v1_4_0_beta1"></a>Doc Values (STATUS: DONE, v1.4.0.Beta1)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>Fielddata is one of the largest consumers of heap memory, and thus one of the primary reasons for running out of memory and causing node instability. Elasticsearch has had the “doc values” option for a while, which allows you to build these structures at index time so that they live on disk instead of in memory. Up until recently, doc values were significantly slower than in-memory fielddata.</p>
<p>By benchmarking and profiling both Lucene and Elasticsearch, we identified the bottlenecks and have made a series of improvements to improve the performance of doc values. They are now almost as fast as the in-memory option.</p>
<p>See <a href="https://github.com/elastic/elasticsearch/issues/6967" class="ulink" target="_top">#6967</a>, <a href="https://github.com/elastic/elasticsearch/issues/6908" class="ulink" target="_top">#6908</a>, <a href="https://github.com/elastic/elasticsearch/issues/4548" class="ulink" target="_top">#4548</a>, <a href="https://github.com/elastic/elasticsearch/issues/3829" class="ulink" target="_top">#3829</a>, <a href="https://github.com/elastic/elasticsearch/issues/4518" class="ulink" target="_top">#4518</a>, <a href="https://github.com/elastic/elasticsearch/issues/5669" class="ulink" target="_top">#5669</a>, <a href="https://issues.apache.org/jira/browse/LUCENE-5748" class="ulink" target="_top">LUCENE-5748</a>, <a href="https://issues.apache.org/jira/browse/LUCENE-5703" class="ulink" target="_top">LUCENE-5703</a>, <a href="https://issues.apache.org/jira/browse/LUCENE-5750" class="ulink" target="_top">LUCENE-5750</a>, <a href="https://issues.apache.org/jira/browse/LUCENE-5721" class="ulink" target="_top">LUCENE-5721</a>, <a href="https://issues.apache.org/jira/browse/LUCENE-5799" class="ulink" target="_top">LUCENE-5799</a>.</p>
<h3><a id="_index_corruption_when_upgrading_lucene_3_x_indices_status_done_v1_4_0_beta1"></a>Index corruption when upgrading Lucene 3.x indices (STATUS: DONE, v1.4.0.Beta1)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>Upgrading indices create with Lucene 3.x (Elasticsearch v0.20 and before) to Lucene 4.7 - 4.9 (Elasticsearch v1.1.0 to v1.3.x), could result in index corruption. <a href="https://issues.apache.org/jira/browse/LUCENE-5907" class="ulink" target="_top">LUCENE-5907</a> fixes this issue in Lucene 4.10.</p>
<h3><a id="_improve_error_handling_when_deleting_files_status_done_v1_4_0_beta1"></a>Improve error handling when deleting files (STATUS: DONE, v1.4.0.Beta1)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>Lucene uses reference counting to prevent files that are still in use from being deleted.  Lucene testing discovered a bug (<a href="https://issues.apache.org/jira/browse/LUCENE-5919" class="ulink" target="_top">LUCENE-5919</a>) when decrementing the ref count on a batch of files. If deleting some of the files resulted in an exception (e.g. due to interference from a virus scanner), the files that had their ref counts decremented successfully could later have their ref counts deleted again, incorrectly, resulting in files being physically deleted before their time. This is fixed in Lucene 4.10.</p>
<h3><a id="_using_lucene_checksums_to_verify_shards_during_snapshotrestore_statusdone_v1_3_3"></a>Using Lucene Checksums to verify shards during snapshot/restore (STATUS:DONE, v1.3.3)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>The snapshot process should verify checksums for each file that is being snapshotted to make sure that created snapshot doesn’t contain corrupted files. If a corrupted file is detected, the snapshot should fail with an error. In order to implement this feature we need to have correct and verifiable checksums stored with segment files, which is only possible for files that were written by the officially supported append-only codecs. See <a href="https://github.com/elastic/elasticsearch/issues/7159" class="ulink" target="_top">#7159</a>.</p>
<h3><a id="_rare_compression_corruption_during_shard_recovery_status_done_v1_3_2"></a>Rare compression corruption during shard recovery (STATUS: DONE, v1.3.2)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>During recovery, the primary shard is copied over the network to become a new replica shard. In rare cases, it was possible for a hash collision to trigger a bug in the compression library that is used to produce corruption in the replica shard. This bug was exposed by the change to validate checksums during recovery. We tracked down the bug in the in compression library and submitted a patch, which was accepted and merged by the upstream project. See <a href="https://github.com/elastic/elasticsearch/issues/7210" class="ulink" target="_top">#7210</a>.</p>
<h3><a id="_safer_recovery_of_replica_shards_status_done_v1_3_0"></a>Safer recovery of replica shards (STATUS: DONE, v1.3.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>If a primary shard fails or is closed while a replica is using it for recovery, we need to ensure that the replica is properly failed as well, and allow recovery to start from the new primary. Also check that an active copy of a shard is available on another node before physically removing an inactive shard from disk. <a href="https://github.com/elastic/elasticsearch/issues/6825" class="ulink" target="_top">#6825</a>, <a href="https://github.com/elastic/elasticsearch/issues/6645" class="ulink" target="_top">#6645</a>, <a href="https://github.com/elastic/elasticsearch/issues/6995" class="ulink" target="_top">#6995</a>.</p>
<h3><a id="_using_lucene_checksums_to_verify_shards_during_recovery_status_done_v1_3_0"></a>Using Lucene Checksums to verify shards during recovery (STATUS: DONE, v1.3.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>Elasticsearch can use Lucene checksums to validate files while <a href="https://github.com/elastic/elasticsearch/issues/6776" class="ulink" target="_top">recovering a replica shard from a primary</a>.</p>
<p>This issue exposed a bug in Elasticsearch’s handling of primary shard failure when having more than 2 replicas, causing the second replica to not be properly unassigned if it is in the middle of recovery. It was fixed with the merge of issue <a href="https://github.com/elastic/elasticsearch/issues/6808" class="ulink" target="_top">#6808</a>.</p>
<p>In order to verify the checksumming mechanism, we added functionality to our testing infrastructure that can corrupt an arbitrary index file and at any point, such as while it’s traveling over the wire or residing on disk. The tests utilizing this feature expect full or partial recovery from the failure while neither losing data nor spreading the corruption.</p>
<h3><a id="_detect_file_corruption_status_done_v1_3_0"></a>Detect File Corruption (STATUS: DONE, v1.3.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>When a corrupted index can be detected during merging or refresh, Elasticsearch will fail the shard if a checksum failure is detected. You can read the full details in pull request <a href="https://github.com/elastic/elasticsearch/issues/6776" class="ulink" target="_top">#6776</a>.</p>
<h3><a id="_network_disconnect_events_could_be_lost_causing_a_zombie_node_to_stay_in_the_cluster_state_status_done_v1_3_0"></a>Network disconnect events could be lost, causing a zombie node to stay in the cluster state (STATUS: DONE, v1.3.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>Previously, there was a very short window in which we could lose a node disconnect event. To prevent this from occurring, we added extra handling of connection errors to our nodes &amp; master fault detection pinging to make sure the node disconnect event is detected. See issue <a href="https://github.com/elastic/elasticsearch/issues/6686" class="ulink" target="_top">#6686</a>.</p>
<h3><a id="_other_fixes_to_lucene_to_address_resiliency_status_done_v1_3_0"></a>Other fixes to Lucene to address resiliency (STATUS: DONE, v1.3.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
NativeLock is released if Lock is closed after failing on obtain <a href="https://issues.apache.org/jira/browse/LUCENE-5738" class="ulink" target="_top">LUCENE-5738</a>.
</li>
<li class="listitem">
NRT Reader close can wipe an index it doesn’t own. <a href="https://issues.apache.org/jira/browse/LUCENE-5574" class="ulink" target="_top">LUCENE-5574</a>
</li>
<li class="listitem">
FSDirectory’s fsync() is lenient, now throws exceptions when errors occur <a href="https://issues.apache.org/jira/browse/LUCENE-5570" class="ulink" target="_top">LUCENE-5570</a>
</li>
<li class="listitem">
fsync() directory when committing <a href="https://issues.apache.org/jira/browse/LUCENE-5588" class="ulink" target="_top">LUCENE-5588</a>
</li>
</ul>
</div>
<h3><a id="_backwards_compatibility_testings_status_done_v1_3_0"></a>Backwards Compatibility Testings (STATUS: DONE, v1.3.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>Since founding Elasticsearch Inc, we grew our test base from ~1k tests to about 4k in just about over a year. We invested massively into our testing infrastructure, running our tests continuously on different operating systems, bare metal hardware and cloud environments, all while randomizing JVMs and their settings.</p>
<p>Yet, backwards compatibility testing was a very manual thing until we released a pretty <a href="https://github.com/elastic/elasticsearch/issues/6393" class="ulink" target="_top">insane bug</a> with Elasticsearch 1.2. We tried to fix places where the absolute value of a number was negative (a documented behavior of Math.abs(int) in Java) and missed that the fix for this also changed the result of our routing function. No matter how much randomization we applied to the tests, we didn’t catch this particular failure. We always had backwards compatibility tests on our list of things to do, but didn’t have them in place back then.</p>
<p>We recently tweaked our testing infrastructure to be able to run tests against a hybrid cluster composed of a released version of Elasticsearch and our current stable branch. This test pattern allowed us to mimic typical upgrade scenarios like rolling upgrades, index backwards compatibility and recovering from old to new nodes.</p>
<p>Now, even the simplest test that relies on routing fails against 1.2.0, which is exactly we were aiming for. The test would not have caught the aforementioned <a href="https://github.com/elastic/elasticsearch/issues/6393" class="ulink" target="_top">routing bug</a> before releasing 1.2.0, but it immediately saved us from <a href="https://github.com/elastic/elasticsearch/issues/6660" class="ulink" target="_top">another problem</a> in the stable branch.</p>
<p>The work on our testing infrastructure is more than just issue prevention, it allows us to develop and test upgrade paths, introduce new features and evolve indexing over time. It isn’t enough to introduce more resilient implementations, we also have to ensure that users take advantage of them when they upgrade.</p>
<p>You can read more about backwards compatibility tests in issue <a href="https://github.com/elastic/elasticsearch/issues/6497" class="ulink" target="_top">#6497</a>.</p>
<h3><a id="_full_translog_writes_on_all_platforms_status_done_v1_2_2_and_v1_3_0"></a>Full Translog Writes on all Platforms (STATUS: DONE, v1.2.2 and v1.3.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>We have recently received bug reports of transaction log corruption that can occur when indexing very large documents (in the area of 300 KB). Although some Linux users reported this behavior, it appears the problem occurs more frequently when running Windows. We traced the source of the problem to the fact that when serializing documents to the transaction log, the Operating System can actually write only part of the document before returning from the write call. We can now detect this situation and make sure that the entire document is properly written. You can read the full details in pull request <a href="https://github.com/elastic/elasticsearch/issues/6576" class="ulink" target="_top">#6576</a>.</p>
<h3><a id="_lucene_checksums_status_done_v1_2_0"></a>Lucene Checksums (STATUS: DONE, v1.2.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>Before Apache Lucene version 4.8, checksums were not computed on generated index files. The result was that it was difficult to identify when or if a Lucene index got corrupted, whether by hardware failure, JVM bug or for an entirely different reason.</p>
<p>For an idea of the checksum efforts in progress in Apache Lucene, see issues <a href="https://issues.apache.org/jira/browse/LUCENE-2446" class="ulink" target="_top">LUCENE-2446</a>, <a href="https://issues.apache.org/jira/browse/LUCENE-5580" class="ulink" target="_top">LUCENE-5580</a> and <a href="https://issues.apache.org/jira/browse/LUCENE-5602" class="ulink" target="_top">LUCENE-5602</a>. The gist is that Lucene 4.8+ now computes full checksums on all index files and it verifies them when opening metadata or other smaller files as well as other files during merges.</p>
<h3><a id="_detect_errors_faster_by_locally_failing_a_shard_upon_an_indexing_error_status_done_v1_2_0"></a>Detect errors faster by locally failing a shard upon an indexing error (STATUS: DONE, v1.2.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>Previously, Elasticsearch notified the master of the shard failure and waited for the master to close the local copy of the shard, thus assigning it to other nodes. This architecture caused delays in failure detection, potentially causing unneeded failures of other incoming requests. In rare cases, such as concurrency racing conditions or certain network partitions configurations, we could lose these failure notifications. We solved this issue by locally failing shards upon indexing errors. See issue <a href="https://github.com/elastic/elasticsearch/issues/5847" class="ulink" target="_top">#5847</a>.</p>
<h3><a id="_snapshotrestore_api_status_done_v1_0_0"></a>Snapshot/Restore API (STATUS: DONE, v1.0.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>In Elasticsearch version 1.0, we significantly improved the backup process by introducing the Snapshot/Restore API. While it was always possible to make backups of Elasticsearch, the Snapshot/Restore API made the backup process much easier.</p>
<p>The backup process is incremental, making it very efficient since only files changed since the last backup are copied. Even with this efficiency introduced, each snapshot contains a full picture of the cluster at the moment when backup started. The restore API allows speedy recovery of a full cluster as well as selected indices.</p>
<p>Since that first release in version 1.0, the API has continued to evolve. In version 1.1.0, we added a new snapshot status API that allows users to monitor the snapshot process. In 1.3.0 we added the ability to <a href="https://github.com/elastic/elasticsearch/issues/6457" class="ulink" target="_top">restore indices without their aliases</a> and in 1.4 we are planning to add the ability to <a href="https://github.com/elastic/elasticsearch/issues/6368" class="ulink" target="_top">restore partial snapshots</a>.</p>
<p>The Snapshot/Restore API supports a number of different repository types for storing backups. Currently, it’s possible to make backups to a shared file system, Amazon S3, HDFS, and Azure storage. We are continuing to work on adding other types of storage systems, as well as improving the robustness of the snapshot/restore process.</p>
<h3><a id="_circuit_breaker_fielddata_status_done_v1_0_0"></a>Circuit Breaker: Fielddata (STATUS: DONE, v1.0.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>Currently, the circuit breaker protects against loading too much field data by estimating how much memory the field data will take to load, then aborting the request if the memory requirements are too high. This feature was added in Elasticsearch version 1.0.0.</p>
<h3><a id="_use_of_paginated_data_structures_to_ease_garbage_collection_status_done_v1_0_0_v1_2_0"></a>Use of Paginated Data Structures to Ease Garbage Collection (STATUS: DONE, v1.0.0 &amp; v1.2.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>Elasticsearch has moved from an object-based cache to a page-based cache recycler as described in issue <a href="https://github.com/elastic/elasticsearch/issues/4557" class="ulink" target="_top">#4557</a>. This change makes garbage collection easier by limiting fragmentation, since all pages have the same size and are recycled. It also allows managing the size of the cache not based on the number of objects it contains, but on the memory that it uses.</p>
<p>These pages are used for two main purposes: implementing higher level data structures such as hash tables that are used internally by aggregations to e.g. map terms to counts, as well as reusing memory in the translog/transport layer as detailed in issue <a href="https://github.com/elastic/elasticsearch/issues/5691" class="ulink" target="_top">#5691</a>.</p>
<h3><a id="_dedicated_master_nodes_resiliency_status_done_v1_0_0"></a>Dedicated Master Nodes Resiliency (STATUS: DONE, v1.0.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>In order to run a more resilient cluster, we recommend running dedicated master nodes to ensure master nodes are not affected by resources consumed by data nodes. We also have made master nodes more resilient to heavy resource usage, mainly associated with large clusters / cluster states.</p>
<p>These changes include:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Improve the balancing algorithm to execute faster across large clusters / many indices. (See issue <a href="https://github.com/elastic/elasticsearch/issues/4458" class="ulink" target="_top">#4458</a> and <a href="https://github.com/elastic/elasticsearch/issues/4459" class="ulink" target="_top">#4459</a>)
</li>
<li class="listitem">
Improve cluster state publishing to not create an additional network buffer per node. (More in <a href="https://github.com/elastic/elasticsearch/commit/a9e259d438c3cb1d3bef757db2d2a91cf85be609" class="ulink" target="_top">this commit</a>.)
</li>
<li class="listitem">
Improve master handling of large scale mapping updates from data nodes by batching them into a single cluster event. (See issue <a href="https://github.com/elastic/elasticsearch/issues/4373" class="ulink" target="_top">#4373</a>.)
</li>
<li class="listitem">
Add an ack mechanism where next phase cluster updates are processed only when nodes acknowledged they received the previous cluster state. (See issues <a href="https://github.com/elastic/elasticsearch/issues/3736" class="ulink" target="_top">#3736</a>, <a href="https://github.com/elastic/elasticsearch/issues/3786" class="ulink" target="_top">#3786</a>, <a href="https://github.com/elastic/elasticsearch/issues/4114" class="ulink" target="_top">#4114</a>, <a href="https://github.com/elastic/elasticsearch/issues/4169" class="ulink" target="_top">#4169</a>, <a href="https://github.com/elastic/elasticsearch/issues/4228" class="ulink" target="_top">#4228</a> and <a href="https://github.com/elastic/elasticsearch/issues/4421" class="ulink" target="_top">#4421</a>, which also include enhancements to the ack mechanism implementation.)
</li>
</ul>
</div>
<h3><a id="_multi_data_paths_may_falsely_report_corrupt_index_status_done_v1_0_0"></a>Multi Data Paths May Falsely Report Corrupt Index (STATUS: DONE, v1.0.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>When using multiple data paths, an index could be falsely reported as corrupted. This has been fixed with pull request <a href="https://github.com/elastic/elasticsearch/issues/4674" class="ulink" target="_top">#4674</a>.</p>
<h3><a id="_randomized_testing_status_done_v1_0_0"></a>Randomized Testing (STATUS: DONE, v1.0.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>In order to best validate for resiliency in Elasticsearch, we rewrote the Elasticsearch test infrastructure to introduce the concept of <a href="https://github.com/randomizedtesting/randomizedtesting" class="ulink" target="_top">randomized testing</a>. Randomized testing allows us to easily enhance the Elasticsearch testing infrastructure with predictably irrational conditions, making the resulting code base more resilient.</p>
<p>Each of our integration tests runs against a cluster with a random number of nodes, and indices have a random number of shards and replicas. Merge settings change for every run, indexing is done in serial or async fashion or even wrapped in a bulk operation and thread pool sizes vary to ensure that we don’t produce a deadlock no matter what happens. The list of places we use this randomization infrastructure is long, and growing every day, and has saved us headaches several times before we shipped a particular feature.</p>
<p>At Elasticsearch, we live the philosophy that we can miss a bug once, but never a second time. We make our tests more evil as you go, introducing randomness in all the areas where we discovered bugs. We figure if our tests don’t fail, we are not trying hard enough! If you are interested in how we have evolved our test infrastructure over time check out <a href="https://github.com/elastic/elasticsearch/issues?q=label%3Atest" class="ulink" target="_top">issues labeled with ``test'' on GitHub</a>.</p>
<h3><a id="_lucene_loses_data_on_file_descriptors_failure_status_done_v0_90_0"></a>Lucene Loses Data On File Descriptors Failure (STATUS: DONE, v0.90.0)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/elasticsearch/edit/master/docs/resiliency/index.asciidoc">edit</a></h3>
<p>When a process runs out of file descriptors, Lucene can causes an index to be completely deleted. This issue was fixed in Lucene (<a href="https://issues.apache.org/jira/browse/LUCENE-4870" class="ulink" target="_top">version 4.2.1</a>) and fixed in an early version of Elasticsearch. See issue <a href="https://github.com/elastic/elasticsearch/issues/2812" class="ulink" target="_top">#2812</a>.</p>
</div>

</div>
</div></body>
</html>
