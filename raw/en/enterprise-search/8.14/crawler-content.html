<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Optimizing web content for the web crawler | Enterprise Search documentation [8.14] | Elastic</title>
<meta class="elastic" name="content" content="Optimizing web content for the web crawler | Enterprise Search documentation [8.14]">

<link rel="home" href="index.html" title="Enterprise Search documentation [8.14]"/>
<link rel="up" href="crawler.html" title="Elastic web crawler"/>
<link rel="prev" href="crawler-troubleshooting.html" title="Troubleshooting crawls"/>
<link rel="next" href="crawler-custom-values-ingest-pipeline.html" title="Customize crawler field values using an ingest pipeline"/>
<meta class="elastic" name="product_version" content="8.14"/>
<meta class="elastic" name="product_name" content="Enterprise Search"/>
<meta class="elastic" name="website_area" content="documentation"/>
<meta name="DC.type" content="Learn/Docs/Enterprise Search/Guide/8.14"/>
<meta name="DC.subject" content="Enterprise Search"/>
<meta name="DC.identifier" content="8.14"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body>
<div class="page_header">
<strong>IMPORTANT</strong>: No additional bug fixes or documentation updates
will be released for this version. For the latest information, see the
<a href="../current/index.html">current release documentation</a>.
</div>
<div class="navheader">
<span class="prev">
<a href="crawler-troubleshooting.html">« Troubleshooting crawls</a>
</span>
<span class="next">
<a href="crawler-custom-values-ingest-pipeline.html">Customize crawler field values using an ingest pipeline »</a>
</span>
</div>
<div class="book" lang="en">
<div class="titlepage">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link">
<div id="related-products" class="dropdown">
<div class="related-products-title"></div>
<div class="dropdown-anchor" tabindex="0">Enterprise Search<span class="dropdown-icon"></span></div>
<div class="dropdown-content">
<ul>
<li class="dropdown-category">Enterprise Search guides</li>
<ul>
<li><a href="/guide/en/enterprise-search/current/index.html" target="_blank">Enterprise Search</a></li>
<li><a href="/guide/en/app-search/current/index.html" target="_blank">App Search</a></li>
<li><a href="/guide/en/workplace-search/current/index.html" target="_blank">Workplace Search</a></li>
</ul>
<li class="dropdown-category">Programming language clients</li>
<ul>
<li><a href="https://www.elastic.co/guide/en/enterprise-search-clients/enterprise-search-node/current/index.html" target="_blank">Node.js client</a></li>
<li><a href="https://www.elastic.co/guide/en/enterprise-search-clients/php/current/index.html" target="_blank">PHP client</a></li>
<li><a href="https://www.elastic.co/guide/en/enterprise-search-clients/python/current/index.html" target="_blank">Python client</a></li>
<li><a href="https://www.elastic.co/guide/en/enterprise-search-clients/ruby/current/index.html" target="_blank">Ruby client</a></li>
</ul>
</ul>
</div>
</div>
</span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="crawler.html">Elastic web crawler</a></span>
</div>
<div>
<div><h1 class="title"><a id="id-1"></a>Optimizing web content for the web crawler</h1><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.14/enterprise-search-docs/crawler-content.asciidoc">edit</a></div>
</div>
<!--EXTRA-->
</div>
<div id="content">
<div class="chapter">
<div class="titlepage"><div><div>
<h2 class="title"><a id="crawler-content"></a>Optimizing web content for the web crawler<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.14/enterprise-search-docs/crawler-content.asciidoc">edit</a></h2>
</div></div></div>

<div class="tip admon">
<div class="icon"></div>
<div class="admon_content">
<p>This documentation explains how to optimize web content for the crawler.
To do this, you must be able to access and modify HTML, <code class="literal">robots.txt</code> files, or sitemap source files.</p>
<p>If you can&#8217;t access these files, <a class="xref" href="crawler-managing.html" title="Managing crawls in Kibana">manage crawls in Kibana</a>.</p>
</div>
</div>
<p>You can optimize your web content source files for the web crawler.</p>
<p>These techniques are similar to search engine optimization (SEO) techniques used for other web crawlers and robots.
For example, you can embed instructions for the web crawler within your HTML content.
You can also prevent the crawler from following links or indexing any content for certain webpages.
Use these tools to manage webpage <span class="strong strong"><strong>discovery</strong></span> and content <span class="strong strong"><strong>extraction</strong></span>.</p>
<p><span class="strong strong"><strong>Discovery</strong></span> concerns which web pages and files from crawled domains get indexed:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="crawler-content.html#crawler-content-canonical-url-link-tag" title="Canonical URL link tags">Canonical URL link tags</a>
</li>
<li class="listitem">
<a class="xref" href="crawler-content.html#crawler-content-robots-meta-tags" title="Robots meta tags">Robots meta tags</a>
</li>
<li class="listitem">
<a class="xref" href="crawler-content.html#crawler-content-nofollow-link" title="Nofollow links">Nofollow links</a>
</li>
<li class="listitem">
<a class="xref" href="crawler-content.html#crawler-content-manage-robots-txt-files" title="robots.txt files"><code class="literal">robots.txt</code> files</a>
</li>
<li class="listitem">
<a class="xref" href="crawler-content.html#crawler-content-sitemap" title="Sitemaps">Sitemaps</a>
</li>
</ul>
</div>
<p><span class="strong strong"><strong>Extraction</strong></span> concerns <em>how</em> content is indexed and mapped to fields in Elasticsearch documents:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="crawler-content.html#crawler-content-inclusion-and-exclusion" title="Data attributes for inclusion and exclusion">Data attributes for inclusion and exclusion</a>
</li>
<li class="listitem">
<a class="xref" href="crawler-content.html#crawler-content-meta-tags-content-extraction" title="Meta tags and data attributes to extract custom fields">Meta tags and data attributes to extract custom fields</a>
</li>
</ul>
</div>
<h3><a id="crawler-content-html"></a>HTML elements and attributes<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.14/enterprise-search-docs/crawler-content.asciidoc">edit</a></h3>
<p>The following sections describe crawler instructions you can embed within HTML elements and attributes.</p>
<h4><a id="crawler-content-canonical-url-link-tag"></a>Canonical URL link tags<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.14/enterprise-search-docs/crawler-content.asciidoc">edit</a></h4>
<p>A <em>canonical URL link tag</em> is an HTML element you can embed within pages that duplicate the content of other pages.
See <a class="xref" href="crawler-managing.html#crawler-managing-duplicate-documents" title="Duplicate document handling">Duplicate document handling</a> for detailed information about managing duplicate content using the Kibana UI.
The canonical URL link tag specifies the canonical URL for that content.</p>
<p>The canonical URL is stored on the document in the <code class="literal">url</code> field, while the <code class="literal">additional_urls</code> field contains all other URLs where the crawler discovered the same content.
If your site contains pages that duplicate the content of other pages, use canonical URL link tags to explicitly manage which URL is stored in the <code class="literal">url</code> field of the indexed document.</p>
<p>Template:</p>
<div class="pre_wrapper lang-html">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-html">&lt;link rel="canonical" href="{CANONICAL_URL}"&gt;</pre>
</div>
<p>Example:</p>
<div class="pre_wrapper lang-html">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-html">&lt;link rel="canonical" href="https://example.com/categories/dresses/starlet-red-medium"&gt;</pre>
</div>
<h4><a id="crawler-content-robots-meta-tags"></a>Robots meta tags<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.14/enterprise-search-docs/crawler-content.asciidoc">edit</a></h4>
<p><em>Robots meta tags</em> are HTML elements you can embed within pages to prevent the crawler from following links or indexing content.
These tags are related to crawl rules.
See <a class="xref" href="crawler-managing.html#crawler-managing-crawl-rules" title="Crawl rules">Crawl rules</a> for detailed information about crawl rules.</p>
<p>Template:</p>
<div class="pre_wrapper lang-html">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-html">&lt;meta name="robots" content="{DIRECTIVES}"&gt;</pre>
</div>
<p>Supported directives:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">noindex</code>
</span>
</dt>
<dd>
The web crawler will not index the page.
If you want to index some, but not all, content on a page, see <a class="xref" href="crawler-content.html#crawler-content-inclusion-and-exclusion" title="Data attributes for inclusion and exclusion">Data attributes for inclusion and exclusion</a>.
</dd>
<dt>
<span class="term">
<code class="literal">nofollow</code>
</span>
</dt>
<dd>
<p>
The web crawler will not follow links from the page.
The web crawler logs a <code class="literal">url_discover_denied</code> event for each link.
</p>
<p>The directive does not prevent the web crawler from indexing the page.</p>
</dd>
</dl>
</div>
<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<p>Currently, content deletion (purge or process crawl) does not honor the <code class="literal">noindex</code> and <code class="literal">nofollow</code> directives.
Crawls will not remove previously indexed pages that now have <code class="literal">noindex</code> and <code class="literal">nofollow</code> directives from the engine at the end of each crawl.
To manually remove obsolete content, create the appropriate crawl rules to exclude the pages and run a process crawl.</p>
</div>
</div>
<p>Examples:</p>
<div class="pre_wrapper lang-html">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-html">&lt;meta name="robots" content="noindex"&gt;
&lt;meta name="robots" content="nofollow"&gt;
&lt;meta name="robots" content="noindex, nofollow"&gt;</pre>
</div>
<h4><a id="crawler-content-inclusion-and-exclusion"></a>Data attributes for inclusion and exclusion<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.14/enterprise-search-docs/crawler-content.asciidoc">edit</a></h4>
<p>Inject HTML <code class="literal">data</code> attributes into your web pages to instruct the web crawler to include or exclude particular sections from extracted content. For example, use this feature to exclude navigation and footer content when crawling, or to exclude sections of content only intended for screen readers.</p>
<p>These attributes work as follows:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
For all pages that contain HTML tags with a <code class="literal">data-elastic-include</code> attribute, the crawler will only index content within those tags.
</li>
<li class="listitem">
For all pages that contain HTML tags with a <code class="literal">data-elastic-exclude</code> attribute, the crawler will skip those tags from content extraction. You can nest <code class="literal">data-elastic-include</code> and <code class="literal">data-elastic-exclude</code> tags, too.
</li>
<li class="listitem">
The web crawler will still crawl any links that appear inside excluded sections as long as the configured crawl rules allow them.
</li>
</ul>
</div>
<h4><a id="crawler-content-inclusion-and-exclusion-examples"></a>Examples<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.14/enterprise-search-docs/crawler-content.asciidoc">edit</a></h4>
<p>A simple content exclusion rule example:</p>
<div class="pre_wrapper lang-html">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-html">&lt;body&gt;
  &lt;p&gt;This is your page content, which will be indexed by the web crawler.
  &lt;div data-elastic-exclude&gt;Content in this div will be excluded from the search index&lt;/div&gt;
&lt;/body&gt;</pre>
</div>
<p>In this more complex example with nested exclusion and inclusion rules, the web crawler will only extract "test1 test3 test5 test7" from the page.</p>
<div class="pre_wrapper lang-html">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-html">&lt;body&gt;
  test1
  &lt;div data-elastic-exclude&gt;
    test2
    &lt;p data-elastic-include&gt;
      test3
      &lt;span data-elastic-exclude&gt;
        test4
        &lt;span data-elastic-include&gt;test5&lt;/span&gt;
      &lt;/span&gt;
    &lt;/p&gt;
    test6
  &lt;/div&gt;
  test7
&lt;/body&gt;</pre>
</div>
<h4><a id="crawler-content-meta-tags-content-extraction"></a>Meta tags and data attributes to extract custom fields<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.14/enterprise-search-docs/crawler-content.asciidoc">edit</a></h4>
<p>The web crawler extracts a predefined, set of fields (url, body content, etc) from each page it visits.
View your documents to see the full schema.
With <em>meta tags and data attributes</em> you can extract custom fields from your HTML pages.</p>
<p>Template:</p>
<div class="pre_wrapper lang-html">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-html">&lt;head&gt;
  &lt;meta class="elastic" name="{FIELD_NAME}" content="{FIELD_VALUE}"&gt;
&lt;/head&gt;
&lt;body&gt;
  &lt;div data-elastic-name="{FIELD_NAME}"&gt;{FIELD_VALUE}&lt;/div&gt;
&lt;/body&gt;</pre>
</div>
<p>The crawled document for this example</p>
<div class="pre_wrapper lang-html">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-html">&lt;head&gt;
  &lt;meta class="elastic" name="product_price" content="99.99"&gt;
&lt;/head&gt;
&lt;body&gt;
  &lt;h1 data-elastic-name="product_name"&gt;Printer&lt;/h1&gt;
&lt;/body&gt;</pre>
</div>
<p>will include 2 additional fields.</p>
<div class="pre_wrapper lang-json">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-json">{
  "product_price": "99.99",
  "product_name": "Printer"
}</pre>
</div>
<p>You can specify multiple <code class="literal">class="elastic"</code> and <code class="literal">data-elastic-name</code> tags.</p>
<p>Template:</p>
<div class="pre_wrapper lang-html">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-html">&lt;head&gt;
  &lt;meta class="elastic" name="{FIELD_NAME_1}" content="{FIELD_VALUE_1}"&gt;
  &lt;meta class="elastic" name="{FIELD_NAME_2}" content="{FIELD_VALUE_2}"&gt;
&lt;/head&gt;
&lt;body&gt;
  &lt;div data-elastic-name="{FIELD_NAME_1}"&gt;{FIELD_VALUE_1}&lt;/div&gt;
  &lt;div data-elastic-name="{FIELD_NAME_2}"&gt;{FIELD_VALUE_2}&lt;/div&gt;
&lt;/body&gt;</pre>
</div>
<p><code class="literal">{FIELD_NAME}</code> must conform to <span class="strong strong"><strong>field name rules</strong></span>:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Must contain a lowercase letter and may only contain lowercase letters, numbers, and underscores.
</li>
<li class="listitem">
Must not contain whitespace or have a leading underscore.
</li>
<li class="listitem">
Must not contain more than 64 characters.
</li>
<li class="listitem">
<p>Must not be any of the following reserved words:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">id</code>
</li>
<li class="listitem">
<code class="literal">engine_id</code>
</li>
<li class="listitem">
<code class="literal">search_index_id</code>
</li>
<li class="listitem">
<code class="literal">highlight</code>
</li>
<li class="listitem">
<code class="literal">any</code>
</li>
<li class="listitem">
<code class="literal">all</code>
</li>
<li class="listitem">
<code class="literal">none</code>
</li>
<li class="listitem">
<code class="literal">or</code>
</li>
<li class="listitem">
<code class="literal">and</code>
</li>
<li class="listitem">
<code class="literal">not</code>
</li>
<li class="listitem">
<code class="literal">additional_urls</code>
</li>
<li class="listitem">
<code class="literal">body_content</code>
</li>
<li class="listitem">
<code class="literal">domains</code>
</li>
<li class="listitem">
<code class="literal">headings</code>
</li>
<li class="listitem">
<code class="literal">last_crawled_at</code>
</li>
<li class="listitem">
<code class="literal">links</code>
</li>
<li class="listitem">
<code class="literal">meta_description</code>
</li>
<li class="listitem">
<code class="literal">meta_keywords</code>
</li>
<li class="listitem">
<code class="literal">title</code>
</li>
<li class="listitem">
<code class="literal">url</code>
</li>
<li class="listitem">
<code class="literal">url_host</code>
</li>
<li class="listitem">
<code class="literal">url_path</code>
</li>
<li class="listitem">
<code class="literal">url_path_dir1</code>
</li>
<li class="listitem">
<code class="literal">url_path_dir2</code>
</li>
<li class="listitem">
<code class="literal">url_path_dir3</code>
</li>
<li class="listitem">
<code class="literal">url_port</code>
</li>
<li class="listitem">
<code class="literal">url_scheme</code>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="tip admon">
<div class="icon"></div>
<div class="admon_content">
<p>It might not be possible to customize the HTML source code for the webpages you want to crawl.
Use <a class="xref" href="crawler-extraction-rules.html" title="Web crawler content extraction rules"><em>Content extraction rules</em></a> to customize how the crawler extracts content from webpages.</p>
</div>
</div>
<h4><a id="crawler-content-nofollow-link"></a>Nofollow links<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.14/enterprise-search-docs/crawler-content.asciidoc">edit</a></h4>
<p><em>Nofollow links</em> are HTML links that instruct the crawler to not follow the URL.</p>
<p>The web crawler will not follow links that include <code class="literal">rel="nofollow"</code> (i.e. will not add links to the crawl queue).
The web crawler logs a <code class="literal">url_discover_denied</code> event for each link.</p>
<p>The link does not prevent the web crawler from indexing the page in which it appears.</p>
<p>Template:</p>
<div class="pre_wrapper lang-html">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-html">&lt;a rel="nofollow" href="{LINK_URL}"&gt;{LINK_TEXT}&lt;/a&gt;</pre>
</div>
<p>Example:</p>
<div class="pre_wrapper lang-html">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-html">&lt;a rel="nofollow" href="/admin/categories"&gt;Edit this category&lt;/a&gt;</pre>
</div>
<h3><a id="crawler-content-manage-robots-txt-files"></a><code class="literal">robots.txt</code> files<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.14/enterprise-search-docs/crawler-content.asciidoc">edit</a></h3>
<div class="tip admon">
<div class="icon"></div>
<div class="admon_content">
<p>It is impossible to configure the web crawler to ignore or work around a domain&#8217;s <code class="literal">robots.txt</code> file.
Remember this if you&#8217;re crawling a domain you don&#8217;t control.</p>
</div>
</div>
<p>A domain may have a <code class="literal">robots.txt</code> file.
This is a plain text file that provides instructions to web crawlers.
The instructions within the file, also called directives, communicate which paths within that domain are disallowed (and allowed) for crawling.</p>
<p>You can also use a <code class="literal">robots.txt</code> file to specify sitemaps for a domain.
See <a class="xref" href="crawler-content.html#crawler-content-sitemap" title="Sitemaps">Sitemaps</a>.</p>
<p>Most web crawlers automatically fetch and parse the <code class="literal">robots.txt</code> file for each domain they crawl.
If you already publish a <code class="literal">robots.txt</code> file for other web crawlers, be aware the web crawler will fetch this file and honor the directives within it.
You may want to add, remove, or update the <code class="literal">robots.txt</code> file for each of your domains.</p>
<p><span class="strong strong"><strong>Example: add a <code class="literal">robots.txt</code> file to a domain</strong></span></p>
<p>To add a <code class="literal">robots.txt</code> file to the domain <code class="literal">https://shop.example.com</code>:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Determine which paths within the domain you&#8217;d like to exclude.
</li>
<li class="listitem">
<p>Create a robots.txt file with the appropriate directives from the <a href="https://en.wikipedia.org/wiki/Robots_exclusion_standard" class="ulink" target="_blank" rel="noopener">Robots exclusion standard</a>.
For instance:</p>
<div class="pre_wrapper lang-yaml">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-yaml">User-agent: *
Disallow: /cart
Disallow: /login
Disallow: /account</pre>
</div>
</li>
<li class="listitem">
Publish the file, with filename <code class="literal">robots.txt</code>, at the root of the domain: <code class="literal">https://shop.example.com/robots.txt</code>.
</li>
</ol>
</div>
<p>The next time the web crawler visits the domain, it will fetch and parse the <code class="literal">robots.txt</code> file.</p>
<div class="tip admon">
<div class="icon"></div>
<div class="admon_content">
<p>The web crawler will crawl only those paths that are allowed by the crawl rules for the domain <span class="strong strong"><strong><em>and</em></strong></span> the directives within the <code class="literal">robots.txt</code> file for the domain.</p>
</div>
</div>
<p>See <a class="xref" href="crawler-managing.html#crawler-managing-crawl-rules" title="Crawl rules">crawl rules</a> for detailed information about crawl rules.</p>
<h4><a id="crawler-content-manage-robots-txt-nonstandard-extensions"></a>Non-standard extensions<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.14/enterprise-search-docs/crawler-content.asciidoc">edit</a></h4>
<p>The Elastic web crawler does not support all <a href="https://en.wikipedia.org/wiki/Robots.txt#Nonstandard_extensions" class="ulink" target="_blank" rel="noopener">Nonstandard extensions</a> to the robots exclusion standard.</p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
</colgroup>
<tbody>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>Directive</strong></span></p></td>
<td align="left" valign="top"><p><span class="strong strong"><strong>Support</strong></span></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Crawl-delay directive</p></td>
<td align="left" valign="top"><p>Not supported</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Sitemap directive</p></td>
<td align="left" valign="top"><p>Supported</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Host directive</p></td>
<td align="left" valign="top"><p>Not supported</p></td>
</tr>
</tbody>
</table>
</div>
<h3><a id="crawler-content-sitemap"></a>Sitemaps<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.14/enterprise-search-docs/crawler-content.asciidoc">edit</a></h3>
<p>A sitemap is an XML file, associated with a domain, that informs web crawlers about pages within that domain.
XML elements within the sitemap identify specific URLs that are available for crawling.
Each domain may have one or more sitemaps.</p>
<p>If you already publish sitemaps for other web crawlers, the web crawler can use the same sitemaps.
To make your sitemaps discoverable, specify them within <code class="literal">robots.txt</code> files.</p>
<p>Sitemaps are related to entry points.
See <a class="xref" href="crawler-managing.html#crawler-managing-entry-points" title="Entry points">entry points</a>.
You can choose to submit URLs to the web crawler using sitemaps, entry points, or a combination of both.</p>
<p>You may prefer using sitemaps over entry points for any of the following reasons:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
You have already been publishing sitemaps for other web crawlers.
</li>
<li class="listitem">
You don&#8217;t have access to the web crawler UI in Kibana.
</li>
<li class="listitem">
You prefer the sitemap file interface over the Kibana UI.
</li>
</ul>
</div>
<p>Use sitemaps to inform the web crawler of pages you think are important, or pages that are isolated and not linked from other pages.
However, be aware the web crawler will visit only those pages from the sitemap that are allowed by the domain&#8217;s crawl rules and <code class="literal">robots.txt</code> file directives.</p>
<h4><a id="crawler-content-sitemap-discovery-management"></a>Sitemap discovery and management<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.14/enterprise-search-docs/crawler-content.asciidoc">edit</a></h4>
<p>To add a sitemap to a domain, you can specify it within a <code class="literal">robots.txt</code> file.
At the start of each crawl, the web crawler fetches and processes each domain&#8217;s <code class="literal">robots.txt</code> file and each sitemap specified within those <code class="literal">robots.txt</code> files.</p>
<h4><a id="crawler-content-sitemap-format"></a>Sitemap format and technical specification<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.14/enterprise-search-docs/crawler-content.asciidoc">edit</a></h4>
<p>The <a href="https://www.sitemaps.org/index.html" class="ulink" target="_blank" rel="noopener">sitemaps standard</a> defines the format and technical specification for sitemaps.
Refer to the standard for the required and optional elements, character escaping, and other technical considerations and examples.</p>
<p>The web crawler does not process optional meta data defined by the standard.
The web crawler extracts a list of URLs from each sitemap and ignores all other information.</p>
<p>There is no guarantee that pages (and their respective linked pages) will be indexed in the order they appear in the sitemap, because crawls are run <em>asynchronously</em>.</p>
<p>Ensure each URL within your sitemap matches the exact domain — here defined as scheme + host + port— for your site.
Different subdomains (like <code class="literal">www.example.com</code> and <code class="literal">blog.example.com</code>), and different schemes (like <code class="literal">http://example.com</code> and <code class="literal">https://example.com</code>), require separate sitemaps.</p>
<p>The web crawler also supports sitemap index files.
Refer to <a href="https://www.sitemaps.org/protocol.html#index" class="ulink" target="_blank" rel="noopener">Using sitemap index files</a> within the sitemap standard for sitemap index file details and examples.</p>
<h4><a id="crawler-content-manage-sitemaps"></a>Manage sitemaps<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.14/enterprise-search-docs/crawler-content.asciidoc">edit</a></h4>
<p><span class="strong strong"><strong>Example: Add a sitemap via <code class="literal">robots.txt</code></strong></span></p>
<p>To add a sitemap to the domain <code class="literal">https://shop.example.com</code>:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Determine which pages within the domain you&#8217;d like to include.
Ensure these paths are allowed by the domain&#8217;s crawl rules and the directives within the domain&#8217;s <code class="literal">robots.txt</code> file.
</li>
<li class="listitem">
<p>Create a sitemap file with the appropriate elements from the <a href="https://www.sitemaps.org/index.html" class="ulink" target="_blank" rel="noopener">sitemap standard</a>.
For instance:</p>
<div class="pre_wrapper lang-xml">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-xml">&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"&gt;
  &lt;url&gt;
    &lt;loc&gt;https://shop.example.com/products/1/&lt;/loc&gt;
  &lt;/url&gt;
  &lt;url&gt;
    &lt;loc&gt;https://shop.example.com/products/2/&lt;/loc&gt;
  &lt;/url&gt;
  &lt;url&gt;
    &lt;loc&gt;https://shop.example.com/products/3/&lt;/loc&gt;
  &lt;/url&gt;
&lt;/urlset&gt;</pre>
</div>
</li>
<li class="listitem">
Publish the file on your site, for example, at the root of the domain: <code class="literal">https://shop.example.com/sitemap.xml</code>.
</li>
<li class="listitem">
<p>Create or modify the <code class="literal">robots.txt</code> file for the domain, located at <code class="literal">https://shop.example.com/robots.txt</code>.
Anywhere within the file, add a <code class="literal">Sitemap</code> directive that provides the location of the sitemap.
For instance:</p>
<div class="pre_wrapper lang-yaml">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-yaml">Sitemap: https://shop.example.com/sitemap.xml</pre>
</div>
</li>
<li class="listitem">
Publish the new or updated <code class="literal">robots.txt</code> file.
</li>
</ol>
</div>
<p>The next time the web crawler visits the domain, it will fetch and parse the <code class="literal">robots.txt</code> file and the sitemap.</p>
<p>Alternatively, you can also manage the sitemaps for a domain through the Kibana UI.
From here, you can view, add, edit, and delete sitemaps.
Use the UI to add custom sitemap definitions that do not live on the domain, and are used only by your crawler.
See <a class="xref" href="crawler-managing.html#crawler-managing-entry-points-sitemaps" title="Entry points and sitemaps">Entry points and sitemaps</a>.</p>
</div>
</div>
</div><div class="navfooter">
<span class="prev">
<a href="crawler-troubleshooting.html">« Troubleshooting crawls</a>
</span>
<span class="next">
<a href="crawler-custom-values-ingest-pipeline.html">Customize crawler field values using an ingest pipeline »</a>
</span>
</div>
</body>
</html>
