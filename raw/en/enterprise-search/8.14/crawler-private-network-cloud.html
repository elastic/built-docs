<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Crawl a private network using a web crawler on Elastic Cloud | Enterprise Search documentation [8.14] | Elastic</title>
<meta class="elastic" name="content" content="Crawl a private network using a web crawler on Elastic Cloud | Enterprise Search documentation [8.14]">

<link rel="home" href="index.html" title="Enterprise Search documentation [8.14]"/>
<link rel="up" href="crawler.html" title="Elastic web crawler"/>
<link rel="prev" href="crawler-custom-values-ingest-pipeline.html" title="Customize crawler field values using an ingest pipeline"/>
<link rel="next" href="crawler-view-events-logs.html" title="View web crawler events logs"/>
<meta class="elastic" name="product_version" content="8.14"/>
<meta class="elastic" name="product_name" content="Enterprise Search"/>
<meta class="elastic" name="website_area" content="documentation"/>
<meta name="DC.type" content="Learn/Docs/Enterprise Search/Guide/8.14"/>
<meta name="DC.subject" content="Enterprise Search"/>
<meta name="DC.identifier" content="8.14"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body>
<div class="page_header">
<strong>IMPORTANT</strong>: No additional bug fixes or documentation updates
will be released for this version. For the latest information, see the
<a href="../current/index.html">current release documentation</a>.
</div>
<div class="navheader">
<span class="prev">
<a href="crawler-custom-values-ingest-pipeline.html">« Customize crawler field values using an ingest pipeline</a>
</span>
<span class="next">
<a href="crawler-view-events-logs.html">View web crawler events logs »</a>
</span>
</div>
<div class="book" lang="en">
<div class="titlepage">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link">
<div id="related-products" class="dropdown">
<div class="related-products-title"></div>
<div class="dropdown-anchor" tabindex="0">Enterprise Search<span class="dropdown-icon"></span></div>
<div class="dropdown-content">
<ul>
<li class="dropdown-category">Enterprise Search guides</li>
<ul>
<li><a href="/guide/en/enterprise-search/current/index.html" target="_blank">Enterprise Search</a></li>
<li><a href="/guide/en/app-search/current/index.html" target="_blank">App Search</a></li>
<li><a href="/guide/en/workplace-search/current/index.html" target="_blank">Workplace Search</a></li>
</ul>
<li class="dropdown-category">Programming language clients</li>
<ul>
<li><a href="https://www.elastic.co/guide/en/enterprise-search-clients/enterprise-search-node/current/index.html" target="_blank">Node.js client</a></li>
<li><a href="https://www.elastic.co/guide/en/enterprise-search-clients/php/current/index.html" target="_blank">PHP client</a></li>
<li><a href="https://www.elastic.co/guide/en/enterprise-search-clients/python/current/index.html" target="_blank">Python client</a></li>
<li><a href="https://www.elastic.co/guide/en/enterprise-search-clients/ruby/current/index.html" target="_blank">Ruby client</a></li>
</ul>
</ul>
</div>
</div>
</span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="crawler.html">Elastic web crawler</a></span>
</div>
<div>
<div><h1 class="title"><a id="id-1"></a>Crawl a private network using a web crawler on Elastic Cloud</h1><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.14/enterprise-search-docs/crawler-private-network.asciidoc">edit</a></div>
</div>
<!--EXTRA-->
</div>
<div id="content">
<div class="chapter">
<div class="titlepage"><div><div>
<h2 class="title"><a id="crawler-private-network-cloud"></a>Crawl a private network using a web crawler on Elastic Cloud<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.14/enterprise-search-docs/crawler-private-network.asciidoc">edit</a></h2>
</div></div></div>

<p>The Elastic web crawler can crawl content on a private network if the content is accessible through an HTTP proxy.</p>
<p>This document explains how to configure the web crawler to crawl a private network by walking through a detailed example.</p>
<p>The web crawler is hosted on Elastic Cloud, but has access to protected resources in private networks and other non-public environments.</p>
<h3><a id="crawler-private-network-cloud-overview"></a>Infrastructure overview<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.14/enterprise-search-docs/crawler-private-network.asciidoc">edit</a></h3>
<p>To demonstrate the authenticated proxy server functionality for the web crawler, we will use the infrastructure outlined in this schematic diagram:</p>
<div class="imageblock">
<div class="content">
<img src="images/crawler-proxy-schematic.png" alt="crawler proxy schematic">
</div>
</div>
<p>This example includes all the components needed to understand the use case:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
A VPC (virtual private network) deployed within Google Cloud
</li>
<li class="listitem">
<p>The VPC contains two cloud instances: <code class="literal">web-server</code> and <code class="literal">proxy</code>:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
The web-server hosts a private website, that is only accessible from within the VPC, using a private domain name: <a href="http://marlin-docs.internal" class="ulink" target="_top">http://marlin-docs.internal</a>.
</li>
<li class="listitem">
The proxy server has basic HTTP authentication set up with the <span class="strong strong"><strong>username</strong></span> <code class="literal">proxyuser</code> and <span class="strong strong"><strong>password</strong></span> <code class="literal">proxypass</code>.
It is accessible from the public internet using the <span class="strong strong"><strong>host name</strong></span> <code class="literal">proxy.example.com</code> and the <span class="strong strong"><strong>port number</strong></span> <code class="literal">3128</code>.
</li>
</ul>
</div>
</li>
<li class="listitem">
<p>An Enterprise Search deployment running outside of the VPC, in Elastic Cloud.</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
This deployment has no access to the private network hosting our content.
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>The web crawler supports both HTTP and HTTPS.
However, we recommend configuring TLS to protect your content and proxy server credentials.</p>
<p>Refer to your proxy server software documentation for more details on how to set this up.</p>
</div>
</div>
<h3><a id="crawler-private-network-cloud-test-proxy"></a>Testing proxy connections<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.14/enterprise-search-docs/crawler-private-network.asciidoc">edit</a></h3>
<p>Before changing your Enterprise Search deployment configuration to use the HTTP proxy described above,
first make sure the proxy works and allows access to the private website.</p>
<p>You can configure your web browser to use the proxy and then try to access the private website.
Alternatively, use the following command to fetch the home page from the site using a given proxy:</p>
<div class="pre_wrapper lang-bash">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-bash">curl --head --proxy https://proxyuser:proxypass@proxy.example.com http://marlin-docs.internal</pre>
</div>
<p>This request makes a <code class="literal">HEAD</code> request to the website, using the proxy.
The response should be an <code class="literal">HTTP 200</code> with a set of additional headers.
Here is an example response:</p>
<div class="pre_wrapper lang-bash">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-bash">HTTP/1.1 200 OK
Content-Type: text/html
Content-Length: 42337
Accept-Ranges: bytes
Server: nginx/1.14.2
Date: Tue, 30 Nov 2021 19:19:14 GMT
Last-Modified: Tue, 30 Nov 2021 17:57:39 GMT
ETag: "61a66613-a561"
Age: 4
X-Cache: HIT from test-proxy
X-Cache-Lookup: HIT from test-proxy:3128
Via: 1.1 test-proxy (squid/4.6)
Connection: keep-alive</pre>
</div>
<p>We have confirmed that our proxy credentials and connection parameters are correct, and can now change the Enterprise Search configuration.</p>
<h3><a id="crawler-private-network-cloud-configure-ent-search"></a>Configuring Enterprise Search<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.14/enterprise-search-docs/crawler-private-network.asciidoc">edit</a></h3>
<p>To prepare our Enterprise Search deployment to use the HTTP proxy for all web crawler operations, we need to add the following <a href="/guide/en/cloud/current/ec-manage-enterprise-search-settings.html" class="ulink" target="_top">user settings</a>:</p>
<div class="pre_wrapper lang-yaml">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-yaml">connector.crawler.http.proxy.host: proxy.example.com
connector.crawler.http.proxy.port: 3128
connector.crawler.http.proxy.protocol: https
connector.crawler.http.proxy.username: proxyuser
connector.crawler.http.proxy.password: proxypass</pre>
</div>
<p>When this configuration is added, the deployment will perform a graceful restart.
You can find detailed instructions on how to work with custom configurations in our <a href="/guide/en/cloud/current/ec-manage-enterprise-search-settings.html" class="ulink" target="_top">official Elastic Cloud documentation</a>.</p>
<div class="important admon">
<div class="icon"></div>
<div class="admon_content">
<p>HTTP proxy with basic authentication requires an appropriate Elastic subscription level.
Refer to the Elastic subscriptions pages for <a href="/subscriptions/cloud" class="ulink" target="_blank" rel="noopener">Elastic Cloud</a> and <a href="/subscriptions" class="ulink" target="_top">self-managed</a> deployments.</p>
</div>
</div>
<h3><a id="crawler-private-network-cloud-test-solution"></a>Testing the solution<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.14/enterprise-search-docs/crawler-private-network.asciidoc">edit</a></h3>
<p>With the configuration complete, we can now test the solution.</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Go to <span class="strong strong"><strong>Search</strong></span> in Kibana.
</li>
<li class="listitem">
Create an Elasticsearch index.
</li>
<li class="listitem">
Choose the <span class="strong strong"><strong>Use the Web Crawler</strong></span> ingestion method.
</li>
<li class="listitem">
Add your <span class="strong strong"><strong>domain URL</strong></span> to be validated by the crawler.
</li>
</ol>
</div>
<p>The validation process will look like the one in this image:</p>
<div class="imageblock">
<div class="content">
<img src="images/crawler-proxy-validation.png" alt="crawler proxy validation">
</div>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>The validation process for adding a domain to the web crawler configuration will skip a number of networking-related checks, since these do not work through a proxy.
If you do not see the warning like in the above image, check your deployment configuration to ensure you have configured the proxy correctly.</p>
</div>
</div>
<p>Our private domain is now added to the configuration.
We can start the crawl, and the content should be ingested into the deployment.
If there are failures, check your <a class="xref" href="crawler-view-events-logs.html" title="View web crawler events logs">crawler logs</a> and proxy server&#8217;s logs for clues about what might be going wrong.</p>
<p>If you are using a Squid proxy server, the logs should look like this:</p>
<div class="pre_wrapper lang-bash">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-bash">1638298043.202      1 99.250.74.78 TCP_MISS/200 65694 GET http://marlin-docs.internal/docs/gcode/M951.html proxyuser HIER_DIRECT/10.188.0.2 text/html
1638298045.286      1 99.250.74.78 TCP_MISS/200 64730 GET http://marlin-docs.internal/docs/gcode/M997.html proxyuser HIER_DIRECT/10.188.0.2 text/html
1638298045.373      1 99.250.74.78 TCP_MISS/200 63609 GET http://marlin-docs.internal/docs/gcode/M999.html proxyuser HIER_DIRECT/10.188.0.2 text/html</pre>
</div>
</div>
</div>
</div><div class="navfooter">
<span class="prev">
<a href="crawler-custom-values-ingest-pipeline.html">« Customize crawler field values using an ingest pipeline</a>
</span>
<span class="next">
<a href="crawler-view-events-logs.html">View web crawler events logs »</a>
</span>
</div>
</body>
</html>
