<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Troubleshooting crawls | Enterprise Search documentation [8.14] | Elastic</title>
<meta class="elastic" name="content" content="Troubleshooting crawls | Enterprise Search documentation [8.14]">

<link rel="home" href="index.html" title="Enterprise Search documentation [8.14]"/>
<link rel="up" href="crawler.html" title="Elastic web crawler"/>
<link rel="prev" href="crawler-known-issues.html" title="Elastic web crawler known issues"/>
<link rel="next" href="crawler-content.html" title="Optimizing web content for the web crawler"/>
<meta class="elastic" name="product_version" content="8.14"/>
<meta class="elastic" name="product_name" content="Enterprise Search"/>
<meta class="elastic" name="website_area" content="documentation"/>
<meta name="DC.type" content="Learn/Docs/Enterprise Search/Guide/8.14"/>
<meta name="DC.subject" content="Enterprise Search"/>
<meta name="DC.identifier" content="8.14"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body>
<div class="page_header">
<strong>IMPORTANT</strong>: No additional bug fixes or documentation updates
will be released for this version. For the latest information, see the
<a href="../current/index.html">current release documentation</a>.
</div>
<div class="navheader">
<span class="prev">
<a href="crawler-known-issues.html">« Elastic web crawler known issues</a>
</span>
<span class="next">
<a href="crawler-content.html">Optimizing web content for the web crawler »</a>
</span>
</div>
<div class="book" lang="en">
<div class="titlepage">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link">
<div id="related-products" class="dropdown">
<div class="related-products-title"></div>
<div class="dropdown-anchor" tabindex="0">Enterprise Search<span class="dropdown-icon"></span></div>
<div class="dropdown-content">
<ul>
<li class="dropdown-category">Enterprise Search guides</li>
<ul>
<li><a href="/guide/en/enterprise-search/current/index.html" target="_blank">Enterprise Search</a></li>
<li><a href="/guide/en/app-search/current/index.html" target="_blank">App Search</a></li>
<li><a href="/guide/en/workplace-search/current/index.html" target="_blank">Workplace Search</a></li>
</ul>
<li class="dropdown-category">Programming language clients</li>
<ul>
<li><a href="https://www.elastic.co/guide/en/enterprise-search-clients/enterprise-search-node/current/index.html" target="_blank">Node.js client</a></li>
<li><a href="https://www.elastic.co/guide/en/enterprise-search-clients/php/current/index.html" target="_blank">PHP client</a></li>
<li><a href="https://www.elastic.co/guide/en/enterprise-search-clients/python/current/index.html" target="_blank">Python client</a></li>
<li><a href="https://www.elastic.co/guide/en/enterprise-search-clients/ruby/current/index.html" target="_blank">Ruby client</a></li>
</ul>
</ul>
</div>
</div>
</span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="crawler.html">Elastic web crawler</a></span>
</div>
<div>
<div><h1 class="title"><a id="id-1"></a>Troubleshooting crawls</h1><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.14/enterprise-search-docs/crawler-troubleshooting.asciidoc">edit</a></div>
</div>
<!--EXTRA-->
</div>
<div id="content">
<div class="chapter">
<div class="titlepage"><div><div>
<h2 class="title"><a id="crawler-troubleshooting"></a>Troubleshooting crawls<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.14/enterprise-search-docs/crawler-troubleshooting.asciidoc">edit</a></h2>
</div></div></div>
<p>This page contains a detailed troubleshooting reference, including information about web crawler <a class="xref" href="crawler-troubleshooting.html#crawler-troubleshooting-event-logs" title="View web crawler events logs">event logs</a>.</p>
<p>This page documents issues users can troubleshoot, which generally fall into three categories.</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<p>See <a class="xref" href="crawler-troubleshooting.html#crawler-troubleshooting-crawl-stability" title="Troubleshoot crawl stability">Crawl stability</a> if:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
You’re not sure where to start (resolve stability issues first)
</li>
<li class="listitem">
No documents in the engine
</li>
<li class="listitem">
Many documents missing or outdated
</li>
<li class="listitem">
Crawl fails
</li>
<li class="listitem">
Crawl runs for the maximum duration (defaults to 24 hours)
</li>
</ul>
</div>
</li>
<li class="listitem">
<p>See <a class="xref" href="crawler-troubleshooting.html#crawler-troubleshooting-content-discovery" title="Troubleshoot content discovery">Content discovery</a> if:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Specific documents are missing or outdated
</li>
</ul>
</div>
</li>
<li class="listitem">
<p>See <a class="xref" href="crawler-troubleshooting.html#crawler-troubleshooting-content-extraction-and-indexing" title="Troubleshoot content extraction and indexing">Content extraction and indexing</a> if:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Specific documents are missing or outdated
</li>
<li class="listitem">
Incorrect content within documents
</li>
<li class="listitem">
Content missing from documents
</li>
</ul>
</div>
</li>
</ul>
</div>
<p>We also provide solutions for specific errors and a list of version history:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="crawler-troubleshooting.html#crawler-troubleshooting-specific-errors" title="Specific errors">Specific errors</a>
</li>
<li class="listitem">
<a class="xref" href="crawler-troubleshooting.html#crawler-troubleshooting-history" title="Version history">Version history</a>
</li>
</ul>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<a id="crawler-troubleshooting-crawl-js-spa-note"></a>
<p>The crawler cannot currently crawl pure JavaScript single-page applications (SPAs).
We recommend looking at <a href="https://developers.google.com/search/docs/advanced/javascript/dynamic-rendering" class="ulink" target="_blank" rel="noopener">dynamic rendering</a> to help your crawler properly index your JavaScript websites.</p>
<p>Another option is to serve a static HTML version of your Javascript website, using a solution such as <a href="https://prerender.io/" class="ulink" target="_blank" rel="noopener">Prerender</a>.</p>
</div>
</div>
<h3><a id="crawler-troubleshooting-crawl-stability"></a>Troubleshoot crawl stability<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.14/enterprise-search-docs/crawler-troubleshooting.asciidoc">edit</a></h3>
<p>Crawl stability issues may prevent the crawler from discovering, extracting, and indexing your content.
It is critical to address these issues first.</p>
<p>Use the following techniques to troubleshoot crawl stability issues.</p>
<p><span class="strong strong"><strong>Validate one or more domains:</strong></span></p>
<p>When adding a domain through the web crawler UI, the crawler validates the domain.
You may have added a domain that experienced validation errors.
Check those errors and re-crawl.</p>
<p>An invalid domain is a common cause for a crawl with no results.
Fix the domain issues and re-crawl.</p>
<p><span class="strong strong"><strong>Analyze web crawler events logs for the most recent crawl:</strong></span></p>
<p>First:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Find the crawl request ID for the most recent crawl.
In the Kibana UI, find this under <span class="strong strong"><strong>Recent crawl requests</strong></span>.
</li>
<li class="listitem">
Filter the web crawler events logs by that ID.
See <a class="xref" href="crawler-view-events-logs.html" title="View web crawler events logs"><em>View web crawler events logs</em></a> to learn how to view these logs in Kibana.
</li>
</ol>
</div>
<div id="crawler-events-logs-viewer" class="imageblock screenshot">
<div class="content">
<img src="images/crawler-events-logs-viewer.png" alt="A picture of the crawler events logs viewer, filtering by a crawl request ID.">
</div>
<div class="title">Figure 4. Crawler events logs viewer</div>
</div>
<p>Then:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Order the events by timestamp, oldest first.
If using the Observability Logs stream, scroll to the end of the list.
</li>
<li class="listitem">
Locate the <code class="literal">crawl-end</code> event and preceding events.
These events communicate what happened before the crawl failed.
</li>
</ul>
</div>
<p><span class="strong strong"><strong>Analyze web crawler system logs:</strong></span></p>
<p>These logs may contain additional information about your crawl.</p>
<p><span class="strong strong"><strong>Modify the web crawler configuration:</strong></span></p>
<p>As a last resort, operators can modify the web crawler configuration, including resource limits.</p>
<p>See <a class="xref" href="configuration.html#configuration-settings-elastic-crawler" title="Elastic web crawler">Elastic web crawler</a> within the Enterprise Search configuration documentation.</p>
<h3><a id="crawler-troubleshooting-content-discovery"></a>Troubleshoot content discovery<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.14/enterprise-search-docs/crawler-troubleshooting.asciidoc">edit</a></h3>
<p>After your crawls are stable, you may find the crawler is not discovering your content as expected.</p>
<p>Use the following techniques to troubleshoot content discovery issues.</p>
<p><span class="strong strong"><strong>Confirm the most recent crawl completed successfully:</strong></span></p>
<p>View the status of the most recent crawl to confirm it completed successfully.</p>
<p>If the crawl failed, look for signs of crawl stability issues.
See <a class="xref" href="crawler-troubleshooting.html#crawler-troubleshooting-crawl-stability" title="Troubleshoot crawl stability">Troubleshoot crawl stability</a>.</p>
<p><span class="strong strong"><strong>View indexed documents to confirm missing pages:</strong></span></p>
<p>Identify which pages are missing from your index, or focus on specific pages.</p>
<p><span class="strong strong"><strong>Analyze web crawler events logs for the most recent crawl:</strong></span></p>
<p>See <a class="xref" href="crawler-view-events-logs.html" title="View web crawler events logs"><em>View web crawler events logs</em></a> to learn how to view crawler events logs in Kibana.</p>
<p>First:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Find the crawl request ID for the most recent crawl.
In the Kibana UI, find this under <span class="strong strong"><strong>Recent crawl requests</strong></span>.
</li>
<li class="listitem">
Filter the web crawler events logs by that ID.
</li>
<li class="listitem">
Find the URL of a specific document missing from the engine.
</li>
<li class="listitem">
Filter the web crawler events logs by that URL.
</li>
</ol>
</div>
<p>Then:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Locate <code class="literal">url-discover</code> events to confirm the crawler has seen links to your page.
The <code class="literal">outcome</code> and <code class="literal">message</code> fields may explain why the web crawler did not crawl the page.
</li>
<li class="listitem">
If <code class="literal">url-discover</code> events indicate discovery was successful, locate <code class="literal">url-fetch</code> events to analyze the fetching phase of the crawl.
</li>
</ul>
</div>
<div id="crawler-events-logs-viewer-by-url" class="imageblock screenshot">
<div class="content">
<img src="images/crawler-events-logs-viewer-by-url.png" alt="A picture of the events logs viewer, filtering by a crawl request ID, URL, and url-fetch event.">
</div>
<div class="title">Figure 5. Crawler events logs viewer</div>
</div>
<p><span class="strong strong"><strong>Analyze web crawler system logs:</strong></span></p>
<p>If you are managing your own Enterprise Search deployment, you can also view the web crawler system logs.
View these logs on disk in the <code class="literal">crawler.log</code> file.</p>
<p>The events in these logs are less verbose then the web crawler events logs, but they can help solve web crawler issues.
Each event has a crawl request ID, which allows you to analyze the logs for a specific crawl.</p>
<p><span class="strong strong"><strong>Address specific content discovery problems:</strong></span></p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Problem</th>
<th align="left" valign="top">Description</th>
<th align="left" valign="top">Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p>External domain</p></td>
<td align="left" valign="top"><p>The web crawler does not follow links that go outside the domains configured for each crawl.</p></td>
<td align="left" valign="top"><p>Add any missing domains.
Remember each unique combination of protocol and hostname needs to be added as a domain.</p>
<p>For example, <code class="literal">https://example.com</code> and <code class="literal">https://www.example.com/page</code> are <span class="strong strong"><strong>different domains</strong></span> and must be added separately.
See <a class="xref" href="crawler-managing.html#crawler-managing-domains" title="Domains">Domains</a>.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Disallowed path</p></td>
<td align="left" valign="top"><p>The web crawler does not follow links whose paths are disallowed by a domain&#8217;s crawl rules or robots.txt directives.</p></td>
<td align="left" valign="top"><p>Manage <a class="xref" href="crawler-managing.html#crawler-managing-crawl-rules" title="Crawl rules">crawl rules</a> and <a class="xref" href="crawler-content.html#crawler-content-manage-robots-txt-files" title="robots.txt files"><code class="literal">robots.txt</code></a> files for each domain, to ensure paths are allowed.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>No incoming links</p></td>
<td align="left" valign="top"><p>The web crawler cannot find pages that have no incoming links, unless you provide the path as an entry point.</p></td>
<td align="left" valign="top"><p>Add links to the content from other content that the web crawler has already discovered, or explicitly add the URL entry point or within a sitemap.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Nofollow links</p></td>
<td align="left" valign="top"><p>The web crawler does not follow <a class="xref" href="crawler-content.html#crawler-content-nofollow-link" title="Nofollow links">nofollow links</a>.</p></td>
<td align="left" valign="top"><p>Remove the nofollow link to allow content discovery.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">nofollow</code> robots meta tag</p></td>
<td align="left" valign="top"><p>If a page contains a <code class="literal">nofollow</code> <a class="xref" href="crawler-content.html#crawler-content-robots-meta-tags" title="Robots meta tags">robots meta tag</a>, the web crawler will not follows links from that page.</p></td>
<td align="left" valign="top"><p>Remove the meta tag from your page.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Page too large</p></td>
<td align="left" valign="top"><p>The web crawler does not parse HTTP responses larger than <code class="literal">connector.crawler.http.response_size.limit</code>.</p></td>
<td align="left" valign="top"><p>Reduce the size of your page.
Or, increase the limit for your deployment.
Increasing the limit may increase crawl durations and resource consumption, and could reduce crawl stability.
Significantly increasing these limits may also have a negative impact on search result relevance.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Too many redirects</p></td>
<td align="left" valign="top"><p>The web crawler does not follow redirect chains longer than <code class="literal">connector.crawler.http.redirects.limit</code>.</p></td>
<td align="left" valign="top"><p>Reduce the number of redirects for the page.
Or, increase the limit for your deployment.
Increasing the limit may increase crawl durations and resource consumption, and could reduce crawl stability.
Significantly increasing these limits may also have a negative impact on search result relevance.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Network latency</p></td>
<td align="left" valign="top"><p>The web crawler fails requests that exceed the following network timeouts:
<code class="literal">connector.crawler.http.connection_timeout</code>,
<code class="literal">connector.crawler.http.read_timeout</code>,
<code class="literal">connector.crawler.http.request_timeout</code>.</p></td>
<td align="left" valign="top"><p>Reduce network latency.
Or, increase these timeouts for your deployment.
Increasing the timeouts may increase crawl durations and resource consumption, and could reduce crawl stability.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>HTTP errors</p></td>
<td align="left" valign="top"><p>The web crawler cannot discover and index content if it cannot fetch HTML pages from a domain.
The web crawler will not index pages that respond with a <code class="literal">4xx</code> or <code class="literal">5xx</code> response code.</p></td>
<td align="left" valign="top"><p>Fix HTTP server errors.
Ensure correct HTTP response codes.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>HTML errors</p></td>
<td align="left" valign="top"><p>The web crawler cannot parse extremely broken HTML pages.
In that case, the web crawler cannot index the page, and cannot discover links coming from that page.</p></td>
<td align="left" valign="top"><p>Use the <a href="https://validator.w3.org/" class="ulink" target="_blank" rel="noopener">W3C markup validation service</a> to identify and resolve HTML errors in your content.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Security</p></td>
<td align="left" valign="top"><p>The web crawler cannot access content requiring authentication or authorization.</p></td>
<td align="left" valign="top"><p>Remove the security to allow access to the web crawler.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Non-HTTP protocol</p></td>
<td align="left" valign="top"><p>The web crawler recognizes only the HTTP and HTTPS protocols.</p></td>
<td align="left" valign="top"><p>Publish your content at URLs using HTTP or HTTPS protocols.</p>
<p>Alternatively, for protocols like FTP, use the <a href="/guide/en/workplace-search/8.14/network-drives.html" class="ulink" target="_blank" rel="noopener">Elastic Enterprise Search network drives connector package</a> to deploy and run a network drives connector on your own infrastructure.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Invalid SSL certificate</p></td>
<td align="left" valign="top"><p>The web crawler will not crawl HTTPS pages with invalid certificates.</p></td>
<td align="left" valign="top"><p>See <a class="xref" href="crawler-troubleshooting.html#crawler-troubleshooting-specific-errors" title="Specific errors">Specific errors</a> for how to set specific certificate authorities, and to disable SSL checks.</p></td>
</tr>
</tbody>
</table>
</div>
<h3><a id="crawler-troubleshooting-content-extraction-and-indexing"></a>Troubleshoot content extraction and indexing<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.14/enterprise-search-docs/crawler-troubleshooting.asciidoc">edit</a></h3>
<p>The web crawler may be discovering your content but not extracting and indexing it as expected.</p>
<p>Use the following techniques to troubleshoot content discovery issues.</p>
<p><span class="strong strong"><strong>Confirm the most recent crawl completed successfully:</strong></span></p>
<p>View the status of the most recent crawl to confirm it completed successfully.</p>
<p>If the crawl failed, look for signs of crawl stability issues.
See <a class="xref" href="crawler-troubleshooting.html#crawler-troubleshooting-crawl-stability" title="Troubleshoot crawl stability">Troubleshoot crawl stability</a>.</p>
<p><span class="strong strong"><strong>View indexed documents to confirm missing pages:</strong></span></p>
<p>Identify which pages are missing from your index, or focus on specific pages.
Browse the documents in your index, under <span class="strong strong"><strong>Content</strong></span> &#8594; <span class="strong strong"><strong>Indices</strong></span> &#8594; <span class="strong strong"><strong>Documents</strong></span>.</p>
<p>If documents are missing from the index, look for signs of content discovery issues.
See <a class="xref" href="crawler-troubleshooting.html#crawler-troubleshooting-content-discovery" title="Troubleshoot content discovery">Troubleshoot content discovery</a>.</p>
<p><span class="strong strong"><strong>Analyze web crawler events logs for the most recent crawl:</strong></span></p>
<p>First:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Find the crawl request ID.
In the Kibana UI, find this under <span class="strong strong"><strong>Recent crawl requests</strong></span>.
</li>
<li class="listitem">
Filter the web crawler events logs by that ID.
</li>
<li class="listitem">
Find the URL of a specific document missing from the engine.
</li>
<li class="listitem">
Filter the web crawler events logs by that URL.
</li>
</ol>
</div>
<p>Then:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Locate <code class="literal">url-extracted</code> events to confirm the crawler was able to extract content from your page.
The <code class="literal">outcome</code> and <code class="literal">message</code> fields may explain why the web crawler could not extract and index the content.
</li>
<li class="listitem">
If <code class="literal">url-extracted</code> events indicate extraction was successful, locate <code class="literal">url-output</code> events to confirm the web crawler attempted ingestion of the page&#8217;s content.
</li>
</ul>
</div>
<p>See <a class="xref" href="crawler-troubleshooting.html#crawler-troubleshooting-event-logs" title="View web crawler events logs">event logs</a>.</p>
<p><span class="strong strong"><strong>Analyze web crawler system logs:</strong></span></p>
<p>These may contain additional information about specific pages.</p>
<p><span class="strong strong"><strong>Address specific content extraction and indexing problems:</strong></span></p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Problem</th>
<th align="left" valign="top">Description</th>
<th align="left" valign="top">Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p>Duplicate content</p></td>
<td align="left" valign="top"><p>If your website contains pages with duplicate content, those pages are stored as a single document within your engine.
The document&#8217;s <code class="literal">additional_urls</code> field indicates the URLs that contain the same content.</p></td>
<td align="left" valign="top"><p>Use a <a class="xref" href="crawler-content.html#crawler-content-canonical-url-link-tag" title="Canonical URL link tags">canonical URL link tag</a> within any document containing duplicate content.
See also <a class="xref" href="crawler-managing.html#crawler-managing-duplicate-documents" title="Duplicate document handling">Duplicate document handling</a>.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Non-HTML content missing</p></td>
<td align="left" valign="top"><p>The web crawler can index non-HTML, downloadable files.</p></td>
<td align="left" valign="top"><p>Ensure that the feature is enabled, and the MIME type of the missing file(s) is included in your configured MIME types.
See <a class="xref" href="crawler-managing.html#crawler-managing-binary-content" title="Binary content extraction">Binary content extraction</a>.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">noindex</code> robots meta tag</p></td>
<td align="left" valign="top"><p>The web crawler will not index pages that include a <code class="literal">noindex</code> robots meta tag.</p></td>
<td align="left" valign="top"><p>Remove the meta tag from your page.
See <a class="xref" href="crawler-content.html#crawler-content-robots-meta-tags" title="Robots meta tags">Robots meta tags</a>.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Page too large</p></td>
<td align="left" valign="top"><p>The web crawler does not parse HTTP responses larger than <code class="literal">connector.crawler.http.response_size.limit</code>.</p></td>
<td align="left" valign="top"><p>Reduce the size of your page.
Or, increase the limit for your deployment.
Increasing the limit may increase crawl durations and resource consumption, and could reduce crawl stability.
Significantly increasing these limits may also have a negative impact on search result relevance.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Truncated fields</p></td>
<td align="left" valign="top"><p>The web crawler truncates some fields before indexing the document, according to the following limits: <code class="literal">connector.crawler.extraction.body_size.limit</code>,
<code class="literal">connector.crawler.extraction.description_size.limit</code>,
<code class="literal">connector.crawler.extraction.headings_count.limit</code>,
<code class="literal">connector.crawler.extraction.indexed_links_count.limit</code>,
<code class="literal">connector.crawler.extraction.keywords_size.limit</code>,
<code class="literal">connector.crawler.extraction.title_size.limit</code>.</p></td>
<td align="left" valign="top"><p>Reduce the length of these fields within your content.
Or, increase these limits for your deployment.
Increasing the limits may increase crawl durations and resource consumption, and could reduce crawl stability.
Significantly increasing these limits may also have a negative impact on search result relevance.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Broken HTML</p></td>
<td align="left" valign="top"><p>The web crawler cannot parse extremely broken HTML pages.</p></td>
<td align="left" valign="top"><p>Use the <a href="https://validator.w3.org/" class="ulink" target="_blank" rel="noopener">W3C markup validation service</a> to identify and resolve HTML errors in your content.</p></td>
</tr>
</tbody>
</table>
</div>
<h3><a id="crawler-troubleshooting-event-logs"></a>View web crawler events logs<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.14/enterprise-search-docs/crawler-troubleshooting.asciidoc">edit</a></h3>
<div class="tip admon">
<div class="icon"></div>
<div class="admon_content">
<p>See <a class="xref" href="crawler-view-events-logs.html" title="View web crawler events logs"><em>View web crawler events logs</em></a> to learn how to view the web crawler events logs in Kibana.
For a complete reference of all events, see <a class="xref" href="crawler-events-logs-reference.html" title="Web crawler events logs reference"><em>Web crawler events logs reference</em></a>.</p>
</div>
</div>
<p>You might need to troubleshoot by viewing the web crawler events logs.
The web crawler records detailed structured events logs for each crawl.
The logs are indexed into Elasticsearch, and you can view the logs in Kibana.</p>
<h4><a id="crawler-troubleshooting-event-logs-id-url"></a>View web crawler events by crawl ID and URL<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.14/enterprise-search-docs/crawler-troubleshooting.asciidoc">edit</a></h4>
<p>To monitor a specific crawl or a specific domain, you must filter the web crawler events logs within Kibana.
See <a class="xref" href="crawler-view-events-logs.html" title="View web crawler events logs"><em>View web crawler events logs</em></a> to learn how to view these logs in Kibana.</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
To view the events for a specific crawl, find the request ID in the Kibana UI, under <span class="strong strong"><strong>Recent crawl requests</strong></span>.
</li>
<li class="listitem">
Filter on the <code class="literal">crawler.crawl.id</code> field.
</li>
<li class="listitem">
<p>Filter further to narrow your results to a specific URL.
Use the following fields:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
The full URL: <code class="literal">url.full</code>
</li>
<li class="listitem">
Required components of the URL: <code class="literal">url.scheme</code>, <code class="literal">url.domain</code>, <code class="literal">url.port</code>, <code class="literal">url.path</code>
</li>
<li class="listitem">
Optional components of the URL: <code class="literal">url.query</code>, <code class="literal">url.fragment</code>, <code class="literal">url.username</code>, <code class="literal">url.password</code>
</li>
</ul>
</div>
</li>
</ul>
</div>
<h3><a id="crawler-troubleshooting-specific-errors"></a>Specific errors<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.14/enterprise-search-docs/crawler-troubleshooting.asciidoc">edit</a></h3>
<p>If you are troubleshooting a specific error, you may find the error message and possible solutions here:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<p><span class="strong strong"><strong>Failed HTTP request: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target</strong></span></p>
<p>The domain&#8217;s SSL certificate is not included in the Java root certificate store.
The domain owner may need to work with the SSL certificate provider to get their root certificate added to the Java root certificate store.</p>
<p>For a self-signed certificate, add your certificate to the web crawler configuration, or change the SSL verification mode.</p>
<p>Add your certificate using the web crawler configuration setting <code class="literal">connector.crawler.security.ssl.certificate_authorities</code>.</p>
<p>If you have file access to the server, specify the location of the file:</p>
<div class="pre_wrapper lang-yaml">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-yaml">connector.crawler.security.ssl.certificate_authorities:
  - /path/to/QuoVadis-PKIoverheid-Organisatie-Server-CA-G3-PEM.pem</pre>
</div>
<p>For Elastic Cloud or other environments without file access, provide the contents of the file inline:</p>
<div class="pre_wrapper lang-yaml">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-yaml">connector.crawler.security.ssl.certificate_authorities:
  - |
    -----BEGIN CERTIFICATE-----
    MIIHWzCCBUOgAwIBAgIINBHa3VUceEkwDQYJKoZIhvcNAQELBQAwajELMAkGA1UE
    BhMCTkwxHjAcBgNVBAoMFVN0YWF0IGRlciBOZWRlcmxhbmRlbjE7MDkGA1UEAwwy
    U3RhYXQgZGVyIE5lZGVybGFuZGVuIE9yZ2FuaXNhdGllIFNlcnZpY2VzIENBIC0g
    RzMwHhcNMTYxMTAzMTQxMjExWhcNMjgxMTEyMDAwMDAwWjCBgjELMAkGA1UEBhMC
    ...
    TkwxIDAeBgNVBAoMF1F1b1ZhZGlzIFRydXN0bGluayBCLlYuMRcwFQYDVQRhDA5O
    VFJOTC0zMDIzNzQ1OTE4MDYGA1UEAwwvUXVvVmFkaXMgUEtJb3ZlcmhlaWQgT3Jn
    YW5pc2F0aWUgU2VydmVyIENBIC0gRzMwggIiMA0GCSqGSIb3DQEBAQUAA4ICDwAw
    SFfzGre9T6yBL4I+6nxG
    -----END CERTIFICATE-----</pre>
</div>
<p>Alternatively, for development and test environments, change the SSL verification mode using <code class="literal">connector.crawler.security.ssl.verification_mode</code>:</p>
<div class="pre_wrapper lang-yaml">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-yaml"># INSECURE - DO NOT USE IN PRODUCTION ENVIRONMENTS
connector.crawler.security.ssl.verification_mode: none</pre>
</div>
</li>
</ul>
</div>
<div class="tip admon">
<div class="icon"></div>
<div class="admon_content">
<p>Operators can configure several web crawler settings.
See <a class="xref" href="configuration.html#configuration-settings-elastic-crawler" title="Elastic web crawler">Elastic web crawler</a> within the Enterprise Search configuration documentation.</p>
</div>
</div>
<h3><a id="crawler-troubleshooting-history"></a>Version history<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.14/enterprise-search-docs/crawler-troubleshooting.asciidoc">edit</a></h3>
<p>See <a class="xref" href="crawler.html#crawler-history" title="Version history">Version history</a> for a list of significant changes to the web crawler.</p>
</div>
</div>
</div><div class="navfooter">
<span class="prev">
<a href="crawler-known-issues.html">« Elastic web crawler known issues</a>
</span>
<span class="next">
<a href="crawler-content.html">Optimizing web content for the web crawler »</a>
</span>
</div>
</body>
</html>
