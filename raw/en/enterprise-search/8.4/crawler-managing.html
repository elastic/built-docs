<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Managing crawls in Kibana | Elastic Enterprise Search documentation [8.4] | Elastic</title>
<meta class="elastic" name="content" content="Managing crawls in Kibana | Elastic Enterprise Search documentation [8.4]">

<link rel="home" href="index.html" title="Elastic Enterprise Search documentation [8.4]"/>
<link rel="up" href="crawler.html" title="Elastic web crawler"/>
<link rel="prev" href="crawler.html" title="Elastic web crawler"/>
<link rel="next" href="crawler-known-issues.html" title="Elastic web crawler known issues"/>
<meta class="elastic" name="product_version" content="8.4"/>
<meta class="elastic" name="product_name" content="Enterprise Search"/>
<meta class="elastic" name="website_area" content="documentation"/>
<meta name="DC.type" content="Learn/Docs/Enterprise Search/Guide/8.4"/>
<meta name="DC.subject" content="Enterprise Search"/>
<meta name="DC.identifier" content="8.4"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body><div class="page_header">
<strong>IMPORTANT</strong>: No additional bug fixes or documentation updates
will be released for this version. For the latest information, see the
<a href="../current/index.html">current release documentation</a>.
</div>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link">
<div id="related-products" class="dropdown">
<div class="related-products-title"></div>
<div class="dropdown-anchor" tabindex="0">Elastic Enterprise Search<span class="dropdown-icon"></span></div>
<div class="dropdown-content">
<ul>
<li class="dropdown-category">Enterprise Search guides</li>
<ul>
<li><a href="/guide/en/enterprise-search/current/index.html" target="_blank">Enterprise Search</a></li>
<li><a href="/guide/en/app-search/current/index.html" target="_blank">App Search</a></li>
<li><a href="/guide/en/workplace-search/current/index.html" target="_blank">Workplace Search</a></li>
</ul>
<li class="dropdown-category">Programming language clients</li>
<ul>
<li><a href="https://www.elastic.co/guide/en/enterprise-search-clients/enterprise-search-node/current/index.html" target="_blank">Node.js client</a></li>
<li><a href="https://www.elastic.co/guide/en/enterprise-search-clients/php/current/index.html" target="_blank">PHP client</a></li>
<li><a href="https://www.elastic.co/guide/en/enterprise-search-clients/python/current/index.html" target="_blank">Python client</a></li>
<li><a href="https://www.elastic.co/guide/en/enterprise-search-clients/ruby/current/index.html" target="_blank">Ruby client</a></li>
</ul>
</ul>
</div>
</div>
</span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="crawler.html">Elastic web crawler</a></span>
</div>
<div class="navheader">
<span class="prev">
<a href="crawler.html">« Elastic web crawler</a>
</span>
<span class="next">
<a href="crawler-known-issues.html">Elastic web crawler known issues »</a>
</span>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="crawler-managing"></a>Managing crawls in Kibana<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.4/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h2>
</div></div></div>

<p>This documentation contains all the information you need for managing crawls using the Kibana UI.</p>
<p>If you&#8217;d prefer to see a concrete crawler use case, see <a class="xref" href="website-search-start.html" title="Getting started with website search">Getting started with website search</a>.
If you need to learn how to optimize source files for the crawler, see <a class="xref" href="crawler-content.html" title="Optimizing web content for the web crawler">Optimizing web content</a>.</p>
<h4><a id="crawler-managing-overview"></a>Overview<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.4/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h4>
<p>It&#8217;s important to understand the primary crawl management tools and how they influence your crawls:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<span class="strong strong"><strong>Domains</strong></span> set crawl boundaries.
</li>
<li class="listitem">
<span class="strong strong"><strong>Entry points</strong></span> and <span class="strong strong"><strong>Sitemaps</strong></span> set starting points within domains.
</li>
<li class="listitem">
<span class="strong strong"><strong>Crawl rules</strong></span> and <code class="literal">robots.txt</code> directives set additional rules for crawling, beyond the starting points.
</li>
</ul>
</div>
<p>Here you&#8217;ll learn about discovering content, extracting content, crawling manually and scheduling crawls.</p>
<p><span class="strong strong"><strong>Discovering content</strong></span>:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="crawler-managing.html#crawler-managing-domains" title="Domains">Domains</a>
</li>
<li class="listitem">
<a class="xref" href="crawler-managing.html#crawler-managing-entry-points-sitemaps" title="Entry points and sitemaps">Entry points and sitemaps</a>
</li>
<li class="listitem">
<a class="xref" href="crawler-managing.html#crawler-managing-crawl-rules" title="Crawl rules">Crawl rules</a>
</li>
</ul>
</div>
<p><span class="strong strong"><strong>Extracting content</strong></span>:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="crawler-managing.html#crawler-managing-duplicate-documents" title="Duplicate document handling">Duplicate document handling</a>
</li>
<li class="listitem">
<a class="xref" href="crawler-managing.html#crawler-managing-binary-content" title="Binary content extraction">Binary content extraction</a>
</li>
</ul>
</div>
<p><span class="strong strong"><strong>Running manual crawls</strong></span>:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="crawler-managing.html#crawler-managing-manual-crawls" title="Running manual crawls">Running manual crawls</a>
</li>
</ul>
</div>
<p><span class="strong strong"><strong>Scheduling automated crawls</strong></span>:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="crawler-managing.html#crawler-managing-schedule" title="Scheduling automated crawls">Scheduling automated crawls</a>
</li>
</ul>
</div>
<h4><a id="crawler-managing-domains"></a>Domains<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.4/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h4>
<p>A domain is a website or property you&#8217;d like to crawl.
You must associate one or more domains to your index&#8217;s web crawler.
The web crawler cannot discover and index content outside of specified domains.</p>
<p>Each domain has a <em>domain URL</em> that identifies the domain using a protocol and hostname.
The domain URL can not include a path. If a path is provided, it will automatically be removed from the domain URL, and instead added as an entry point.</p>
<p>Each unique combination of protocol and hostname is a separate domain.
This can be a source of confusion.
Note that each of the following is its own domain:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">http://example.com</code>
</li>
<li class="listitem">
<code class="literal">https://example.com</code>
</li>
<li class="listitem">
<code class="literal">http://www.example.com</code>
</li>
<li class="listitem">
<code class="literal">https://www.example.com</code>
</li>
<li class="listitem">
<code class="literal">http://shop.example.com</code>
</li>
<li class="listitem">
<code class="literal">https://shop.example.com</code>
</li>
</ul>
</div>
<p>Each domain has:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
One or more entry points.
</li>
<li class="listitem">
One or more crawl rules.
</li>
<li class="listitem">
Zero or one <code class="literal">robots.txt</code> files.
</li>
<li class="listitem">
Zero or more sitemaps.
</li>
</ul>
</div>
<p>Manage the domains for a crawl in the Kibana UI.
Add your first domain on the getting started screen.
From there, you can view, add, manage, and delete domains.</p>
<h4><a id="crawler-managing-entry-points-sitemaps"></a>Entry points and sitemaps<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.4/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h4>
<h5><a id="crawler-managing-entry-points"></a>Entry points<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.4/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h5>
<p>Each domain must have at least one entry point.
Entry points are the paths from which the crawler will start each crawl.
Ensure entry points for each domain are allowed by the domain&#8217;s crawl rules, and the directives within the domain&#8217;s <code class="literal">robots.txt</code> file.
See <a class="xref" href="crawler-content.html#crawler-content-manage-robots-txt-files" title="robots.txt files"><code class="literal">robots.txt</code> files</a> to learn about managing <code class="literal">robots.txt</code> files.</p>
<p>Add multiple entries, if some pages are not discoverable from the first entry point.
For example, if your domain contains an “island” page that is not linked from other pages, simply add that full URL as an entry point.
If your domain has many pages that are not linked from other pages, it may be easier to reference them all via a sitemap.</p>
<h5><a id="crawler-managing-sitemaps"></a>Sitemaps<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.4/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h5>
<p>If the website you are crawling uses sitemaps, you can specify the sitemap URLs.
Note that you can choose to submit URLs to the web crawler using sitemaps, entry points, or a combination of both.</p>
<p>You can manage the sitemaps for a domain through the Kibana UI:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Navigate to <span class="strong strong"><strong>Enterprise Search &#8594; Content &#8594; Elasticsearch indices &#8594; <em>your-index</em> &#8594; Manage domains</strong></span>.
</li>
<li class="listitem">
Select a domain.
</li>
<li class="listitem">
Click <span class="strong strong"><strong>Add sitemap</strong></span>.
</li>
</ul>
</div>
<p>From here, you can view, add, edit, and delete sitemaps.
To add a sitemap to a domain you manage, you can specify it within a <code class="literal">robots.txt</code> file.
At the start of each crawl, the web crawler fetches and processes each domain&#8217;s <code class="literal">robots.txt</code> file and each sitemap specified within those files.</p>
<p>You may prefer to use sitemaps over entry points, because you have already published sitemaps for other web crawlers.</p>
<p>See <a class="xref" href="crawler-content.html#crawler-content-sitemap" title="Sitemaps">Sitemaps</a> if you are editing and managing sitemap source files.</p>
<h4><a id="crawler-managing-crawl-rules"></a>Crawl rules<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.4/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h4>
<p>A crawl rule is a crawler instruction to allow or disallow specific <em>paths</em> within a domain.
For a concrete example of crawl rules in action, see our <a class="xref" href="website-search-start.html" title="Getting started with website search">website search guide</a>.
Remember that <span class="strong strong"><strong>order matters</strong></span> and each URL is evaluated according to the first match.
The web crawler will crawl only those paths that are allowed by the crawl rules for the domain and the directives within the <code class="literal">robots.txt</code> file for the domain.
Ensure entry points for each domain are allowed.</p>
<div class="tip admon">
<div class="icon"></div>
<div class="admon_content">
<p>The web crawler will crawl only those paths that are allowed by the crawl rules for the domain <span class="strong strong"><strong><em>and</em></strong></span> the directives within the <code class="literal">robots.txt</code> file for the domain.
See <a class="xref" href="crawler-content.html#crawler-content-manage-robots-txt-files" title="robots.txt files"><code class="literal">robots.txt</code> files</a> to learn about using <code class="literal">robots.txt</code> files to allow/disallow paths.</p>
</div>
</div>
<h5><a id="crawler-managing-crawl-rules-logic"></a>Crawl rule logic (rules)<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.4/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h5>
<p>The logic for each rule is as follows:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
Begins with
</span>
</dt>
<dd>
<p>
The <em>path pattern</em> is a literal string <em>except</em> for the character <code class="literal">*</code>, which is a meta character that will match anything.
</p>
<p>The rule matches when the <em>path pattern</em> matches the <span class="strong strong"><strong>beginning</strong></span> of the <em>path</em> (which always begins with <code class="literal">/</code>).</p>
<p>If using this rule, begin your <em>path pattern</em> with <code class="literal">/</code>.</p>
</dd>
<dt>
<span class="term">
Ends with
</span>
</dt>
<dd>
<p>
The <em>path pattern</em> is a literal string <em>except</em> for the character <code class="literal">*</code>, which is a meta character that will match anything.
</p>
<p>The rule matches when the <em>path pattern</em> matches the <span class="strong strong"><strong>end</strong></span> of the <em>path</em>.</p>
</dd>
<dt>
<span class="term">
Contains
</span>
</dt>
<dd>
<p>
The <em>path pattern</em> is a literal string <em>except</em> for the character <code class="literal">*</code>, which is a meta character that will match anything.
</p>
<p>The rule matches when the <em>path pattern</em> matches anywhere <span class="strong strong"><strong>within</strong></span> the <em>path</em>.</p>
</dd>
<dt>
<span class="term">
Regex
</span>
</dt>
<dd>
<p>
The <em>path pattern</em> is a regular expression compatible with the Ruby language regular expression engine.
In addition to literal characters, the path pattern may include
<a href="https://ruby-doc.org/core-2.5.1/Regexp.html#class-Regexp-label-Metacharacters+and+Escapes" class="ulink" target="_blank" rel="noopener">metacharacters</a>, <a href="https://ruby-doc.org/core-2.5.1/Regexp.html#class-Regexp-label-Character+Classes" class="ulink" target="_blank" rel="noopener">character classes</a>, and <a href="https://ruby-doc.org/core-2.5.1/Regexp.html#class-Regexp-label-Repetition" class="ulink" target="_blank" rel="noopener">repetitions</a>.
You can test Ruby regular expressions using <a href="https://rubular.com" class="ulink" target="_blank" rel="noopener">Rubular</a>.
</p>
<p>The rule matches when the <em>path pattern</em> matches the <span class="strong strong"><strong>beginning</strong></span> of the <em>path</em> (which always begins with <code class="literal">/</code>).</p>
<p>If using this rule, begin your <em>path pattern</em> with <code class="literal">\/</code> or a metacharacter or character class that matches <code class="literal">/</code>.</p>
</dd>
</dl>
</div>
<h5><a id="crawler-managing-crawl-rules-examples"></a>Crawl rule matching<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.4/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h5>
<p>The following table provides various examples of crawl rule matching:</p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
<col class="col_4"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">URL path</th>
<th align="left" valign="top">Rule</th>
<th align="left" valign="top">Path pattern</th>
<th align="left" valign="top">Match?</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p><code class="literal">/foo/bar</code></p></td>
<td align="left" valign="top"><p><em>Begins with</em></p></td>
<td align="left" valign="top"><p><code class="literal">/foo</code></p></td>
<td align="left" valign="top"><p>YES</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/foo/bar</code></p></td>
<td align="left" valign="top"><p><em>Begins with</em></p></td>
<td align="left" valign="top"><p><code class="literal">/*oo</code></p></td>
<td align="left" valign="top"><p>YES</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/bar/foo</code></p></td>
<td align="left" valign="top"><p><em>Begins with</em></p></td>
<td align="left" valign="top"><p><code class="literal">/foo</code></p></td>
<td align="left" valign="top"><p>NO</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/foo/bar</code></p></td>
<td align="left" valign="top"><p><em>Begins with</em></p></td>
<td align="left" valign="top"><p><code class="literal">foo</code></p></td>
<td align="left" valign="top"><p>NO</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/blog/posts/hello-world</code></p></td>
<td align="left" valign="top"><p><em>Ends with</em></p></td>
<td align="left" valign="top"><p><code class="literal">world</code></p></td>
<td align="left" valign="top"><p>YES</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/blog/posts/hello-world</code></p></td>
<td align="left" valign="top"><p><em>Ends with</em></p></td>
<td align="left" valign="top"><p><code class="literal">hello-*</code></p></td>
<td align="left" valign="top"><p>YES</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/blog/world-hello</code></p></td>
<td align="left" valign="top"><p><em>Ends with</em></p></td>
<td align="left" valign="top"><p><code class="literal">world</code></p></td>
<td align="left" valign="top"><p>NO</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/blog/world-hello</code></p></td>
<td align="left" valign="top"><p><em>Ends with</em></p></td>
<td align="left" valign="top"><p><code class="literal">*world</code></p></td>
<td align="left" valign="top"><p>NO</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/fruits/bananas</code></p></td>
<td align="left" valign="top"><p><em>Contains</em></p></td>
<td align="left" valign="top"><p><code class="literal">banana</code></p></td>
<td align="left" valign="top"><p>YES</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/fruits/apples</code></p></td>
<td align="left" valign="top"><p><em>Contains</em></p></td>
<td align="left" valign="top"><p><code class="literal">banana</code></p></td>
<td align="left" valign="top"><p>NO</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/2020</code></p></td>
<td align="left" valign="top"><p><em>Regex</em></p></td>
<td align="left" valign="top"><p><code class="literal">\/[0-9]{3,5}</code></p></td>
<td align="left" valign="top"><p>YES</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/20</code></p></td>
<td align="left" valign="top"><p><em>Regex</em></p></td>
<td align="left" valign="top"><p><code class="literal">\/[0-9]{3,5}</code></p></td>
<td align="left" valign="top"><p>NO</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/2020</code></p></td>
<td align="left" valign="top"><p><em>Regex</em></p></td>
<td align="left" valign="top"><p><code class="literal">[0-9]{3,5}</code></p></td>
<td align="left" valign="top"><p>NO</p></td>
</tr>
</tbody>
</table>
</div>
<h5><a id="crawler-managing-crawl-rule-restricting-paths"></a>Restricting paths using crawl rules<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.4/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h5>
<p>The domain dashboard adds a default crawl rule to each domain: <code class="literal">Allow</code> if <code class="literal">Regex</code> <code class="literal">.*</code>.
You cannot delete or re-order this rule through the dashboard.</p>
<p>This rule is permissive, allowing all paths within the domain.
To restrict paths, use either of the following techniques:</p>
<p>Add rules that disallow specific paths (e.g. disallow the blog):</p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Policy</th>
<th align="left" valign="top">Rule</th>
<th align="left" valign="top">Path pattern</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p><code class="literal">Disallow</code></p></td>
<td align="left" valign="top"><p><code class="literal">Begins with</code></p></td>
<td align="left" valign="top"><p><code class="literal">/blog</code></p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">Allow</code></p></td>
<td align="left" valign="top"><p><code class="literal">Regex</code></p></td>
<td align="left" valign="top"><p><code class="literal">.*</code></p></td>
</tr>
</tbody>
</table>
</div>
<p>Or, add rules that allow specific paths and disallow all others (e.g. allow <em>only</em> the blog):</p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Policy</th>
<th align="left" valign="top">Rule</th>
<th align="left" valign="top">Path pattern</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p><code class="literal">Allow</code></p></td>
<td align="left" valign="top"><p><code class="literal">Begins with</code></p></td>
<td align="left" valign="top"><p><code class="literal">/blog</code></p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">Disallow</code></p></td>
<td align="left" valign="top"><p><code class="literal">Regex</code></p></td>
<td align="left" valign="top"><p><code class="literal">.*</code></p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">Allow</code></p></td>
<td align="left" valign="top"><p><code class="literal">Regex</code></p></td>
<td align="left" valign="top"><p><code class="literal">.*</code></p></td>
</tr>
</tbody>
</table>
</div>
<p>When you restrict a crawl to specific paths, be sure to add <a class="xref" href="crawler-managing.html#crawler-managing-entry-points" title="Entry points">entry points</a> that allow the crawler to discover those paths.
For example, if your crawl rules restrict the crawler to <code class="literal">/blog</code>, add <code class="literal">/blog</code> as an entry point.
If you leave only the default entry point <code class="literal">/</code>, the crawl will end immediately, since <code class="literal">/</code> is disallowed.</p>
<h4><a id="crawler-managing-duplicate-documents"></a>Duplicate document handling<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.4/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h4>
<p>By default, the web crawler identifies groups of duplicate web documents and stores each group as a single document in your index.
The document&#8217;s <code class="literal">url</code> and <code class="literal">additional_urls</code> fields represent all the URLs where the web crawler discovered the document’s content — or a sample of URLs if more than 100.
The <code class="literal">url</code> field represents the <em>canonical</em> URL, or the first discovered URL if no canonical URL is defined.
If you manage your site&#8217;s HTML source files, see <a class="xref" href="crawler-content.html#crawler-content-canonical-url-link-tag" title="Canonical URL link tags">Canonical URL link tags</a> to learn how to embed canonical URL link tag elements in pages that duplicate the content of other pages.</p>
<p>The crawler identifies duplicate content intelligently, ignoring insignificant differences such as navigation, whitespace, style, and scripts.
More specifically, the crawler combines the values of specific fields, and it hashes the result to create a unique "fingerprint" to represent the content of the web document.</p>
<p>The web crawler then checks your index for an existing document with the same content hash.
If it doesn’t find one, it saves a new document to the index.
If it does exist, the crawler updates the existing document instead of saving a new one.
The crawler adds the additional URL at which the content was discovered.</p>
<p>You can manage which fields the web crawler uses to create the content hash.
You can also disable this feature and allow duplicate documents.</p>
<p>Set the default fields for all domains using the following configuration setting: <code class="literal">connector.crawler.extraction.default_deduplication_fields</code>.</p>
<p>Manage these settings for each domain within the web crawler UI.</p>
<h5><a id="crawler-managing-duplicate-documents-manage"></a>Manage duplicate document handling<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.4/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h5>
<p>After extracting the content of a web document, the web crawler compares that content to your existing documents, to check for duplication.
To compare documents, the web crawler examines specific fields.</p>
<p>Manage these fields for each domain within the web crawler UI:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Navigate to <span class="strong strong"><strong>Enterprise Search → Content → Indices → <em>your-index-name</em> → domain name</strong></span>.
</li>
<li class="listitem">
Locate the section named <span class="strong strong"><strong>Duplicate document handling</strong></span>.
</li>
<li class="listitem">
Select or deselect the fields you’d like the crawler to use.
Alternatively, allow duplicate documents for a domain by deselecting <span class="strong strong"><strong>Prevent duplicate documents</strong></span>.
</li>
</ol>
</div>
<p>If you want to manage duplicate documents by editing your HTML content, see <a class="xref" href="crawler-content.html#crawler-content-canonical-url-link-tag" title="Canonical URL link tags">Canonical URL link tags</a>.</p>
<h4><a id="crawler-managing-binary-content"></a>Binary content extraction<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.4/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h4>
<p>The web crawler can extract content from downloadable binary files, such as PDF and DOCX files.
To use this feature, you must:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Enable binary content extraction with the configuration: <code class="literal">connector.crawler.content_extraction.enabled: true</code>.
</li>
<li class="listitem">
<p>Select which MIME types should have their contents extracted.
For example: <code class="literal">connector.crawler.content_extraction.mime_types: ["application/pdf", "application/msword"]</code>.</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
The MIME type is determined by the HTTP response&#8217;s <code class="literal">Content-Type</code> header when downloading a given file.
</li>
<li class="listitem">
While intended primarily for PDF and Microsoft Office formats, you can use any of the supported formats documented by <a href="https://tika.apache.org" class="ulink" target="_blank" rel="noopener">Apache Tika</a>.
</li>
<li class="listitem">
No default <code class="literal">mime_types</code> are defined.
You must configure at least one MIME type in order to extract non-HTML content.
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>The ingest attachment processor does not support compressed files, e.g., an archive file containing a set of PDFs.
Expand the archive file and make individual uncompressed files available for the web crawler to process.</p>
</div>
</div>
<p>Enterprise Search uses an <a href="/guide/en/elasticsearch/reference/8.4/ingest.html" class="ulink" target="_blank" rel="noopener">Elasticsearch ingest pipeline</a> to power the web crawler&#8217;s binary content extraction.
This pipeline, named <code class="literal">ent_search_crawler</code>, is automatically created when Enterprise Search first starts.</p>
<p>You can <a href="/guide/en/elasticsearch/reference/8.4/ingest.html#create-manage-ingest-pipelines" class="ulink" target="_blank" rel="noopener">view and update</a> this pipeline in Kibana or with Elasticsearch APIs.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>If you make changes to the default <code class="literal">ent_search_crawler</code> ingest pipeline, these will not be overwritten when you upgrade Enterprise Search, provided you have incremented its <code class="literal">version</code> above the upgrade version.</p>
</div>
</div>
<h4><a id="crawler-managing-manual-crawls"></a>Running manual crawls<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.4/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h4>
<p>Manual crawls are useful for testing and debugging the web crawler.
Your first crawl will be manual by default.</p>
<p>Other use cases for manual crawls include:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<span class="strong strong"><strong>Crawling content only once for a specific purpose</strong></span>: For example, crawling a website you don&#8217;t control to make it easier to search its pages.
</li>
<li class="listitem">
<span class="strong strong"><strong>Crawling content that changes infrequently</strong></span>: For example, it might make sense to only run manual crawls when content is updated.
</li>
<li class="listitem">
<span class="strong strong"><strong>Your team needs to closely manage usage costs</strong></span>: For example, you only run crawls when needed, such as after updating a website.
</li>
</ul>
</div>
<h5><a id="crawler-managing-manual-crawls-how-to"></a>How to run a manual crawl<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.4/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h5>
<p>To run a manual crawl, follow these steps in the web crawler UI:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Navigate to your crawler index in <span class="strong strong"><strong>Content</strong></span> &#8594; <span class="strong strong"><strong>Indices</strong></span> &#8594; <em>index-name</em>.
</li>
<li class="listitem">
Click on <span class="strong strong"><strong>Crawl</strong></span>.
</li>
<li class="listitem">
<p>You have 3 options for manual crawls:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<span class="strong strong"><strong>Crawl all domains on this index</strong></span>
</li>
<li class="listitem">
<span class="strong strong"><strong>Crawl with custom settings</strong></span>
</li>
<li class="listitem">
<span class="strong strong"><strong>Reapply crawl rules</strong></span>
</li>
</ul>
</div>
</li>
</ol>
</div>
<h5><a id="crawler-crawl-custom-settings"></a>Crawl with custom settings<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.4/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h5>
<p>Set up a one-time crawl with custom settings.
We recommend using this option for tests, because it allows you to further restrict which pages are crawled.</p>
<p><span class="strong strong"><strong>Crawl with custom settings</strong></span> gives you the option to:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<p>Set a maximum crawl depth, to specify how many pages deep the crawler traverses.</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Set the value to <code class="literal">1</code>, for example, to limit the crawl to only entry points.
</li>
</ul>
</div>
</li>
<li class="listitem">
Crawl select domains.
</li>
<li class="listitem">
Define seed URLs with sitemaps and entry points.
</li>
</ul>
</div>
<h5><a id="crawler-re-apply-crawl-rules"></a>Reapply crawl rules<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.4/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h5>
<p>If you&#8217;ve modified crawl rules, you can apply the updated rules to existing documents without running a full crawl.
The web crawler will remove all existing documents that are no longer allowed by your current crawl rules.
This operation is called a process crawl.</p>
<p>We recommend cancelling any active web crawls, before opting to re-apply crawl rules.
A web crawl running concurrently with a process crawl may continue to index fresh documents with out-of-date configuration.
Changes in crawl rule configuration will only apply to documents indexed at the time of the request.</p>
<h4><a id="crawler-managing-schedule"></a>Scheduling automated crawls<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.4/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h4>
<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<p>In 8.4.0, 8.4.1, and 8.4.2, the Elastic web crawler does not respect a crawl schedule, when configured.
Crawls must be manually triggered in the UI.</p>
</div>
</div>
<p>You can schedule new crawls to start automatically.
New crawls will be skipped if there is an active crawl.</p>
<p>For example, consider a crawl that completes on a Tuesday.
If the crawl is configured to run every 7 days, the next crawl would start on the following Tuesday.
If the crawl is configured to run every 3 days, then the next crawl would start on Friday.</p>
<p>Scheduling a crawl does not necessarily run the crawl immediately.</p>
<p>To manage automated crawls within the UI:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Navigate to your index and select the <span class="strong strong"><strong>Scheduling</strong></span> tab.
</li>
<li class="listitem">
Schedule crawls under <span class="strong strong"><strong>Automated Crawl Scheduling</strong></span>.
</li>
<li class="listitem">
Toggle <span class="strong strong"><strong>Crawl automatically</strong></span>.
</li>
<li class="listitem">
Set the crawl frequency.
</li>
<li class="listitem">
Save your settings.
</li>
</ol>
</div>
<p>The crawl schedule will perform a full crawl on every domain on this index.
Note that a scheduled crawl does not necessarily run immediately.</p>
<h4><a id="crawler-managing-next-steps"></a>Next steps<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.4/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h4>
<p>See <a class="xref" href="crawler-troubleshooting.html" title="Troubleshooting crawls">Troubleshooting crawls</a> to learn how to troubleshoot issues with your crawls.</p>
</div>
<div class="navfooter">
<span class="prev">
<a href="crawler.html">« Elastic web crawler</a>
</span>
<span class="next">
<a href="crawler-known-issues.html">Elastic web crawler known issues »</a>
</span>
</div>
</div>
</body>
</html>
