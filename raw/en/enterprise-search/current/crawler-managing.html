<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Managing crawls in Kibana | Enterprise Search documentation [8.15] | Elastic</title>
<meta class="elastic" name="content" content="Managing crawls in Kibana | Enterprise Search documentation [8.15]">

<link rel="home" href="index.html" title="Enterprise Search documentation [8.15]"/>
<link rel="up" href="crawler.html" title="Elastic web crawler"/>
<link rel="prev" href="crawler.html" title="Elastic web crawler"/>
<link rel="next" href="crawler-extraction-rules.html" title="Web crawler content extraction rules"/>
<meta class="elastic" name="product_version" content="8.15"/>
<meta class="elastic" name="product_name" content="Enterprise Search"/>
<meta class="elastic" name="website_area" content="documentation"/>
<meta name="DC.type" content="Learn/Docs/Enterprise Search/Guide/8.15"/>
<meta name="DC.subject" content="Enterprise Search"/>
<meta name="DC.identifier" content="8.15"/>
</head>
<body>
<div class="navheader">
<span class="prev">
<a href="crawler.html">« Elastic web crawler</a>
</span>
<span class="next">
<a href="crawler-extraction-rules.html">Web crawler content extraction rules »</a>
</span>
</div>
<div class="book" lang="en">
<div class="titlepage">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link">
<div id="related-products" class="dropdown">
<div class="related-products-title"></div>
<div class="dropdown-anchor" tabindex="0">Enterprise Search<span class="dropdown-icon"></span></div>
<div class="dropdown-content">
<ul>
<li class="dropdown-category">Enterprise Search guides</li>
<ul>
<li><a href="/guide/en/enterprise-search/current/index.html" target="_blank">Enterprise Search</a></li>
<li><a href="/guide/en/app-search/current/index.html" target="_blank">App Search</a></li>
<li><a href="/guide/en/workplace-search/current/index.html" target="_blank">Workplace Search</a></li>
</ul>
<li class="dropdown-category">Programming language clients</li>
<ul>
<li><a href="https://www.elastic.co/guide/en/enterprise-search-clients/enterprise-search-node/current/index.html" target="_blank">Node.js client</a></li>
<li><a href="https://www.elastic.co/guide/en/enterprise-search-clients/php/current/index.html" target="_blank">PHP client</a></li>
<li><a href="https://www.elastic.co/guide/en/enterprise-search-clients/python/current/index.html" target="_blank">Python client</a></li>
<li><a href="https://www.elastic.co/guide/en/enterprise-search-clients/ruby/current/index.html" target="_blank">Ruby client</a></li>
</ul>
</ul>
</div>
</div>
</span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="crawler.html">Elastic web crawler</a></span>
</div>
<div>
<div><h1 class="title"><a id="id-1"></a>Managing crawls in Kibana</h1><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.15/enterprise-search-docs/crawler-managing.asciidoc">edit</a></div>
</div>
<!--EXTRA-->
</div>
<div id="content">
<div class="chapter">
<div class="titlepage"><div><div>
<h2 class="title"><a id="crawler-managing"></a>Managing crawls in Kibana<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.15/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h2>
</div></div></div>

<p>This documentation contains all the information you need for managing crawls using the Kibana UI.</p>
<p>If you&#8217;d prefer to see a concrete crawler use case, see <a class="xref" href="website-search-start.html" title="Website search tutorial">Website search tutorial</a>.
If you need to learn how to optimize source files for the crawler, see <a class="xref" href="crawler-content.html" title="Optimizing web content for the web crawler"><em>Optimizing web content</em></a>.</p>
<h3><a id="crawler-managing-overview"></a>Overview<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.15/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h3>
<p>It&#8217;s important to understand the primary crawl management tools and how they influence your crawls:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<span class="strong strong"><strong>Domains</strong></span> set crawl boundaries.
</li>
<li class="listitem">
<span class="strong strong"><strong>Entry points</strong></span> and <span class="strong strong"><strong>Sitemaps</strong></span> set starting points within domains.
</li>
<li class="listitem">
<span class="strong strong"><strong>Crawl rules</strong></span> and <code class="literal">robots.txt</code> directives set additional rules for crawling, beyond the starting points.
</li>
</ul>
</div>
<p>Here you&#8217;ll learn about discovering content, extracting content, crawling manually and scheduling crawls.</p>
<p><span class="strong strong"><strong>Discovering content</strong></span>:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="crawler-managing.html#crawler-managing-domains" title="Domains">Domains</a>
</li>
<li class="listitem">
<a class="xref" href="crawler-managing.html#crawler-managing-entry-points-sitemaps" title="Entry points and sitemaps">Entry points and sitemaps</a>
</li>
<li class="listitem">
<a class="xref" href="crawler-managing.html#crawler-managing-authentication" title="Authentication">Authentication</a>
</li>
<li class="listitem">
<a class="xref" href="crawler-managing.html#crawler-managing-crawl-rules" title="Crawl rules">Crawl rules</a>
</li>
<li class="listitem">
<a class="xref" href="crawler-managing.html#crawler-managing-user-agent" title="User Agent">User Agent</a>
</li>
</ul>
</div>
<p><span class="strong strong"><strong>Extracting content</strong></span>:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="crawler-managing.html#crawler-managing-duplicate-documents" title="Duplicate document handling">Duplicate document handling</a>
</li>
<li class="listitem">
<a class="xref" href="crawler-managing.html#crawler-managing-binary-content" title="Binary content extraction">Binary content extraction</a>
</li>
<li class="listitem">
<a class="xref" href="crawler-managing.html#crawler-managing-html-storage" title="Storing full HTML">Storing full HTML</a>
</li>
<li class="listitem">
<a class="xref" href="crawler-managing.html#crawler-managing-extraction-rules" title="Content extraction rules">Content extraction rules</a>
</li>
<li class="listitem">
<a class="xref" href="crawler-managing.html#crawler-managing-ingest-pipelines" title="Manage ingest pipelines">Manage ingest pipelines</a>
</li>
</ul>
</div>
<p><span class="strong strong"><strong>Running manual crawls</strong></span>:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="crawler-managing.html#crawler-managing-manual-crawls" title="Running manual crawls">Running manual crawls</a>
</li>
</ul>
</div>
<p><span class="strong strong"><strong>Scheduling automated crawls</strong></span>:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="crawler-managing.html#crawler-managing-schedule" title="Scheduling automated crawls">Scheduling automated crawls</a>
</li>
<li class="listitem">
<a class="xref" href="crawler-managing.html#crawler-managing-multiple-schedules" title="Scheduling multiple crawls">Scheduling multiple crawls</a>
</li>
</ul>
</div>
<h3><a id="crawler-managing-domains"></a>Domains<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.15/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h3>
<p>A domain is a website or property you&#8217;d like to crawl.
You must associate one or more domains to your index&#8217;s web crawler.
The web crawler cannot discover and index content outside of specified domains.</p>
<p>Each domain has a <em>domain URL</em> that identifies the domain using a protocol and hostname.
The domain URL can not include a path. If a path is provided, it will automatically be removed from the domain URL, and instead added as an entry point.</p>
<p>Each unique combination of protocol and hostname is a separate domain.
This can be a source of confusion.
Note that each of the following is its own domain:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">http://example.com</code>
</li>
<li class="listitem">
<code class="literal">https://example.com</code>
</li>
<li class="listitem">
<code class="literal">http://www.example.com</code>
</li>
<li class="listitem">
<code class="literal">https://www.example.com</code>
</li>
<li class="listitem">
<code class="literal">http://shop.example.com</code>
</li>
<li class="listitem">
<code class="literal">https://shop.example.com</code>
</li>
</ul>
</div>
<p>Each domain has:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
One or more entry points.
</li>
<li class="listitem">
One or more crawl rules.
</li>
<li class="listitem">
Zero or one <code class="literal">robots.txt</code> files.
</li>
<li class="listitem">
Zero or more sitemaps.
</li>
</ul>
</div>
<p>Manage the domains for a crawl in the Kibana UI.
Add your first domain on the getting started screen.
From there, you can view, add, manage, and delete domains.</p>
<h3><a id="crawler-managing-entry-points-sitemaps"></a>Entry points and sitemaps<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.15/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h3>
<h4><a id="crawler-managing-entry-points"></a>Entry points<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.15/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h4>
<p>Each domain must have at least one entry point.
Entry points are the paths from which the crawler will start each crawl.
Ensure entry points for each domain are allowed by the domain&#8217;s crawl rules, and the directives within the domain&#8217;s <code class="literal">robots.txt</code> file.
See <a class="xref" href="crawler-content.html#crawler-content-manage-robots-txt-files" title="robots.txt files"><code class="literal">robots.txt</code> files</a> to learn about managing <code class="literal">robots.txt</code> files.</p>
<p>Add multiple entries, if some pages are not discoverable from the first entry point.
For example, if your domain contains an “island” page that is not linked from other pages, simply add that full URL as an entry point.
If your domain has many pages that are not linked from other pages, it may be easier to reference them all via a sitemap.</p>
<h4><a id="crawler-managing-sitemaps"></a>Sitemaps<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.15/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h4>
<p>If the website you are crawling uses sitemaps, you can specify the sitemap URLs.
Note that you can choose to submit URLs to the web crawler using sitemaps, entry points, or a combination of both.</p>
<p>You can manage the sitemaps for a domain through the Kibana UI:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Navigate to <span class="strong strong"><strong>Search &#8594; Content &#8594; Elasticsearch indices &#8594; <em>your-index</em> &#8594; Manage domains</strong></span>.
</li>
<li class="listitem">
Select a domain.
</li>
<li class="listitem">
Click <span class="strong strong"><strong>Add sitemap</strong></span>.
</li>
</ul>
</div>
<p>From here, you can view, add, edit, and delete sitemaps.
To add a sitemap to a domain you manage, you can specify it within a <code class="literal">robots.txt</code> file.
At the start of each crawl, the web crawler fetches and processes each domain&#8217;s <code class="literal">robots.txt</code> file and each sitemap specified within those files.</p>
<p>You may prefer to use sitemaps over entry points, because you have already published sitemaps for other web crawlers.</p>
<p>See <a class="xref" href="crawler-content.html#crawler-content-sitemap" title="Sitemaps">Sitemaps</a> if you are editing and managing sitemap source files.</p>
<h3><a id="crawler-managing-authentication"></a>Authentication<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.15/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h3>
<p>If the domain you are crawling has pages that require authentication, you can manage the authentication settings in the Kibana UI.
The web crawler supports two authentication methods:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Basic authentication (username and password)
</li>
<li class="listitem">
Authentication header (e.g. bearer tokens)
</li>
</ol>
</div>
<p>Follow these steps to add authentication information in the Kibana UI:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Navigate to <span class="strong strong"><strong>Search &#8594; Content &#8594; Elasticsearch indices &#8594; <em>your-index</em> &#8594; Manage domains</strong></span>.
</li>
<li class="listitem">
Select a domain.
</li>
<li class="listitem">
Click <span class="strong strong"><strong>Add credentials</strong></span>.
</li>
<li class="listitem">
Add the <span class="strong strong"><strong>Basic authentication</strong></span> username/password or <span class="strong strong"><strong>Authentication header</strong></span>.
</li>
<li class="listitem">
Click <span class="strong strong"><strong>Save</strong></span>.
</li>
</ul>
</div>
<p>You can only save one authentication method per domain.</p>
<div class="exampleblock">
<div class="content">
<p>We do not currently support form based authentication.</p>
</div>
</div>
<h3><a id="crawler-managing-crawl-rules"></a>Crawl rules<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.15/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h3>
<p>A crawl rule is a crawler instruction to allow or disallow specific <em>paths</em> within a domain.
For a concrete example of crawl rules in action, see our <a class="xref" href="website-search-start.html" title="Website search tutorial">website search guide</a>.
Remember that <span class="strong strong"><strong>order matters</strong></span> and each URL is evaluated according to the first match.
The web crawler will crawl only those paths that are allowed by the crawl rules for the domain and the directives within the <code class="literal">robots.txt</code> file for the domain.
Ensure entry points for each domain are allowed.</p>
<div class="tip admon">
<div class="icon"></div>
<div class="admon_content">
<p>The web crawler will crawl only those paths that are allowed by the crawl rules for the domain <span class="strong strong"><strong><em>and</em></strong></span> the directives within the <code class="literal">robots.txt</code> file for the domain.
See <a class="xref" href="crawler-content.html#crawler-content-manage-robots-txt-files" title="robots.txt files"><code class="literal">robots.txt</code> files</a> to learn about using <code class="literal">robots.txt</code> files to allow/disallow paths.</p>
</div>
</div>
<h4><a id="crawler-managing-crawl-rules-logic"></a>Crawl rule logic (rules)<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.15/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h4>
<p>The logic for each rule is as follows:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
Begins with
</span>
</dt>
<dd>
<p>
The <em>path pattern</em> is a literal string <em>except</em> for the character <code class="literal">*</code>, which is a meta character that will match anything.
</p>
<p>The rule matches when the <em>path pattern</em> matches the <span class="strong strong"><strong>beginning</strong></span> of the <em>path</em> (which always begins with <code class="literal">/</code>).</p>
<p>If using this rule, begin your <em>path pattern</em> with <code class="literal">/</code>.</p>
</dd>
<dt>
<span class="term">
Ends with
</span>
</dt>
<dd>
<p>
The <em>path pattern</em> is a literal string <em>except</em> for the character <code class="literal">*</code>, which is a meta character that will match anything.
</p>
<p>The rule matches when the <em>path pattern</em> matches the <span class="strong strong"><strong>end</strong></span> of the <em>path</em>.</p>
</dd>
<dt>
<span class="term">
Contains
</span>
</dt>
<dd>
<p>
The <em>path pattern</em> is a literal string <em>except</em> for the character <code class="literal">*</code>, which is a meta character that will match anything.
</p>
<p>The rule matches when the <em>path pattern</em> matches anywhere <span class="strong strong"><strong>within</strong></span> the <em>path</em>.</p>
</dd>
<dt>
<span class="term">
Regex
</span>
</dt>
<dd>
<p>
The <em>path pattern</em> is a regular expression compatible with the Ruby language regular expression engine.
In addition to literal characters, the path pattern may include
<a href="https://ruby-doc.org/core-2.5.1/Regexp.html#class-Regexp-label-Metacharacters+and+Escapes" class="ulink" target="_blank" rel="noopener">metacharacters</a>, <a href="https://ruby-doc.org/core-2.5.1/Regexp.html#class-Regexp-label-Character+Classes" class="ulink" target="_blank" rel="noopener">character classes</a>, and <a href="https://ruby-doc.org/core-2.5.1/Regexp.html#class-Regexp-label-Repetition" class="ulink" target="_blank" rel="noopener">repetitions</a>.
You can test Ruby regular expressions using <a href="https://rubular.com" class="ulink" target="_blank" rel="noopener">Rubular</a>.
</p>
<p>The rule matches when the <em>path pattern</em> matches the <span class="strong strong"><strong>beginning</strong></span> of the <em>path</em> (which always begins with <code class="literal">/</code>).</p>
<p>If using this rule, begin your <em>path pattern</em> with <code class="literal">\/</code> or a metacharacter or character class that matches <code class="literal">/</code>.</p>
</dd>
</dl>
</div>
<h4><a id="crawler-managing-crawl-rules-examples"></a>Crawl rule matching<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.15/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h4>
<p>The following table provides various examples of crawl rule matching:</p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
<col class="col_4"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">URL path</th>
<th align="left" valign="top">Rule</th>
<th align="left" valign="top">Path pattern</th>
<th align="left" valign="top">Match?</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p><code class="literal">/foo/bar</code></p></td>
<td align="left" valign="top"><p><em>Begins with</em></p></td>
<td align="left" valign="top"><p><code class="literal">/foo</code></p></td>
<td align="left" valign="top"><p>YES</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/foo/bar</code></p></td>
<td align="left" valign="top"><p><em>Begins with</em></p></td>
<td align="left" valign="top"><p><code class="literal">/*oo</code></p></td>
<td align="left" valign="top"><p>YES</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/bar/foo</code></p></td>
<td align="left" valign="top"><p><em>Begins with</em></p></td>
<td align="left" valign="top"><p><code class="literal">/foo</code></p></td>
<td align="left" valign="top"><p>NO</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/foo/bar</code></p></td>
<td align="left" valign="top"><p><em>Begins with</em></p></td>
<td align="left" valign="top"><p><code class="literal">foo</code></p></td>
<td align="left" valign="top"><p>NO</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/blog/posts/hello-world</code></p></td>
<td align="left" valign="top"><p><em>Ends with</em></p></td>
<td align="left" valign="top"><p><code class="literal">world</code></p></td>
<td align="left" valign="top"><p>YES</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/blog/posts/hello-world</code></p></td>
<td align="left" valign="top"><p><em>Ends with</em></p></td>
<td align="left" valign="top"><p><code class="literal">hello-*</code></p></td>
<td align="left" valign="top"><p>YES</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/blog/world-hello</code></p></td>
<td align="left" valign="top"><p><em>Ends with</em></p></td>
<td align="left" valign="top"><p><code class="literal">world</code></p></td>
<td align="left" valign="top"><p>NO</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/blog/world-hello</code></p></td>
<td align="left" valign="top"><p><em>Ends with</em></p></td>
<td align="left" valign="top"><p><code class="literal">*world</code></p></td>
<td align="left" valign="top"><p>NO</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/fruits/bananas</code></p></td>
<td align="left" valign="top"><p><em>Contains</em></p></td>
<td align="left" valign="top"><p><code class="literal">banana</code></p></td>
<td align="left" valign="top"><p>YES</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/fruits/apples</code></p></td>
<td align="left" valign="top"><p><em>Contains</em></p></td>
<td align="left" valign="top"><p><code class="literal">banana</code></p></td>
<td align="left" valign="top"><p>NO</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/2020</code></p></td>
<td align="left" valign="top"><p><em>Regex</em></p></td>
<td align="left" valign="top"><p><code class="literal">\/[0-9]{3,5}</code></p></td>
<td align="left" valign="top"><p>YES</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/20</code></p></td>
<td align="left" valign="top"><p><em>Regex</em></p></td>
<td align="left" valign="top"><p><code class="literal">\/[0-9]{3,5}</code></p></td>
<td align="left" valign="top"><p>NO</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">/2020</code></p></td>
<td align="left" valign="top"><p><em>Regex</em></p></td>
<td align="left" valign="top"><p><code class="literal">[0-9]{3,5}</code></p></td>
<td align="left" valign="top"><p>NO</p></td>
</tr>
</tbody>
</table>
</div>
<h4><a id="crawler-managing-crawl-rule-restricting-paths"></a>Restricting paths using crawl rules<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.15/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h4>
<p>The domain dashboard adds a default crawl rule to each domain: <code class="literal">Allow</code> if <code class="literal">Regex</code> <code class="literal">.*</code>.
You cannot delete or re-order this rule through the dashboard.</p>
<p>This rule is permissive, allowing all paths within the domain.
To restrict paths, use either of the following techniques:</p>
<p>Add rules that disallow specific paths (e.g. disallow the blog):</p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Policy</th>
<th align="left" valign="top">Rule</th>
<th align="left" valign="top">Path pattern</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p><code class="literal">Disallow</code></p></td>
<td align="left" valign="top"><p><code class="literal">Begins with</code></p></td>
<td align="left" valign="top"><p><code class="literal">/blog</code></p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">Allow</code></p></td>
<td align="left" valign="top"><p><code class="literal">Regex</code></p></td>
<td align="left" valign="top"><p><code class="literal">.*</code></p></td>
</tr>
</tbody>
</table>
</div>
<p>Or, add rules that allow specific paths and disallow all others (e.g. allow <em>only</em> the blog):</p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Policy</th>
<th align="left" valign="top">Rule</th>
<th align="left" valign="top">Path pattern</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p><code class="literal">Allow</code></p></td>
<td align="left" valign="top"><p><code class="literal">Begins with</code></p></td>
<td align="left" valign="top"><p><code class="literal">/blog</code></p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">Disallow</code></p></td>
<td align="left" valign="top"><p><code class="literal">Regex</code></p></td>
<td align="left" valign="top"><p><code class="literal">.*</code></p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">Allow</code></p></td>
<td align="left" valign="top"><p><code class="literal">Regex</code></p></td>
<td align="left" valign="top"><p><code class="literal">.*</code></p></td>
</tr>
</tbody>
</table>
</div>
<p>When you restrict a crawl to specific paths, be sure to add <a class="xref" href="crawler-managing.html#crawler-managing-entry-points" title="Entry points">entry points</a> that allow the crawler to discover those paths.
For example, if your crawl rules restrict the crawler to <code class="literal">/blog</code>, add <code class="literal">/blog</code> as an entry point.
If you leave only the default entry point <code class="literal">/</code>, the crawl will end immediately, since <code class="literal">/</code> is disallowed.</p>
<h3><a id="crawler-managing-user-agent"></a>User Agent<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.15/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h3>
<p>The User Agent is a request header that allows websites to identify the request sender.
The default User Agent for the Elastic web crawler  is <code class="literal">Elastic-Crawler (&lt;crawler_version_number&gt;)</code>.
For example, in version <code class="literal">8.6.0</code> the User Agent is <code class="literal">Elastic-Crawler (8.6.0)</code>.
Every request sent by the Elastic crawler will contain this header.</p>
<p>The User Agent header can be changed in the <code class="literal">enterprise-search.yml</code> file.
See <a class="xref" href="configuration.html#configuration-settings-elastic-crawler-http-user-agent">Elastic crawler configuration settings</a> for more information.</p>
<h3><a id="crawler-managing-duplicate-documents"></a>Duplicate document handling<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.15/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h3>
<p>By default, the web crawler identifies groups of duplicate web documents and stores each group as a single document in your index.
The document&#8217;s <code class="literal">url</code> and <code class="literal">additional_urls</code> fields represent all the URLs where the web crawler discovered the document’s content — or a sample of URLs if more than 100.
The <code class="literal">url</code> field represents the <em>canonical</em> URL, or the first discovered URL if no canonical URL is defined.
If you manage your site&#8217;s HTML source files, see <a class="xref" href="crawler-content.html#crawler-content-canonical-url-link-tag" title="Canonical URL link tags">Canonical URL link tags</a> to learn how to embed canonical URL link tag elements in pages that duplicate the content of other pages.</p>
<p>The crawler identifies duplicate content intelligently, ignoring insignificant differences such as navigation, whitespace, style, and scripts.
More specifically, the crawler combines the values of specific fields, and it hashes the result to create a unique "fingerprint" to represent the content of the web document.</p>
<p>The web crawler then checks your index for an existing document with the same content hash.
If it doesn’t find one, it saves a new document to the index.
If it does exist, the crawler updates the existing document instead of saving a new one.
The crawler adds the additional URL at which the content was discovered.</p>
<p>You can manage which fields the web crawler uses to create the content hash.
You can also disable this feature and allow duplicate documents.</p>
<p>Set the default fields for all domains using the following configuration setting: <code class="literal">connector.crawler.extraction.default_deduplication_fields</code>.</p>
<p>Manage these settings for each domain within the web crawler UI.</p>
<h4><a id="crawler-managing-duplicate-documents-manage"></a>Manage duplicate document handling<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.15/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h4>
<p>After extracting the content of a web document, the web crawler compares that content to your existing documents, to check for duplication.
To compare documents, the web crawler examines specific fields.</p>
<p>Manage these fields for each domain within the web crawler UI:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Navigate to <span class="strong strong"><strong>Search → Content → Indices → <em>your-index-name</em> → domain name</strong></span>.
</li>
<li class="listitem">
Locate the section named <span class="strong strong"><strong>Duplicate document handling</strong></span>.
</li>
<li class="listitem">
Select or deselect the fields you’d like the crawler to use.
Alternatively, allow duplicate documents for a domain by deselecting <span class="strong strong"><strong>Prevent duplicate documents</strong></span>.
</li>
</ol>
</div>
<p>If you want to manage duplicate documents by editing your HTML content, see <a class="xref" href="crawler-content.html#crawler-content-canonical-url-link-tag" title="Canonical URL link tags">Canonical URL link tags</a>.</p>
<h3><a id="crawler-managing-binary-content"></a>Binary content extraction<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.15/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h3>
<p>The web crawler can extract content from downloadable binary files, such as PDF and DOCX files.
To use this feature, you must:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Enable binary content extraction with the configuration: <code class="literal">connector.crawler.content_extraction.enabled: true</code>.
</li>
<li class="listitem">
<p>Select which MIME types should have their contents extracted.
For example: <code class="literal">connector.crawler.content_extraction.mime_types: ["application/pdf", "application/msword"]</code>.</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
The MIME type is determined by the HTTP response&#8217;s <code class="literal">Content-Type</code> header when downloading a given file.
</li>
<li class="listitem">
While intended primarily for PDF and Microsoft Office formats, you can use any of the supported formats documented by <a href="https://tika.apache.org" class="ulink" target="_blank" rel="noopener">Apache Tika</a>.
</li>
<li class="listitem">
No default <code class="literal">mime_types</code> are defined.
You must configure at least one MIME type in order to extract non-HTML content.
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>The ingest attachment processor does not support compressed files, e.g., an archive file containing a set of PDFs.
Expand the archive file and make individual uncompressed files available for the web crawler to process.</p>
</div>
</div>
<p>Enterprise Search uses an <a href="/guide/en/elasticsearch/reference/8.15/ingest.html" class="ulink" target="_blank" rel="noopener">Elasticsearch ingest pipeline</a> to power the web crawler&#8217;s binary content extraction.
The default pipeline, <code class="literal">ent-search-generic-ingestion</code> (<code class="literal">ent_search_crawler</code> before 8.5), is automatically created when Enterprise Search first starts.</p>
<p>You can <a href="/guide/en/elasticsearch/reference/8.15/ingest.html#create-manage-ingest-pipelines" class="ulink" target="_blank" rel="noopener">view</a> this pipeline in Kibana.
Customizing your pipeline usage is also an option.
See <a href="/guide/en/elasticsearch/reference/8.15/ingest-pipeline-search.html" class="ulink" target="_top">Ingest pipelines for Search indices</a>.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>If you make changes to the default <code class="literal">ent_search_crawler</code> ingest pipeline, these will not be overwritten when you upgrade Enterprise Search, provided you have incremented its <code class="literal">version</code> above the upgrade version.</p>
</div>
</div>
<p>See <a class="xref" href="crawler-managing.html#crawler-managing-ingest-pipelines" title="Manage ingest pipelines">Manage ingest pipelines</a> on this page for more information.</p>
<h3><a id="crawler-managing-html-storage"></a>Storing full HTML<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.15/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h3>
<p>You can enable your crawler to save the full HTML of pages that it indexes.
The full HTML value will be saved under the field <code class="literal">full_html</code> in the <a class="xref" href="crawler-schema.html" title="Web crawler schema"><em>Web crawler schema</em></a> as a string.</p>
<p>Each crawler index can have this setting toggled on or off.
This setting is off by default.</p>
<p>In Kibana, navigate to:</p>
<p><span class="strong strong"><strong>Search → Content → Elasticsearch indices</strong></span></p>
<p>Then choose the index to manage and choose the <span class="strong strong"><strong>Configuration</strong></span> tab.
Within that tab, toggle the <code class="literal">Store full HTML</code> on or off.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>Enabling full HTML extraction can dramatically increase the index size if the site being crawled is large.</p>
</div>
</div>
<h3><a id="crawler-managing-extraction-rules"></a>Content extraction rules<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.15/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h3>
<p>See <a class="xref" href="crawler-extraction-rules.html" title="Web crawler content extraction rules"><em>Content extraction rules</em></a>.</p>
<h3><a id="crawler-managing-ingest-pipelines"></a>Manage ingest pipelines<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.15/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h3>
<p>The web crawler extracts webpage content and transforms it into fields in Elasticsearch documents, according to the <a class="xref" href="crawler-schema.html" title="Web crawler schema"><em>Web crawler schema</em></a>.
However, you can use <span class="strong strong"><strong>ingest pipelines</strong></span> to perform additional processing and transformation on each document, before it is written to Elasticsearch.</p>
<p>Each crawler index has a default ingest pipeline, which you can customize or replace through Kibana.</p>
<p>In Kibana, navigate to:</p>
<p><span class="strong strong"><strong>Search → Content → Elasticsearch indices</strong></span></p>
<p>Then choose the index to manage and choose the <span class="strong strong"><strong>Pipelines</strong></span> tab.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>This functionality applies to all search indices, including crawler and API indices.
Refer to the following documentation for further details: <a href="/guide/en/elasticsearch/reference/8.15/ingest-pipeline-search.html" class="ulink" target="_top">Ingest pipelines for Search indices</a>.</p>
</div>
</div>
<h3><a id="crawler-managing-manual-crawls"></a>Running manual crawls<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.15/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h3>
<p>Manual crawls are useful for testing and debugging the web crawler.
Your first crawl will be manual by default.</p>
<p>Other use cases for manual crawls include:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<span class="strong strong"><strong>Crawling content only once for a specific purpose</strong></span>: For example, crawling a website you don&#8217;t control to make it easier to search its pages.
</li>
<li class="listitem">
<span class="strong strong"><strong>Crawling content that changes infrequently</strong></span>: For example, it might make sense to only run manual crawls when content is updated.
</li>
<li class="listitem">
<span class="strong strong"><strong>Your team needs to closely manage usage costs</strong></span>: For example, you only run crawls when needed, such as after updating a website.
</li>
</ul>
</div>
<h4><a id="crawler-managing-manual-crawls-how-to"></a>How to run a manual crawl<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.15/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h4>
<p>To run a manual crawl, follow these steps in the web crawler UI:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Navigate to your crawler index in <span class="strong strong"><strong>Content</strong></span> &#8594; <span class="strong strong"><strong>Indices</strong></span> &#8594; <em>index-name</em>.
</li>
<li class="listitem">
Click on <span class="strong strong"><strong>Crawl</strong></span>.
</li>
<li class="listitem">
<p>You have 3 options for manual crawls:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<span class="strong strong"><strong>Crawl all domains on this index</strong></span>
</li>
<li class="listitem">
<span class="strong strong"><strong>Crawl with custom settings</strong></span>
</li>
<li class="listitem">
<span class="strong strong"><strong>Reapply crawl rules</strong></span>
</li>
</ul>
</div>
</li>
</ol>
</div>
<h4><a id="crawler-crawl-custom-settings"></a>Crawl with custom settings<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.15/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h4>
<p>Set up a one-time crawl with custom settings.
We recommend using this option for tests, because it allows you to further restrict which pages are crawled.</p>
<p><span class="strong strong"><strong>Crawl with custom settings</strong></span> gives you the option to:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<p>Set a maximum crawl depth, to specify how many pages deep the crawler traverses.</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Set the value to <code class="literal">1</code>, for example, to limit the crawl to only entry points.
</li>
</ul>
</div>
</li>
<li class="listitem">
Crawl select domains.
</li>
<li class="listitem">
Define seed URLs with sitemaps and entry points.
</li>
</ul>
</div>
<h4><a id="crawler-re-apply-crawl-rules"></a>Reapply crawl rules<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.15/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h4>
<p>If you&#8217;ve modified crawl rules, you can apply the updated rules to existing documents without running a full crawl.
The web crawler will remove all existing documents that are no longer allowed by your current crawl rules.
This operation is called a process crawl.</p>
<p>We recommend cancelling any active web crawls, before opting to re-apply crawl rules.
A web crawl running concurrently with a process crawl may continue to index fresh documents with out-of-date configuration.
Changes in crawl rule configuration will only apply to documents indexed at the time of the request.</p>
<h3><a id="crawler-managing-schedule"></a>Scheduling automated crawls<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.15/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h3>
<p>You can schedule new crawls to start automatically.
New crawls will be skipped if there is an active crawl.</p>
<p>To manage automated crawls within the UI:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Navigate to your index and select the <span class="strong strong"><strong>Scheduling</strong></span> tab.
</li>
<li class="listitem">
Toggle <span class="strong strong"><strong>Enable recurring crawls with the following schedule</strong></span>.
</li>
<li class="listitem">
<p>Set up your desired scheduling type from the two options.</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
<a class="xref" href="crawler-managing.html#crawler-managing-schedule-interval" title="Interval scheduling">Interval scheduling</a>.
</li>
<li class="listitem">
<a class="xref" href="crawler-managing.html#crawler-managing-schedule-specific-time" title="Specific-time scheduling">Specific-time scheduling</a>.
</li>
</ol>
</div>
</li>
<li class="listitem">
Save your settings.
</li>
</ol>
</div>
<p>The crawl schedule will perform a full crawl on every domain on this index.</p>
<p>Here&#8217;s what the scheduling options look like in the Kibana UI:</p>
<div class="imageblock">
<div class="content">
<img src="images/crawler-scheduling.png" alt="Crawler scheduling UI">
</div>
</div>
<h4><a id="crawler-managing-schedule-interval"></a>Interval scheduling<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.15/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h4>
<p>Use interval schedules to automatically launch crawls at a set interval after the previous crawl completed.
For example, consider a crawl that completes on a Tuesday.
If the crawl is configured to run every 7 days, the next crawl would start on the following Tuesday.
If the crawl is configured to run every 3 days, then the next crawl would start on Friday.</p>
<p>Scheduling an interval crawl does not necessarily run the crawl immediately.</p>
<h4><a id="crawler-managing-schedule-specific-time"></a>Specific-time scheduling<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.15/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h4>
<p>Use specific-time schedules to run crawls at a specific time.
Specific-time schedules do not consider the duration of the previous crawl.
If the previous crawl is still running, then the crawler will not run again until the next scheduled time.</p>
<p>Just like interval scheduling, scheduling a specific-time crawl does not necessarily run the crawl immediately.</p>
<h3><a id="crawler-managing-multiple-schedules"></a>Scheduling multiple crawls<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.15/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h3>
<h4><a id="crawler-managing-multiple-schedules-kibana-ui"></a>Schedule multiple crawls in Kibana UI<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.15/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h4>
<p>To manage multiple custom crawls within the UI:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Navigate to your crawler index
</li>
<li class="listitem">
In the top-right corner click <span class="strong strong"><strong>Crawl</strong></span> and select <span class="strong strong"><strong>Crawl with custom settings</strong></span> from the dropdown
</li>
<li class="listitem">
Select crawl type to <span class="strong strong"><strong>Multiple crawls</strong></span>
</li>
</ol>
</div>
<div class="imageblock screenshot">
<div class="content">
<img src="images/crawler-scheduling-multiple-crawls.png" alt="Scheduling multiple crawls UI">
</div>
</div>
<p>In the Multiple crawls panel you can:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Add and delete custom crawls
</li>
<li class="listitem">
Set a max crawl depth
</li>
<li class="listitem">
Select the domains, sitemaps and entrypoint URLs
</li>
<li class="listitem">
Manage the specific-time scheduling
</li>
</ol>
</div>
<h3><a id="crawler-managing-next-steps"></a>Next steps<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/8.15/enterprise-search-docs/crawler-managing.asciidoc">edit</a></h3>
<p>See <a class="xref" href="crawler-troubleshooting.html" title="Troubleshooting crawls"><em>Troubleshooting crawls</em></a> to learn how to troubleshoot issues with your crawls.</p>
</div>
</div>
</div><div class="navfooter">
<span class="prev">
<a href="crawler.html">« Elastic web crawler</a>
</span>
<span class="next">
<a href="crawler-extraction-rules.html">Web crawler content extraction rules »</a>
</span>
</div>
</body>
</html>
