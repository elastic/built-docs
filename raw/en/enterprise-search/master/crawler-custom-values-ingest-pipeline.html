<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Customize crawler field values using an ingest pipeline | Elastic Enterprise Search documentation [master] | Elastic</title>
<meta class="elastic" name="content" content="Customize crawler field values using an ingest pipeline | Elastic Enterprise Search documentation [master]">

<link rel="home" href="index.html" title="Elastic Enterprise Search documentation [master]"/>
<link rel="up" href="crawler.html" title="Elastic web crawler"/>
<link rel="prev" href="crawler-content.html" title="Optimizing web content for the web crawler"/>
<link rel="next" href="crawler-custom-fields-proxy.html" title="Extract custom fields using web crawler and proxy"/>
<meta class="elastic" name="product_version" content="master"/>
<meta class="elastic" name="product_name" content="Enterprise Search"/>
<meta class="elastic" name="website_area" content="documentation"/>
<meta name="DC.type" content="Learn/Docs/Enterprise Search/Guide/master"/>
<meta name="DC.subject" content="Enterprise Search"/>
<meta name="DC.identifier" content="master"/>
</head>
<body><div class="page_header">
You are looking at preliminary documentation for a future release.
Not what you want? See the
<a href="../current/index.html">current release documentation</a>.
</div>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="index.html">Elastic Enterprise Search documentation [master]</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="crawler.html">Elastic web crawler</a></span>
</div>
<div class="navheader">
<span class="prev">
<a href="crawler-content.html">« Optimizing web content for the web crawler</a>
</span>
<span class="next">
<a href="crawler-custom-fields-proxy.html">Extract custom fields using web crawler and proxy »</a>
</span>
</div>
<div class="chapter">
<div class="titlepage"><div><div>
<h2 class="title"><a id="crawler-custom-values-ingest-pipeline"></a>Customize crawler field values using an ingest pipeline<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-custom-values-ingest-pipeline.asciidoc">edit</a></h2>
</div></div></div>

<p>In this guide, you&#8217;ll learn how to use ingest pipelines to remove unwanted content from webpages you&#8217;ve crawled using the Elastic web crawler.
Most websites contain content that is not needed for a search experience, such as navigation links, headers and footers, and other boilerplate.</p>
<p>If you can edit the HTML for the web pages you&#8217;re crawling, you can use <a class="xref" href="crawler-content.html#crawler-content-meta-tags-content-extraction" title="Meta tags and data attributes to extract custom fields">Meta tags and data attributes to extract custom fields</a>.
However, if you don&#8217;t control the source HTML, you&#8217;ll need a workaround to remove this content from your documents.</p>
<p>One approach is to use a <a class="xref" href="crawler-custom-fields-proxy.html" title="Extract custom fields using web crawler and proxy">proxy server</a> to inject meta tags into the HTML before crawling.
The drawback here is that you need to run a proxy server, which adds complexity and overhead.</p>
<p>In this guide, we&#8217;ll show you another option, using <span class="strong strong"><strong>ingest pipelines</strong></span>.
Ingest pipelines are a native Elasticsearch feature for transforming data.
We&#8217;ll use a custom ingest pipeline to modify the values of specific fields in our documents, <em>before</em> they are written to Elasticsearch.
The advantage of this approach is that we can manage everything from the Kibana UI.</p>
<h3><a id="crawler-custom-values-ingest-pipeline-problem-statement"></a>The problem and solution<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-custom-values-ingest-pipeline.asciidoc">edit</a></h3>
<p>In this example, imagine we are creating a search experience for a news website.
For this example, we chose the <a href="https://www.theskimm.com" class="ulink" target="_top">https://www.theskimm.com</a>.
We use the Elastic web crawler to crawl the website and index the content.
However, the website contains a lot of boilerplate content that we don&#8217;t want to include in our search results.
We know this because we&#8217;ve crawled the website and inspected the fields in the Elasticsearch documents created by the crawler.</p>
<p>To remove this content, we&#8217;ll use an ingest pipeline to modify the values of specific fields in our documents.</p>
<p>In this example we will:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<p><span class="strong strong"><strong>Create an Elasticsearch index</strong></span> using the web crawler ingestion method</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Configure the crawler to crawl a subset of <code class="literal">theskimm.com</code> webpages
</li>
</ul>
</div>
</li>
<li class="listitem">
<p><span class="strong strong"><strong>Create a custom ingest pipeline</strong></span></p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Add a set of processors to the pipeline
</li>
<li class="listitem">
Configure the processors to find and remove boilerplate content
</li>
</ul>
</div>
</li>
<li class="listitem">
<span class="strong strong"><strong>Test the pipeline</strong></span> against sample documents
</li>
<li class="listitem">
<span class="strong strong"><strong>Crawl the website</strong></span> using the custom pipeline
</li>
</ul>
</div>
<h3><a id="crawler-custom-values-ingest-pipeline-prerequisites"></a>Prerequisites<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-custom-values-ingest-pipeline.asciidoc">edit</a></h3>
<p>To use Enterprise Search features you need <a class="xref" href="prerequisites.html" title="Prerequisites">a subscription, a deployment, and a user</a>.
You get all three with an Elastic Cloud deployment.
If you&#8217;re brand new to Elastic start a <a href="https://cloud.elastic.co/registration" class="ulink" target="_blank" rel="noopener">free Elastic Cloud trial</a>.</p>
<p>Within <span class="strong strong"><strong>Advanced settings</strong></span>, ensure the Enterprise Search <em>Size per zone</em> is set to at least <span class="strong strong"><strong>4 GB RAM</strong></span>.</p>
<h3><a id="crawler-custom-values-ingest-pipeline-crawler-setup"></a>Configure your web crawler index<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-custom-values-ingest-pipeline.asciidoc">edit</a></h3>
<p>First we need to create an Elasticsearch index and configure the web crawler to crawl the domain:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
In Kibana, navigate to <span class="strong strong"><strong>Enterprise Search</strong></span> &#8594; <span class="strong strong"><strong>Content</strong></span> &#8594; <span class="strong strong"><strong>Indices</strong></span> to create your index.
</li>
<li class="listitem">
Add the domain <a href="https://www.theskimm.com" class="ulink" target="_top">https://www.theskimm.com</a> to your index.
</li>
<li class="listitem">
<p>Add the following crawl rules to crawl a subset of the domain:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<span class="strong strong"><strong>Policy</strong></span>:<code class="literal">Allow</code>, <span class="strong strong"><strong>Rule</strong></span>:<code class="literal">Begins with</code>, <span class="strong strong"><strong>Path pattern</strong></span>: <code class="literal">/news/2022</code>
</li>
<li class="listitem">
<span class="strong strong"><strong>Policy</strong></span>:<code class="literal">Disallow</code>, <span class="strong strong"><strong>Rule</strong></span>: <code class="literal">Regex</code>, <span class="strong strong"><strong>Path pattern</strong></span>: <code class="literal">.*</code>
</li>
</ul>
</div>
</li>
<li class="listitem">
Crawl the website.
</li>
</ul>
</div>
<p>The crawl shouldn&#8217;t take long (less than one minute), because we&#8217;ll only be indexing around 250 documents.
Once the crawl is complete, take a minute to inspect a few documents.
Go to the <span class="strong strong"><strong>Documents</strong></span> tab and click on a few documents to inspect the content.</p>
<p>You&#8217;ll notice a few things:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<p>In the <code class="literal">body_content</code> field, header and footer content is duplicated across all documents.</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
There&#8217;s some boilerplate header content like <code class="literal">News Money Wellness Life Events PFL Midterms | Daily Skimm MORE+ Login search Sign up Menu News ...</code>
</li>
<li class="listitem">
There&#8217;s some boilerplate footer content like <code class="literal">Live Smarter Sign up for the Daily Skimm email newsletter. Delivered to your inbox every morning and prepares you for your day in minutes. ... © 2022 theSkimm, All rights reserved This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply.</code>
</li>
</ul>
</div>
</li>
<li class="listitem">
All <code class="literal">title</code> field values are suffixed with <code class="literal">| theSkimm</code> (with leading and trailing spaces)
</li>
</ul>
</div>
<p>We don&#8217;t want this redundant content in our search results, so we&#8217;ll need to remove it.
To do this, we&#8217;ll create a custom ingest pipeline that uses a set of processors to find and remove patterns from specific fields.</p>
<h3><a id="crawler-custom-values-ingest-pipeline-create-pipeline"></a>Create a custom ingest pipeline<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-custom-values-ingest-pipeline.asciidoc">edit</a></h3>
<p>When you create an index in Enterprise Search, a default ingest pipeline is set up with several processors, to optimize your content for search.
This pipeline is called <code class="literal">ent-search-generic-ingestion</code>.
Enterprise Search also enables you to easily create <span class="strong strong"><strong>index-specific</strong></span> ingest pipelines for custom processing.
We&#8217;ll create a new pipeline with three processors targeting specific fields in our documents.</p>
<p>To create a custom pipeline:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
In Kibana, Go to the <span class="strong strong"><strong>Pipelines</strong></span> tab for the index you created in the previous step.
</li>
<li class="listitem">
Select <span class="strong strong"><strong>Copy and customize</strong></span>.
</li>
<li class="listitem">
This creates a new pipeline called <code class="literal">&lt;index-name&gt;@custom</code>.
</li>
</ul>
</div>
<p>An ingest pipeline is a list of processors that are applied to a document in order.
The pipeline we created is empty, so now we&#8217;ll need to add some processors.</p>
<h3><a id="crawler-custom-values-ingest-pipeline-custom-pipeline"></a>The <code class="literal">@custom</code> pipeline<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-custom-values-ingest-pipeline.asciidoc">edit</a></h3>
<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<p>You should not rename this pipeline.</p>
</div>
</div>
<p>The <code class="literal">&lt;index-name&gt;@custom</code> pipeline is empty by default.
We&#8217;ll add processors in the Kibana UI.</p>
<p>For this exercise we&#8217;ll use the <a href="/guide/en/elasticsearch/reference/master/gsub-processor.html" class="ulink" target="_blank" rel="noopener">Gsub processor</a>.
The Gsub processor allows you to replace (or simply delete) substrings within a field.
We&#8217;ll use it to remove the header/footer and title boilerplate from our document fields.</p>
<p>We need to configure <span class="strong strong"><strong>three</strong></span> Gsub processors:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
To remove the <span class="strong strong"><strong>header boilerplate</strong></span> from the <code class="literal">body_content</code> field.
</li>
<li class="listitem">
To remove the <span class="strong strong"><strong>footer boilerplate</strong></span> from the <code class="literal">body_content</code> field.
</li>
<li class="listitem">
To remove the <span class="strong strong"><strong>title suffix</strong></span> from the <code class="literal">title</code> field.
</li>
</ol>
</div>
<p>The following table shows the configuration for each processor.
When you configure your three processors, you&#8217;ll use the <span class="strong strong"><strong>Field</strong></span> and <span class="strong strong"><strong>Pattern</strong></span> values from this table.</p>
<div class="table">
<a id="crawler-custom-values-ingest-pipeline-custom-pipeline-table"></a>
<p class="title"><strong>Table 1. Table of ingest pipeline processors</strong></p>
<div class="table-contents">
<table border="1" cellpadding="4px" summary="Table of ingest pipeline processors">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
<col class="col_4"/>
<col class="col_5"/>
<col class="col_6"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Processor</th>
<th align="left" valign="top">Processor name</th>
<th align="left" valign="top">Field</th>
<th align="left" valign="top">Pattern</th>
<th align="left" valign="top">Replacement</th>
<th align="left" valign="top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>1</strong></span></p></td>
<td align="left" valign="top"><p><code class="literal">Gsub</code></p></td>
<td align="left" valign="top"><p><code class="literal">body_content</code></p></td>
<td align="left" valign="top"><p>News Money Wellness.*?The Story</p></td>
<td align="left" valign="top"><p>-</p></td>
<td align="left" valign="top"><p>Find and remove boilerplate header content</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>2</strong></span></p></td>
<td align="left" valign="top"><p><code class="literal">Gsub</code></p></td>
<td align="left" valign="top"><p><code class="literal">body_content</code></p></td>
<td align="left" valign="top"><p>Live Smarter Sign up for the Daily Skimm email newsletter.*$</p></td>
<td align="left" valign="top"><p>-</p></td>
<td align="left" valign="top"><p>Find and remove boilerplate footer content</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>3</strong></span></p></td>
<td align="left" valign="top"><p><code class="literal">Gsub</code></p></td>
<td align="left" valign="top"><p><code class="literal">title</code></p></td>
<td align="left" valign="top"><p>\\|.*</p></td>
<td align="left" valign="top"><p>-</p></td>
<td align="left" valign="top"><p>Find and remove boilerplate suffix from title</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<h3><a id="crawler-custom-values-ingest-pipeline-add-pipeline"></a>Add and configure processors<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-custom-values-ingest-pipeline.asciidoc">edit</a></h3>
<p>Because we&#8217;re adding three Gsub processors, you&#8217;ll need to <span class="strong strong"><strong>repeat these steps three times</strong></span>.</p>
<p>To add a processor:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
In the <span class="strong strong"><strong>Content</strong></span> overview for your index, select the <span class="strong strong"><strong>Pipelines</strong></span> tab.
</li>
<li class="listitem">
Select <span class="strong strong"><strong>Edit pipeline</strong></span> for the <code class="literal">&lt;index-name&gt;@custom</code> pipeline.
This navigates you to the <span class="strong strong"><strong>Stack Management &#8594; Ingest Pipelines</strong></span> page.
</li>
<li class="listitem">
Select <span class="strong strong"><strong>Manage</strong></span> and <span class="strong strong"><strong>Edit</strong></span> in the modal.
</li>
<li class="listitem">
Select <span class="strong strong"><strong>Add processor</strong></span> and scroll through the list of available processors.
</li>
<li class="listitem">
Select the <span class="strong strong"><strong>Gsub</strong></span> processor.
</li>
</ul>
</div>
<p>Next, we&#8217;ll configure each processor to find and remove matched patterns from specific fields.
Again, you&#8217;ll need to repeat this operation three times, once for each processor.</p>
<p>Follow these steps, copying the <a class="xref" href="crawler-custom-values-ingest-pipeline.html#crawler-custom-values-ingest-pipeline-custom-pipeline-table" title="Table of ingest pipeline processors">values in the table above</a> to configure each processor.</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Under <span class="strong strong"><strong>Field</strong></span>, enter the field which contains the text you want to remove.
</li>
<li class="listitem">
In the <span class="strong strong"><strong>Pattern</strong></span> field, enter the regex pattern you want to remove.
Pay attention to any leading or trailing spaces.
</li>
<li class="listitem">
Leave the <span class="strong strong"><strong>Replacement</strong></span> fields blank, because we want to simply remove the matching text.
</li>
<li class="listitem">
Select <span class="strong strong"><strong>Ignore missing</strong></span> to prevent processors from failing if the field is missing.
</li>
<li class="listitem">
When you&#8217;ve configured the three processors, select <span class="strong strong"><strong>Save pipeline</strong></span> to save your pipeline configurations.
</li>
</ul>
</div>
<p>Our pipeline has been configured, but we should test it to make sure the three processors work as expected.
This allows us to correct any problems <em>before</em> we start crawling.
This is important, particularly if you&#8217;re crawling a large number of webpages.</p>
<h3><a id="crawler-custom-values-ingest-pipeline-test-pipeline"></a>Test your custom ingest pipeline<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-custom-values-ingest-pipeline.asciidoc">edit</a></h3>
<p>You can test your custom pipeline in the Kibana UI:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
In the <span class="strong strong"><strong>Content</strong></span> overview for your index, click the <span class="strong strong"><strong>Pipelines</strong></span> tab.
</li>
<li class="listitem">
Select <span class="strong strong"><strong>Edit pipeline</strong></span> for the <code class="literal">&lt;index-name&gt;@custom</code> pipeline.
This navigates you to the <span class="strong strong"><strong>Stack Management &#8594; Ingest Pipelines</strong></span> page.
</li>
<li class="listitem">
Select <span class="strong strong"><strong>Manage</strong></span> and <span class="strong strong"><strong>Edit</strong></span> in the modal.
</li>
<li class="listitem">
Select <span class="strong strong"><strong>Add documents</strong></span> next to <span class="strong strong"><strong>Test pipeline</strong></span> in the <span class="strong strong"><strong>Processors</strong></span> section.
</li>
<li class="listitem">
Add a document to test your pipeline.
Use a document from your index by providing the document&#8217;s index and <span class="strong strong"><strong>document ID</strong></span>.
Find this information in the <span class="strong strong"><strong>Documents</strong></span> tab of your index overview page.
</li>
</ul>
</div>
<h3><a id="crawler-custom-values-ingest-pipeline-crawl"></a>Crawl your webpages<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-custom-values-ingest-pipeline.asciidoc">edit</a></h3>
<p>Now that we&#8217;ve tested our pipeline, we can start crawling our webpages.</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Go to your index in the Kibana UI and launch a crawl.
</li>
<li class="listitem">
Verify the structure of your documents in the <span class="strong strong"><strong>Documents</strong></span> tab of the index overview page.
Check that the boilerplate content has been removed from the <code class="literal">body_content</code> and <code class="literal">title</code> fields.
</li>
</ul>
</div>
<p>If you&#8217;ve followed along, well done!
You&#8217;ve successfully configured your Enterprise Search deployment to customize how webpages are indexed.
You&#8217;ve learned the basics of a powerful set of tools for creating custom search experiences.
Use this guide as a blueprint for more complex use cases.
There are currently around 40 available processors that are ready to use.
See <a href="/guide/en/elasticsearch/reference/master/processors.html" class="ulink" target="_blank" rel="noopener">the Elasticsearch documentation</a> for the full list.</p>
<h3><a id="crawler-custom-values-ingest-pipeline-learn-more"></a>Learn more<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-custom-values-ingest-pipeline.asciidoc">edit</a></h3>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Dive deeper into <a href="/guide/en/elasticsearch/reference/master/ingest.html" class="ulink" target="_blank" rel="noopener">ingest pipelines</a> in the Elasticsearch documentation.
</li>
<li class="listitem">
Learn more about <a class="xref" href="ingest-pipelines.html" title="Ingest pipelines">ingest pipelines in <span class="strong strong"><strong>Enterprise Search</strong></span></a>.
</li>
<li class="listitem">
The <a class="xref" href="crawler-schema.html" title="Web crawler schema"><em>Web crawler schema</em></a> details exactly how the crawler transforms HTML content into Elasticsearch documents.
</li>
<li class="listitem">
If you can edit the HTML for the web pages you&#8217;re crawling, see <a class="xref" href="crawler-content.html" title="Optimizing web content for the web crawler"><em>Optimizing web content</em></a> for a more direct approach.
</li>
<li class="listitem">
If you&#8217;d prefer to use a proxy server to customize webpages before they are crawled, see <a class="xref" href="crawler-custom-fields-proxy.html" title="Extract custom fields using web crawler and proxy"><em>Custom fields using proxy</em></a>.
</li>
<li class="listitem">
Learn all about the <a class="xref" href="crawler.html" title="Elastic web crawler">Elastic web crawler</a>.
</li>
</ul>
</div>
</div>
<div class="navfooter">
<span class="prev">
<a href="crawler-content.html">« Optimizing web content for the web crawler</a>
</span>
<span class="next">
<a href="crawler-custom-fields-proxy.html">Extract custom fields using web crawler and proxy »</a>
</span>
</div>
</div>
</body>
</html>
