<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Web crawler content extraction rules | Enterprise Search documentation [master] | Elastic</title>
<meta class="elastic" name="content" content="Web crawler content extraction rules | Enterprise Search documentation [master]">

<link rel="home" href="index.html" title="Enterprise Search documentation [master]"/>
<link rel="up" href="crawler.html" title="Elastic web crawler"/>
<link rel="prev" href="crawler-managing.html" title="Managing crawls in Kibana"/>
<link rel="next" href="crawler-known-issues.html" title="Elastic web crawler known issues"/>
<meta class="elastic" name="product_version" content="master"/>
<meta class="elastic" name="product_name" content="Enterprise Search"/>
<meta class="elastic" name="website_area" content="documentation"/>
<meta name="DC.type" content="Learn/Docs/Enterprise Search/Guide/master"/>
<meta name="DC.subject" content="Enterprise Search"/>
<meta name="DC.identifier" content="master"/>
</head>
<body><div class="page_header">
You are looking at preliminary documentation for a future release.
Not what you want? See the
<a href="../current/index.html">current release documentation</a>.
</div>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="index.html">Enterprise Search documentation [master]</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="crawler.html">Elastic web crawler</a></span>
</div>
<div class="navheader">
<span class="prev">
<a href="crawler-managing.html">« Managing crawls in Kibana</a>
</span>
<span class="next">
<a href="crawler-known-issues.html">Elastic web crawler known issues »</a>
</span>
</div>
<div class="chapter">
<div class="titlepage"><div><div>
<h2 class="title"><a id="crawler-extraction-rules"></a>Web crawler content extraction rules<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-extraction-rules.asciidoc">edit</a></h2>
</div></div></div>

<p>Extraction rules enable you to customize how the crawler extracts content from webpages.</p>
<p>Extraction rules are designed to make data extraction more flexible.
Add your own rules to supplement standard content extraction like HTML body or title content.
For example, you might want to build a search experience based on specific information presented on your website.</p>
<p>To add new extraction rules or modify existing rules, go to the <span class="strong strong"><strong>Manage domains</strong></span> tab in Kibana.
Find this under <span class="strong strong"><strong>Enterprise Search &gt; Content &gt; Elasticsearch indices &gt; <em>&lt;search-my-crawler-index&gt;</em></strong></span>.
Next, find the <span class="strong strong"><strong>Extraction rules</strong></span> tab for your domain.</p>
<div class="imageblock screenshot">
<div class="content">
<img src="images/crawler-extraction-rules.png" alt="The web crawler extraction rules management UI">
</div>
</div>
<h3><a id="crawler-extraction-rules-url-filters"></a>URL filters<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-extraction-rules.asciidoc">edit</a></h3>
<p>URL filters enable you to target certain pages or URLs to extract content from.
For example, imagine only a subset of HTML pages contain content version and author information.
You can create a URL filter that matches only those pages.</p>
<p>URL filters work exactly the same way as crawl rules.
Refer to <a class="xref" href="crawler-managing.html#crawler-managing-crawl-rules" title="Crawl rules">crawl rules</a> for more information.</p>
<p>In the example below, the extraction rules will be applied only on the pages whose URLs start with <code class="literal">/blog</code> or contain <code class="literal">2022</code>.</p>
<div class="imageblock screenshot">
<div class="content">
<img src="images/crawler-extraction-rules-url-filters.png" alt="The web crawler extraction rules URL filters">
</div>
</div>
<h3><a id="crawler-extraction-rules-content-fields"></a>Content fields<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-extraction-rules.asciidoc">edit</a></h3>
<p>Use content fields to pinpoint which parts of a webpage to pull data from.
Each extraction rule can have multiple content fields with different configurations.
To create a new content field you need to specify several required options:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
<p><span class="strong strong"><strong>Field name</strong></span>: The web crawler stores extracted content in Elasticsearch documents under this name.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>Be aware that all specified content fields will irretrievably change the Elasticsearch index.
You can only add new fields, but you can&#8217;t rename or delete them.</p>
</div>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>You can&#8217;t use certain reserved field names:
<code class="literal">body_content</code>, <code class="literal">domains</code>, <code class="literal">headings</code>, <code class="literal">meta_description</code>, <code class="literal">title</code>, <code class="literal">url</code>, <code class="literal">url_host</code>, <code class="literal">url_path</code>, <code class="literal">url_path_dir1</code>, <code class="literal">url_path_dir2</code>, <code class="literal">url_path_dir3</code>, <code class="literal">url_port</code>, <code class="literal">url_scheme</code>.</p>
</div>
</div>
</li>
<li class="listitem">
<span class="strong strong"><strong>Source</strong></span>: The crawler can use either raw HTML or URL as a content source.
For both source types you should define <code class="literal">selector</code>: CSS selector or regular expression accordingly.
</li>
<li class="listitem">
<p><span class="strong strong"><strong>Content</strong></span>: Use <code class="literal">Extracted value</code> if you defined <code class="literal">Source</code>.
Use <code class="literal">A fixed value</code> to specify a fixed value (string) to inject if the field is found in the source. This will override the extracted content with your fixed value.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>Content fields are stored as <code class="literal">string</code> if selector returns a single value, or as <code class="literal">array</code> if selector returns multiple values.
This option can be changed later, however you need perform a full crawl to reindex all documents.</p>
</div>
</div>
</li>
</ol>
</div>
<p>In the screenshot below, we are adding a new field that will be extracted from the HTML pages using the CSS selector <code class="literal">#author</code>.
The extracted value will be stored as a string under <code class="literal">blog-author</code>.</p>
<div class="imageblock screenshot">
<div class="content">
<img src="images/crawler-extraction-rules-content-field.png" alt="The web crawler extraction rules content field">
</div>
</div>
<h3><a id="crawler-extraction-rules-css-selectors"></a>CSS selectors<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-extraction-rules.asciidoc">edit</a></h3>
<p>When CSS selector is chosen the web crawler will try to apply this during HTML parsing and store the extracted value in an Elasticsearch document.
When the web crawler applies selectors, it also strips all HTML tags and returns only inner text.
If the selector returns multiple values, the web crawler will either concatenate all the values into a single string, or store it as array of values.
See <a class="xref" href="crawler-extraction-rules.html#crawler-extraction-rules-field-content" title="Storing different kinds of content">Storing different kinds of content</a></p>
<p>Extraction rules support CSS Level 3.
For more information refer to the official <a href="https://www.w3.org/TR/selectors-3/" class="ulink" target="_blank" rel="noopener">W3C documentation</a>.</p>
<h3><a id="crawler-extraction-rules-url-patterns"></a>URL patterns<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-extraction-rules.asciidoc">edit</a></h3>
<p>To extract content from URLs you need to use Regex as a selector.
We recommend using <code class="literal">capturing groups</code> to explicitly indicate which part of the regular expression needs to stored as a content field.</p>
<p>Here are some examples:</p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
<col class="col_4"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">String</th>
<th align="left" valign="top">Regex</th>
<th align="left" valign="top">Match result</th>
<th align="left" valign="top">Match group (final result)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p><code class="literal">https://example.org/posts/2023/01/20/post-1</code></p></td>
<td align="left" valign="top"><p><code class="literal">posts\/([0-9]{4})</code></p></td>
<td align="left" valign="top"><p><code class="literal">posts/2023</code></p></td>
<td align="left" valign="top"><p><code class="literal">2023</code></p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">https://example.org/posts/2023/01/20/post-1</code></p></td>
<td align="left" valign="top"><p><code class="literal">posts\/([0-9]{4})\/([0-9]{2})\/([0-9]{2})</code></p></td>
<td align="left" valign="top"><p><code class="literal">posts/2023/01/20</code></p></td>
<td align="left" valign="top"><p><code class="literal">[2023, 01, 20]</code></p></td>
</tr>
</tbody>
</table>
</div>
<h3><a id="crawler-extraction-rules-field-content"></a>Storing different kinds of content<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-extraction-rules.asciidoc">edit</a></h3>
<p>Extracted values can be stored in Elasticseach as a <code class="literal">string</code> or an <code class="literal">array</code>.
This is useful if your CSS selector or Regex returns multiple values.
If <code class="literal">string</code> is selected then the web crawler will combine all the values into a string.
Otherwise all the values will be stored as array items.</p>
<p>If you want to convert the extracted value into a different data type you can use <a href="/guide/en/elasticsearch/reference/master/convert-processor.html" class="ulink" target="_blank" rel="noopener">Convert processor</a></p>
<p>Learn more about <a href="/guide/en/elasticsearch/reference/master/ingest.html" class="ulink" target="_blank" rel="noopener">Ingest pipelines</a></p>
<h3><a id="crawler-binary-content"></a>Binary content extraction<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-extraction-rules.asciidoc">edit</a></h3>
<p>See <a class="xref" href="crawler-managing.html#crawler-managing-binary-content" title="Binary content extraction">Binary content extraction</a></p>
<h3><a id="crawler-extraction-rules-lear-more"></a>Learn more<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/enterprise-search-pubs/edit/main/enterprise-search-docs/crawler-extraction-rules.asciidoc">edit</a></h3>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="crawler-managing.html#crawler-managing-domains" title="Domains">Managing domains</a>
</li>
<li class="listitem">
<a class="xref" href="crawler-managing.html#crawler-managing-crawl-rules" title="Crawl rules">Crawl rules</a>
</li>
<li class="listitem">
<a class="xref" href="crawler-troubleshooting.html" title="Troubleshooting crawls"><em>Troubleshooting crawls</em></a>
</li>
</ul>
</div>
</div>
<div class="navfooter">
<span class="prev">
<a href="crawler-managing.html">« Managing crawls in Kibana</a>
</span>
<span class="next">
<a href="crawler-known-issues.html">Elastic web crawler known issues »</a>
</span>
</div>
</div>
</body>
</html>
