<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Scaling Elastic Agent on Kubernetes | Fleet and Elastic Agent Guide [master] | Elastic</title>
<meta class="elastic" name="content" content="Scaling Elastic Agent on Kubernetes | Fleet and Elastic Agent Guide [master]">

<link rel="home" href="index.html" title="Fleet and Elastic Agent Guide [master]"/>
<link rel="up" href="install-elastic-agents-in-containers.html" title="Install Elastic Agents in a containerized environment"/>
<link rel="prev" href="running-on-kubernetes-standalone.html" title="Run Elastic Agent Standalone on Kubernetes"/>
<link rel="next" href="agent-environment-variables.html" title="Elastic Agent environment variables"/>
<meta class="elastic" name="product_version" content="master"/>
<meta class="elastic" name="product_name" content="Fleet and Elastic Agent"/>
<meta class="elastic" name="website_area" content="documentation"/>
<meta name="DC.type" content="Learn/Docs/Fleet/Guide/Elastic Agent/master"/>
<meta name="DC.subject" content="Fleet and Elastic Agent"/>
<meta name="DC.identifier" content="master"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body><div class="page_header">
You are looking at preliminary documentation for a future release.
Not what you want? See the
<a href="../current/index.html">current release documentation</a>.
</div>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="index.html">Fleet and Elastic Agent Guide [master]</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="elastic-agent-installation.html">Install Elastic Agents</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="install-elastic-agents-in-containers.html">Install Elastic Agents in a containerized environment</a></span>
</div>
<div class="navheader">
<span class="prev">
<a href="running-on-kubernetes-standalone.html">« Run Elastic Agent Standalone on Kubernetes</a>
</span>
<span class="next">
<a href="agent-environment-variables.html">Elastic Agent environment variables »</a>
</span>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="scaling-on-kubernetes"></a>Scaling Elastic Agent on Kubernetes<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/ingest-docs/edit/main/docs/en/ingest-management/elastic-agent/scaling-on-kubernetes.asciidoc">edit</a></h3>
</div></div></div>
<p>For more information on how to deploy Elastic Agent on Kubernetes, please review these pages:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="running-on-kubernetes-managed-by-fleet.html" title="Run Elastic Agent on Kubernetes managed by Fleet">Run Elastic Agent on Kubernetes managed by Fleet</a>.
</li>
<li class="listitem">
<a class="xref" href="running-on-kubernetes-standalone.html" title="Run Elastic Agent Standalone on Kubernetes">Run Elastic Agent Standalone on Kubernetes</a>.
</li>
</ul>
</div>
<h5><a id="_observability_at_scale"></a>Observability at scale<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/ingest-docs/edit/main/docs/en/ingest-management/elastic-agent/scaling-on-kubernetes.asciidoc">edit</a></h5>
<p>This document summarizes some key factors and best practices for using <a href="/guide/en/welcome-to-elastic/current/getting-started-kubernetes.html" class="ulink" target="_top">Elastic Observability</a> to monitor Kubernetes infrastructure at scale. Users need to consider different parameters and adjust Elastic Stack accordingly. These elements are affected as the size of Kubernetes cluster increases:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
The amount of metrics being collected from several Kubernetes endpoints
</li>
<li class="listitem">
The Elastic Agent&#8217;s resources to cope with the high CPU and Memory needs for the internal processing
</li>
<li class="listitem">
The Elasticsearch resources needed due to the higher rate of metric ingestion
</li>
<li class="listitem">
The Dashboard&#8217;s visualizations response times as more data are requested on a given time window
</li>
</ul>
</div>
<p>The document is divided in two main sections:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="scaling-on-kubernetes.html#configuration-practices" title="Configuration Best Practices">Configuration Best Practices</a>
</li>
<li class="listitem">
<a class="xref" href="scaling-on-kubernetes.html#validation-and-troubleshooting-practices" title="Validation and Troubleshooting practices">Validation and Troubleshooting practices</a>
</li>
</ul>
</div>
<h5><a id="configuration-practices"></a>Configuration Best Practices<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/ingest-docs/edit/main/docs/en/ingest-management/elastic-agent/scaling-on-kubernetes.asciidoc">edit</a></h5>
<h6><a id="_configure_agent_resources"></a>Configure Agent Resources<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/ingest-docs/edit/main/docs/en/ingest-management/elastic-agent/scaling-on-kubernetes.asciidoc">edit</a></h6>
<p>The Kubernetes Observability is based on <a href="https://docs.elastic.co/en/integrations/kubernetes" class="ulink" target="_top">Elastic Kubernetes integration</a>, which collects metrics from several components:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<p><span class="strong strong"><strong>Per node:</strong></span></p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
kubelet
</li>
<li class="listitem">
controller-manager
</li>
<li class="listitem">
scheduler
</li>
<li class="listitem">
proxy
</li>
</ul>
</div>
</li>
<li class="listitem">
<p><span class="strong strong"><strong>Cluster wide (such as unique metrics for the whole cluster):</strong></span></p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
kube-state-metrics
</li>
<li class="listitem">
apiserver
</li>
</ul>
</div>
</li>
</ul>
</div>
<p>Controller manager and Scheduler datastreams are being enabled only on the specific node that actually runs based on autodiscovery rules</p>
<p>The default manifest provided deploys Elastic Agent as DaemonSet which results in an Elastic Agent being deployed on every node of the Kubernetes cluster.</p>
<p>Additionally, by default one agent is elected as <span class="strong strong"><strong>leader</strong></span> (for more information visit <a class="xref" href="kubernetes_leaderelection-provider.html" title="Kubernetes LeaderElection Provider">Kubernetes LeaderElection Provider</a>). The Elastic Agent Pod which holds the leadership lock is responsible for collecting the cluster-wide metrics in addition to its node&#8217;s metrics.</p>
<div class="imageblock screenshot">
<div class="content">
<img src="../images/k8sscaling.png" alt="Elastic Agent as daemonset">
</div>
</div>
<p>The above schema explains how Elastic Agent collects and sends metrics to Elasticsearch. Because of Leader Agent being responsible to also collecting cluster-lever metrics, this means that it requires additional resources.</p>
<p>The DaemonSet deployment approach with leader election simplifies the installation of the Elastic Agent because we define less Kubernetes Resources in our manifest and we only need one single Agent policy for our Agents. Hence it is the default supported method for <a class="xref" href="running-on-kubernetes-managed-by-fleet.html" title="Run Elastic Agent on Kubernetes managed by Fleet">Managed Elastic Agent installation</a></p>
<h6><a id="_specifying_resources_and_limits_in_agent_manifests"></a>Specifying resources and limits in Agent manifests<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/ingest-docs/edit/main/docs/en/ingest-management/elastic-agent/scaling-on-kubernetes.asciidoc">edit</a></h6>
<p>Resourcing of your Pods and the Scheduling priority (check section <a class="xref" href="scaling-on-kubernetes.html#agent-scheduling" title="Agent Scheduling">Scheduling priority</a>) of them are two topics that might be affected as the Kubernetes cluster size increases.
The increasing demand of resources might result to under-resource the Elastic Agents of your cluster.</p>
<p>Based on our tests we advise to configure only the <code class="literal">limit</code> section of the <code class="literal">resources</code> section in the manifest. In this way the <code class="literal">request</code>'s settings of the <code class="literal">resources</code> will fall back to the <code class="literal">limits</code> specified. The <code class="literal">limits</code> is the upper bound limit of your microservice process, meaning that can operate in less resources and protect Kubernetes to assign bigger usage and protect from possible resource exhaustion.</p>
<div class="pre_wrapper lang-yaml">
<pre class="programlisting prettyprint lang-yaml">resources:
    limits:
      cpu: "1000m"
      memory: "200Mi"</pre>
</div>
<p>Based on our <a href="https://github.com/elastic/elastic-agent/blob/main/docs/elastic-agent-scaling-tests.md" class="ulink" target="_top">Elastic Agent Scaling tests</a>, the following table provides guidelines to adjust Elastic Agent limits on different Kubernetes sizes:</p>
<p>Sample Elastic Agent Configurations:</p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
</colgroup>
<tbody>
<tr>
<td align="left" valign="top"><p>No of Pods in K8s Cluster</p></td>
<td align="left" valign="top"><p>Leader Agent Resources</p></td>
<td align="left" valign="top"><p>Rest of Agents</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>1000</p></td>
<td align="left" valign="top"><p>cpu: "1500m",  memory: "800Mi"</p></td>
<td align="left" valign="top"><p>cpu: "300m",  memory: "600Mi"</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>3000</p></td>
<td align="left" valign="top"><p>cpu: "2000m",  memory: "1500Mi"</p></td>
<td align="left" valign="top"><p>cpu: "400m",  memory: "800Mi"</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>5000</p></td>
<td align="left" valign="top"><p>cpu: "3000m",  memory: "2500Mi"</p></td>
<td align="left" valign="top"><p>cpu: "500m",  memory: "900Mi"</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>10000</p></td>
<td align="left" valign="top"><p>cpu: "3000m",  memory: "3600Mi"</p></td>
<td align="left" valign="top"><p>cpu: "700m",  memory: "1000Mi"</p></td>
</tr>
</tbody>
</table>
</div>
<div class="quoteblock">
<blockquote>
<p>The above tests were performed with Elastic Agent version 8.7 and scraping period of <code class="literal">10sec</code> (period setting for the Kubernetes integration). Those numbers are just indicators and should be validated for each different Kubernetes environment and amount of workloads.</p>
</blockquote>
</div>
<h6><a id="_proposed_agent_installations_for_large_scale"></a>Proposed Agent Installations for large scale<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/ingest-docs/edit/main/docs/en/ingest-management/elastic-agent/scaling-on-kubernetes.asciidoc">edit</a></h6>
<p>Although daemonset installation is simple, it can not accomodate the varying agent resource requirements depending on the collected metrics. The need for appropriate resource assignment at large scale requires more granular installation methods.</p>
<p>Elastic Agent deployment is broken in groups as follows:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
A dedicated Elastic Agent deployment of a single Agent for collecting cluster wide metrics from the apiserver
</li>
<li class="listitem">
Node level Elastic Agents(no leader Agent) in a Daemonset
</li>
<li class="listitem">
kube-state-metrics shards and Elastic Agents in the StatefulSet defined in the kube-state-metrics autosharding manifest
</li>
</ul>
</div>
<p>Each of these groups of Elastic Agents will have its own policy specific to its function and can be resourced independently in the appropriate manifest to accomodate its specific resource requirements.</p>
<p>Resource assignment led us to alternatives installation methods.</p>
<div class="important admon">
<div class="icon"></div>
<div class="admon_content">
<p>The main suggestion for big scale clusters <span class="strong strong"><strong>is to install Elastic Agent as side container along with <code class="literal">kube-state-metrics</code> Shard</strong></span>. The installation is explained in details <a href="https://github.com/elastic/elastic-agent/tree/main/docs/manifests/kustomize-autosharding" class="ulink" target="_top">Elastic Agent with Kustomize in Autosharding</a></p>
</div>
</div>
<p>The following <span class="strong strong"><strong>alternative configuration methods</strong></span> have been verified:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
<p>With <code class="literal">hostNetwork:false</code></p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Elastic Agent as Side Container within KSM Shard pod
</li>
<li class="listitem">
For non-leader Elastic Agent deployments that collect per KSM shards
</li>
</ul>
</div>
</li>
<li class="listitem">
With <code class="literal">taint/tolerations</code> to isolate the Elastic Agent daemonset pods from rest of deployments
</li>
</ol>
</div>
<p>You can find more information in the document called <a href="https://github.com/elastic/elastic-agent/blob/ksmsharding/docs/elastic-agent-ksm-sharding.md" class="ulink" target="_top">Elastic Agent Manifests in order to support Kube-State-Metrics Sharding</a>.</p>
<p>Based on our <a href="https://github.com/elastic/elastic-agent/blob/ksmsharding/docs/elastic-agent-scaling-tests.md" class="ulink" target="_top">Elastic Agent scaling tests</a>, the following table aims to assist users on how to configure their KSM Sharding as Kubernetes cluster scales:</p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
</colgroup>
<tbody>
<tr>
<td align="left" valign="top"><p>No of Pods in K8s Cluster</p></td>
<td align="left" valign="top"><p>No of KSM Shards</p></td>
<td align="left" valign="top"><p>Agent Resources</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>1000</p></td>
<td align="left" valign="top"><p>No Sharding can be handled with default KSM config</p></td>
<td align="left" valign="top"><p>limits: memory: 700Mi , cpu:500m</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>3000</p></td>
<td align="left" valign="top"><p>4 Shards</p></td>
<td align="left" valign="top"><p>limits: memory: 1400Mi , cpu:1500m</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>5000</p></td>
<td align="left" valign="top"><p>6 Shards</p></td>
<td align="left" valign="top"><p>limits: memory: 1400Mi , cpu:1500m</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>10000</p></td>
<td align="left" valign="top"><p>8 Shards</p></td>
<td align="left" valign="top"><p>limits: memory: 1400Mi , cpu:1500m</p></td>
</tr>
</tbody>
</table>
</div>
<div class="quoteblock">
<blockquote>
<p>The tests above were performed with Elastic Agent version 8.8 + TSDB Enabled and scraping period of <code class="literal">10sec</code> (for the Kubernetes integration). Those numbers are just indicators and should be validated per different Kubernetes policy configuration, along with applications that the Kubernetes cluster might include</p>
</blockquote>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>Tests have run until 10K pods per cluster. Scaling to bigger number of pods might require additional confguration from Kubernetes Side and Cloud Providers but the basic idea of installing Elastic Agent while horizontally scaling KSM remains the same.</p>
</div>
</div>
<h6><a id="agent-scheduling"></a>Agent Scheduling<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/ingest-docs/edit/main/docs/en/ingest-management/elastic-agent/scaling-on-kubernetes.asciidoc">edit</a></h6>
<p>Setting the low priority to Elastic Agent comparing to other pdos might also result to Elastic Agent being in Pending State.The scheduler tries to preempt (evict) lower priority Pods to make scheduling of the higher pending Pods possible.</p>
<p>Trying to prioritise the agent installation before rest of application microservices, <a href="https://github.com/elastic/elastic-agent/blob/main/docs/manifests/elastic-agent-managed-gke-autopilot.yaml#L8-L16" class="ulink" target="_top">PriorityClasses suggested</a></p>
<h6><a id="_kubernetes_policy_configuration"></a>Kubernetes Policy Configuration<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/ingest-docs/edit/main/docs/en/ingest-management/elastic-agent/scaling-on-kubernetes.asciidoc">edit</a></h6>
<p>Policy configuration of Kubernetes package can heavily affect the amount of metrics collected and finally ingested. Factors that should be considered in order to make your collection and ingestin lighter:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Scraping period of Kubernetes endpoints
</li>
<li class="listitem">
Disabling log collection
</li>
<li class="listitem">
Keep audit logs disabled
</li>
<li class="listitem">
Disable events dataset
</li>
<li class="listitem">
Disable Kubernetes control plane datasets in Cloud managed Kubernetes instances (see more info ** <a class="xref" href="running-on-gke-managed-by-fleet.html" title="Run Elastic Agent on GKE managed by Fleet">Run Elastic Agent on GKE managed by Fleet</a>, <a class="xref" href="running-on-eks-managed-by-fleet.html" title="Run Elastic Agent on Amazon EKS managed by Fleet">Run Elastic Agent on Amazon EKS managed by Fleet</a>, <a class="xref" href="running-on-aks-managed-by-fleet.html" title="Run Elastic Agent on Azure AKS managed by Fleet">Run Elastic Agent on Azure AKS managed by Fleet</a> pages)
</li>
</ul>
</div>
<p>User experience regarding Dashboard responses, is also affected from the size of data being requested. As dashbords can contain multiple visualisations, the general conisderation is to split visualisasations and group them according to the frequency of access. The less number of visualisations tends to improve user experience.</p>
<p>Additionally, <a href="https://github.com/elastic/integrations/blob/main/docs/dashboard_guidelines.md" class="ulink" target="_top">Dashboard Guidelines</a> is constantly updated also to track needs of observability at scale.</p>
<h6><a id="_elastic_stack_configuration"></a>Elastic Stack Configuration<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/ingest-docs/edit/main/docs/en/ingest-management/elastic-agent/scaling-on-kubernetes.asciidoc">edit</a></h6>
<p>The configuration of Elastic Stack needs to be taken under consideration in large scale deployments. In case of Elastic Cloud deployments the choice of the deployment <a href="/guide/en/cloud/current/ec-getting-started-profiles.html" class="ulink" target="_top">Elastic Cloud hardware profile</a> is important.</p>
<p>For heavy processing and big ingestion rate needs, the <code class="literal">CPU-optimised</code> profile is proposed.</p>
<h5><a id="validation-and-troubleshooting-practices"></a>Validation and Troubleshooting practices<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/ingest-docs/edit/main/docs/en/ingest-management/elastic-agent/scaling-on-kubernetes.asciidoc">edit</a></h5>
<h6><a id="_define_if_agents_are_collecting_as_expected"></a>Define if Agents are collecting as expected<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/ingest-docs/edit/main/docs/en/ingest-management/elastic-agent/scaling-on-kubernetes.asciidoc">edit</a></h6>
<p>After Elastic Agent deployment, we need to verify that Agent services are healthy, not restarting (stability) and that collection of metrics continues with expected rate (latency).</p>
<p><span class="strong strong"><strong>For stability:</strong></span></p>
<p>If Elastic Agent is configured as managed, in Kibana you can observe under <span class="strong strong"><strong>Fleet&gt;Agents</strong></span></p>
<div class="imageblock screenshot">
<div class="content">
<img src="../images/agent-status.png" alt="Elastic Agent Status">
</div>
</div>
<p>Additionally you can verify the process status with following commands:</p>
<div class="pre_wrapper lang-bash">
<pre class="programlisting prettyprint lang-bash">kubectl get pods -A | grep elastic
kube-system   elastic-agent-ltzkf                        1/1     Running   0          25h
kube-system   elastic-agent-qw6f4                        1/1     Running   0          25h
kube-system   elastic-agent-wvmpj                        1/1     Running   0          25h</pre>
</div>
<p>Find leader agent:</p>
<div class="pre_wrapper lang-bash">
<pre class="programlisting prettyprint lang-bash">❯ k get leases -n kube-system | grep elastic
NAME                                      HOLDER                                                                       AGE
elastic-agent-cluster-leader   elastic-agent-leader-elastic-agent-qw6f4                                     25h</pre>
</div>
<p>Exec into Leader agent and verify the process status:</p>
<div class="pre_wrapper lang-bash">
<pre class="programlisting prettyprint lang-bash">❯ kubectl exec -ti -n kube-system elastic-agent-qw6f4 -- bash
root@gke-gke-scaling-gizas-te-default-pool-6689889a-sz02:/usr/share/elastic-agent# ./elastic-agent status
State: HEALTHY
Message: Running
Fleet State: HEALTHY
Fleet Message: (no message)
Components:
  * kubernetes/metrics  (HEALTHY)
                        Healthy: communicating with pid '42423'
  * filestream          (HEALTHY)
                        Healthy: communicating with pid '42431'
  * filestream          (HEALTHY)
                        Healthy: communicating with pid '42443'
  * beat/metrics        (HEALTHY)
                        Healthy: communicating with pid '42453'
  * http/metrics        (HEALTHY)
                        Healthy: communicating with pid '42462'</pre>
</div>
<p>It is a common problem of lack of CPU/memory resources that agent process restart as Kubernetes size grows. In the logs of agent you</p>
<div class="pre_wrapper lang-json">
<pre class="programlisting prettyprint lang-json">kubectl logs -n kube-system elastic-agent-qw6f4 | grep "kubernetes/metrics"
[ouptut truncated ...]

(HEALTHY-&gt;STOPPED): Suppressing FAILED state due to restart for '46554' exited with code '-1'","log":{"source":"elastic-agent"},"component":{"id":"kubernetes/metrics-default","state":"STOPPED"},"unit":{"id":"kubernetes/metrics-default-kubernetes/metrics-kube-state-metrics-c6180794-70ce-4c0d-b775-b251571b6d78","type":"input","state":"STOPPED","old_state":"HEALTHY"},"ecs.version":"1.6.0"}
{"log.level":"info","@timestamp":"2023-04-03T09:33:38.919Z","log.origin":{"file.name":"coordinator/coordinator.go","file.line":861},"message":"Unit state changed kubernetes/metrics-default-kubernetes/metrics-kube-apiserver-c6180794-70ce-4c0d-b775-b251571b6d78 (HEALTHY-&gt;STOPPED): Suppressing FAILED state due to restart for '46554' exited with code '-1'","log":{"source":"elastic-agent"}</pre>
</div>
<p>You can verify the instant resource consumption by running <code class="literal">top pod</code> command and indentify if agents are close to the limits you have specified in your manifest.</p>
<div class="pre_wrapper lang-bash">
<pre class="programlisting prettyprint lang-bash">kubectl top pod  -n kube-system | grep elastic
NAME                                                             CPU(cores)   MEMORY(bytes)
elastic-agent-ltzkf                                              30m          354Mi
elastic-agent-qw6f4                                              67m          467Mi
elastic-agent-wvmpj                                              27m          357Mi</pre>
</div>
<h6><a id="_verify_ingestion_latency"></a>Verify Ingestion Latency<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/ingest-docs/edit/main/docs/en/ingest-management/elastic-agent/scaling-on-kubernetes.asciidoc">edit</a></h6>
<p>Kibana Discovery can be used to identify frequency of your metrics being ingested.</p>
<p>Filter for Pod dataset:</p>
<div class="imageblock screenshot">
<div class="content">
<img src="../images/pod-latency.png" alt="Kubernetes Pod Metricset">
</div>
</div>
<p>Filter for State_Pod dataset</p>
<div class="imageblock screenshot">
<div class="content">
<img src="../images/state-pod.png" alt="Kubernetes State Pod Metricset">
</div>
</div>
<p>Identify how many events have been sent to Elasticsearch:</p>
<div class="pre_wrapper lang-bash">
<pre class="programlisting prettyprint lang-bash">kubectl logs -n kube-system elastic-agent-h24hh -f | grep -i state_pod
[ouptut truncated ...]

"state_pod":{"events":2936,"success":2936}</pre>
</div>
<p>The number of events denotes the number of documents that should be depicted inside Kibana Discovery page.</p>
<div class="quoteblock">
<blockquote>
<p>For eg, in a cluster with 798 pods, then 798 docs should be depicted in block of ingestion inside Kibana</p>
</blockquote>
</div>
<h6><a id="_define_if_elasticsearch_is_the_bottleneck_of_ingestion"></a>Define if Elasticsearch is the bottleneck of ingestion<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/ingest-docs/edit/main/docs/en/ingest-management/elastic-agent/scaling-on-kubernetes.asciidoc">edit</a></h6>
<p>In some cases maybe the Elasticsearch can not cope with the rate of data that are trying to be ingested. In order to verify the resource utilisation, installation of [Elastic StackMonitoring Cluster](<a href="/guide/en/elasticsearch/reference/current/monitoring-overview.html" class="ulink" target="_top">https://www.elastic.co/guide/en/elasticsearch/reference/current/monitoring-overview.html</a>) is advised.</p>
<p>Additionally, in Elastic Cloud deployments you can navigate to <span class="strong strong"><strong>Manage Deployment &gt; Deployments &gt; Monitoring &gt; Performance</strong></span>.
Corresponding dashboards for <code class="literal">CPU Usage</code>, <code class="literal">Index Response Times</code> and <code class="literal">Memory Pressure</code> can reveal possible problems and suggest vertical scaling of Elastic Stack resources.</p>
<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="_relevant_links"></a>Relevant links<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/ingest-docs/edit/main/docs/en/ingest-management/elastic-agent/scaling-on-kubernetes.asciidoc">edit</a></h4>
</div></div></div>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a href="/guide/en/welcome-to-elastic/current/getting-started-kubernetes.html" class="ulink" target="_top">Monitor Kubernetes Infrastructure</a>
</li>
<li class="listitem">
<a href="/blog/kubernetes-cluster-metrics-logs-monitoring" class="ulink" target="_top">Blog: Managing your Kubernetes cluster with Elastic Observability</a>
</li>
</ul>
</div>
</div>

</div>
<div class="navfooter">
<span class="prev">
<a href="running-on-kubernetes-standalone.html">« Run Elastic Agent Standalone on Kubernetes</a>
</span>
<span class="next">
<a href="agent-environment-variables.html">Elastic Agent environment variables »</a>
</span>
</div>
</div>
</body>
</html>
