<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Apache Spark Integration | Elastic integrations | Elastic</title>
<meta class="elastic" name="content" content="Apache Spark Integration | Elastic integrations">

<link rel="home" href="index.html" title="Elastic integrations"/>
<link rel="up" href="apache-intro.html" title="Apache"/>
<link rel="prev" href="apache.html" title="Apache Integration"/>
<link rel="next" href="apache_tomcat.html" title="Apache Tomcat Integration"/>
<meta class="elastic" name="product_version" content="main"/>
<meta class="elastic" name="product_name" content="Integrations"/>
<meta class="elastic" name="website_area" content="documentation"/>
<meta name="DC.type" content="Learn/Docs/Integrations/Reference"/>
<meta name="DC.subject" content="Integrations"/>
<meta name="DC.identifier" content="main"/>
</head>
<body>
<div class="navheader">
<span class="prev">
<a href="apache.html">« Apache Integration</a>
</span>
<span class="next">
<a href="apache_tomcat.html">Apache Tomcat Integration »</a>
</span>
</div>
<div class="book" lang="en">
<div class="titlepage">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="index.html">Elastic integrations</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="apache-intro.html">Apache</a></span>
</div>
<div>
<div><h1 class="title"><a id="id-1"></a>Apache Spark Integration</h1><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/integration-docs/edit/main/dist/apache_spark/index.asciidoc">edit</a></div>
</div>
<!--EXTRA-->
</div>
<div id="content">
<div class="section">
<div class="titlepage"><div><div>
<div class="position-relative"><h2 class="title"><a id="apache_spark"></a>Apache Spark Integration</h2><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/integration-docs/edit/main/dist/apache_spark/index.asciidoc">edit</a></div>
</div></div></div>

<div class="condensed-table">
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
</colgroup>
<tbody>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>Version</strong></span></p></td>
<td align="left" valign="top"><p>1.3.1 (<a class="xref" href="apache_spark.html#apache_spark-changelog" title="Changelog">View all</a>)</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>Compatible Kibana version(s)</strong></span></p></td>
<td align="left" valign="top"><p>8.13.0 or higher</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>Supported Serverless project types</strong></span><br>
 <a class="xref" href="serverless-support.html" title="Supported Serverless project types"><span class="small">What&#8217;s this?</span></a></p></td>
<td align="left" valign="top"><p>Security<br>
Observability</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>Subscription level</strong></span><br>
<a href="/subscriptions" class="ulink" target="_top"><span class="small">What&#8217;s this?</span></a></p></td>
<td align="left" valign="top"><p>Basic</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>Level of support</strong></span><br>
<a class="xref" href="support.html" title="Level of support"><span class="small">What&#8217;s this?</span></a></p></td>
<td align="left" valign="top"><p>Elastic</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="position-relative"><h4><a id="apache_spark-overview"></a>Overview</h4><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/integration-docs/edit/main/dist/apache_spark/index.asciidoc">edit</a></div>
<p><a href="https://spark.apache.org" class="ulink" target="_top">Apache Spark</a> is an open-source, distributed computing system that provides a fast and general-purpose cluster-computing framework. It offers in-memory data processing capabilities, which significantly enhances the performance of big data analytics applications. Spark provides support for a variety of programming languages including Scala, Python, Java, and R, and comes with built-in modules for SQL, streaming, machine learning, and graph processing. This makes it a versatile tool for a wide range of data processing and analysis tasks.</p>
<p>Use the Apache Spark integration to:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Collect metrics related to the application, driver, executor and node.
</li>
<li class="listitem">
Create visualizations to monitor, measure, and analyze usage trends and key data, deriving business insights.
</li>
<li class="listitem">
Create alerts to reduce the MTTD and MTTR by referencing relevant logs when troubleshooting an issue.
</li>
</ul>
</div>
<div class="position-relative"><h4><a id="apache_spark-data-streams"></a>Data streams</h4><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/integration-docs/edit/main/dist/apache_spark/index.asciidoc">edit</a></div>
<p>The Apache Spark integration collects metrics data.</p>
<p>Metrics provide insight into the statistics of Apache Spark. The <code class="literal">Metric</code> data streams collected by the Apache Spark integration include <code class="literal">application</code>, <code class="literal">driver</code>, <code class="literal">executor</code>, and <code class="literal">node</code>, allowing users to monitor and troubleshoot the performance of their Apache Spark instance.</p>
<p>Data streams:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">application</code>: Collects information related to the number of cores used, application name, runtime in milliseconds and current status of the application.
</li>
<li class="listitem">
<code class="literal">driver</code>: Collects information related to the driver details, job durations, task execution, memory usage, executor status and JVM metrics.
</li>
<li class="listitem">
<code class="literal">executor</code>: Collects information related to the operations, memory usage, garbage collection, file handling, and threadpool activity.
</li>
<li class="listitem">
<code class="literal">node</code>: Collects information related to the application count, waiting applications, worker metrics, executor count, core usage and memory usage.
</li>
</ul>
</div>
<p>Note:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Users can monitor and view the metrics inside the ingested documents for Apache Spark under the <code class="literal">metrics-*</code> index pattern in <code class="literal">Discover</code>.
</li>
</ul>
</div>
<div class="position-relative"><h4><a id="apache_spark-compatibility"></a>Compatibility</h4><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/integration-docs/edit/main/dist/apache_spark/index.asciidoc">edit</a></div>
<p>This integration has been tested against <code class="literal">Apache Spark version 3.5.0</code>.</p>
<div class="position-relative"><h4><a id="apache_spark-requirements"></a>Requirements</h4><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/integration-docs/edit/main/dist/apache_spark/index.asciidoc">edit</a></div>
<p>You need Elasticsearch for storing and searching your data and Kibana for visualizing and managing it. You can use our hosted Elasticsearch Service on Elastic Cloud, which is recommended, or self-manage the Elastic Stack on your own hardware.</p>
<p>In order to ingest data from Apache Spark, you must know the full hosts for the Main and Worker nodes.</p>
<p>To proceed with the Jolokia setup, Apache Spark should be installed as a standalone setup. Make sure that the spark folder is installed in the <code class="literal">/usr/local</code> path. If not, then specify the path of spark folder in the further steps. You can install the standalone setup from the official download page of <a href="https://spark.apache.org/downloads.html" class="ulink" target="_top">Apache Spark</a>.</p>
<p>In order to gather Spark statistics, we need to download and enable Jolokia JVM Agent.</p>
<div class="pre_wrapper lang-txt">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-txt">cd /usr/share/java/
wget -O jolokia-agent.jar http://search.maven.org/remotecontent?filepath=org/jolokia/jolokia-jvm/1.3.6/jolokia-jvm-1.3.6-agent.jar</pre>
</div>
<p>As far, as Jolokia JVM Agent is downloaded, we should configure Apache Spark, to use it as JavaAgent and expose metrics via HTTP/JSON. Edit spark-env.sh. It should be in <code class="literal">/usr/local/spark/conf</code> and add following parameters (Assuming that spark install folder is <code class="literal">/usr/local/spark</code>, if not change the path to one on which Spark is installed):</p>
<div class="pre_wrapper lang-txt">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-txt">export SPARK_MASTER_OPTS="$SPARK_MASTER_OPTS -javaagent:/usr/share/java/jolokia-agent.jar=config=/usr/local/spark/conf/jolokia-master.properties"</pre>
</div>
<p>Now, create <code class="literal">/usr/local/spark/conf/jolokia-master.properties</code> file with following content:</p>
<div class="pre_wrapper lang-txt">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-txt">host=0.0.0.0
port=7777
agentContext=/jolokia
backlog=100

policyLocation=file:///usr/local/spark/conf/jolokia.policy
historyMaxEntries=10
debug=false
debugMaxEntries=100
maxDepth=15
maxCollectionSize=1000
maxObjects=0</pre>
</div>
<p>Now we need to create /usr/local/spark/conf/jolokia.policy with following content:</p>
<div class="pre_wrapper lang-xml">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-xml">&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;restrict&gt;
  &lt;http&gt;
    &lt;method&gt;get&lt;/method&gt;
    &lt;method&gt;post&lt;/method&gt;
  &lt;/http&gt;
  &lt;commands&gt;
    &lt;command&gt;read&lt;/command&gt;
  &lt;/commands&gt;
&lt;/restrict&gt;</pre>
</div>
<p>Configure Agent with following in conf/bigdata.ini file:</p>
<div class="pre_wrapper lang-txt">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-txt">[Spark-Master]
stats: http://127.0.0.1:7777/jolokia/read</pre>
</div>
<p>Restart Spark master.</p>
<p>Follow the same set of steps for Spark Worker, Driver and Executor.</p>
<div class="position-relative"><h4><a id="apache_spark-setup"></a>Setup</h4><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/integration-docs/edit/main/dist/apache_spark/index.asciidoc">edit</a></div>
<p>For step-by-step instructions on how to set up an integration, see the <a href="/guide/en/starting-with-the-elasticsearch-platform-and-its-solutions/current/getting-started-observability.html" class="ulink" target="_top">Getting Started</a> guide.</p>
<div class="position-relative"><h4><a id="apache_spark-validation"></a>Validation</h4><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/integration-docs/edit/main/dist/apache_spark/index.asciidoc">edit</a></div>
<p>After the integration is successfully configured, click on the <em>Assets</em> tab of the Apache Spark Integration to display the available dashboards. Select the dashboard for your configured data stream, which should be populated with the required data.</p>
<div class="position-relative"><h4><a id="apache_spark-troubleshooting"></a>Troubleshooting</h4><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/integration-docs/edit/main/dist/apache_spark/index.asciidoc">edit</a></div>
<p>If <code class="literal">host.ip</code> appears conflicted under the <code class="literal">metrics-*</code> data view, this issue can be resolved by <a href="/guide/en/elasticsearch/reference/current/tsds-reindex.html" class="ulink" target="_top">reindexing</a> the <code class="literal">Application</code>, <code class="literal">Driver</code>, <code class="literal">Executor</code> and <code class="literal">Node</code> data stream.</p>
<div class="position-relative"><h4><a id="apache_spark-metrics"></a>Metrics</h4><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/integration-docs/edit/main/dist/apache_spark/index.asciidoc">edit</a></div>
<div class="position-relative"><h5><a id="apache_spark-application"></a>Application</h5><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/integration-docs/edit/main/dist/apache_spark/index.asciidoc">edit</a></div>
<p>The <code class="literal">application</code> data stream collects metrics related to the number of cores used, application name, runtime in milliseconds, and current status of the application.</p>
<details>
<summary class="title"><span class="strong strong"><strong>Example</strong></span></summary>
<div class="content">
<p>An example event for <code class="literal">application</code> looks as following:</p>
<div class="pre_wrapper lang-json">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-json">{
    "@timestamp": "2023-09-28T09:24:33.812Z",
    "agent": {
        "ephemeral_id": "20d060ec-da41-4f14-a187-d020b9fbec7d",
        "id": "a6bdbb4a-4bac-4243-83cb-dba157f24987",
        "name": "docker-fleet-agent",
        "type": "metricbeat",
        "version": "8.8.0"
    },
    "apache_spark": {
        "application": {
            "cores": 8,
            "mbean": "metrics:name=application.PythonWordCount.1695893057562.cores,type=gauges",
            "name": "PythonWordCount.1695893057562"
        }
    },
    "data_stream": {
        "dataset": "apache_spark.application",
        "namespace": "ep",
        "type": "metrics"
    },
    "ecs": {
        "version": "8.11.0"
    },
    "elastic_agent": {
        "id": "a6bdbb4a-4bac-4243-83cb-dba157f24987",
        "snapshot": false,
        "version": "8.8.0"
    },
    "event": {
        "agent_id_status": "verified",
        "dataset": "apache_spark.application",
        "duration": 23828342,
        "ingested": "2023-09-28T09:24:37Z",
        "kind": "metric",
        "module": "apache_spark",
        "type": [
            "info"
        ]
    },
    "host": {
        "architecture": "x86_64",
        "containerized": true,
        "hostname": "docker-fleet-agent",
        "id": "e8978f2086c14e13b7a0af9ed0011d19",
        "ip": [
            "172.20.0.7"
        ],
        "mac": [
            "02-42-C0-A8-F5-07"
        ],
        "name": "docker-fleet-agent",
        "os": {
            "codename": "focal",
            "family": "debian",
            "kernel": "3.10.0-1160.90.1.el7.x86_64",
            "name": "Ubuntu",
            "platform": "ubuntu",
            "type": "linux",
            "version": "20.04.6 LTS (Focal Fossa)"
        }
    },
    "metricset": {
        "name": "jmx",
        "period": 60000
    },
    "service": {
        "address": "http://apache-spark-main:7777/jolokia/%3FignoreErrors=true&amp;canonicalNaming=false",
        "type": "jolokia"
    }
}</pre>
</div>
</div>
</details>
<p><span class="strong strong"><strong>ECS Field Reference</strong></span></p>
<p>Please refer to the following <a href="/guide/en/ecs/current/ecs-field-reference.html" class="ulink" target="_top">document</a> for detailed information on ECS fields.</p>
<details>
<summary class="title"><span class="strong strong"><strong>Exported fields</strong></span></summary>
<div class="content">
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
<col class="col_4"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Field</th>
<th align="left" valign="top">Description</th>
<th align="left" valign="top">Type</th>
<th align="left" valign="top">Metric Type</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p>@timestamp</p></td>
<td align="left" valign="top"><p>Event timestamp.</p></td>
<td align="left" valign="top"><p>date</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>agent.id</p></td>
<td align="left" valign="top"><p>Unique identifier of this agent (if one exists). Example: For Beats this would be beat.id.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.application.cores</p></td>
<td align="left" valign="top"><p>Number of cores.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.application.mbean</p></td>
<td align="left" valign="top"><p>The name of the jolokia mbean.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.application.name</p></td>
<td align="left" valign="top"><p>Name of the application.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.application.runtime.ms</p></td>
<td align="left" valign="top"><p>Time taken to run the application (ms).</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.application.status</p></td>
<td align="left" valign="top"><p>Current status of the application.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>cloud.account.id</p></td>
<td align="left" valign="top"><p>The cloud account or organization id used to identify different entities in a multi-tenant environment. Examples: AWS account id, Google Cloud ORG Id, or other unique identifier.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>cloud.availability_zone</p></td>
<td align="left" valign="top"><p>Availability zone in which this host, resource, or service is located.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>cloud.instance.id</p></td>
<td align="left" valign="top"><p>Instance ID of the host machine.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>cloud.provider</p></td>
<td align="left" valign="top"><p>Name of the cloud provider. Example values are aws, azure, gcp, or digitalocean.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>cloud.region</p></td>
<td align="left" valign="top"><p>Region in which this host, resource, or service is located.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>container.id</p></td>
<td align="left" valign="top"><p>Unique container id.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>data_stream.dataset</p></td>
<td align="left" valign="top"><p>Data stream dataset.</p></td>
<td align="left" valign="top"><p>constant_keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>data_stream.namespace</p></td>
<td align="left" valign="top"><p>Data stream namespace.</p></td>
<td align="left" valign="top"><p>constant_keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>data_stream.type</p></td>
<td align="left" valign="top"><p>Data stream type.</p></td>
<td align="left" valign="top"><p>constant_keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>host.name</p></td>
<td align="left" valign="top"><p>Name of the host. It can contain what hostname returns on Unix systems, the fully qualified domain name (FQDN), or a name specified by the user. The recommended value is the lowercase FQDN of the host.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>service.address</p></td>
<td align="left" valign="top"><p>Address where data about this service was collected from. This should be a URI, network address (ipv4:port or [ipv6]:port) or a resource path (sockets).</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
</tbody>
</table>
</div>
</div>
</details>
<div class="position-relative"><h5><a id="apache_spark-driver"></a>Driver</h5><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/integration-docs/edit/main/dist/apache_spark/index.asciidoc">edit</a></div>
<p>The <code class="literal">driver</code> data stream collects metrics related to the driver details, job durations, task execution, memory usage, executor status, and JVM metrics.</p>
<details>
<summary class="title"><span class="strong strong"><strong>Example</strong></span></summary>
<div class="content">
<p>An example event for <code class="literal">driver</code> looks as following:</p>
<div class="pre_wrapper lang-json">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-json">{
    "@timestamp": "2023-09-29T12:04:40.050Z",
    "agent": {
        "ephemeral_id": "e3534e18-b92f-4b1b-bd39-43ff9c8849d4",
        "id": "a76f5e50-2a98-4b96-80f6-026ad822e3e8",
        "name": "docker-fleet-agent",
        "type": "metricbeat",
        "version": "8.8.0"
    },
    "apache_spark": {
        "driver": {
            "application_name": "app-20230929120427-0000",
            "jvm": {
                "cpu": {
                    "time": 25730000000
                }
            },
            "mbean": "metrics:name=app-20230929120427-0000.driver.JVMCPU.jvmCpuTime,type=gauges"
        }
    },
    "data_stream": {
        "dataset": "apache_spark.driver",
        "namespace": "ep",
        "type": "metrics"
    },
    "ecs": {
        "version": "8.11.0"
    },
    "elastic_agent": {
        "id": "a76f5e50-2a98-4b96-80f6-026ad822e3e8",
        "snapshot": false,
        "version": "8.8.0"
    },
    "event": {
        "agent_id_status": "verified",
        "dataset": "apache_spark.driver",
        "duration": 177706950,
        "ingested": "2023-09-29T12:04:41Z",
        "kind": "metric",
        "module": "apache_spark",
        "type": [
            "info"
        ]
    },
    "host": {
        "architecture": "x86_64",
        "containerized": true,
        "hostname": "docker-fleet-agent",
        "id": "e8978f2086c14e13b7a0af9ed0011d19",
        "ip": [
            "172.26.0.7"
        ],
        "mac": [
            "02-42-AC-1A-00-07"
        ],
        "name": "docker-fleet-agent",
        "os": {
            "codename": "focal",
            "family": "debian",
            "kernel": "3.10.0-1160.90.1.el7.x86_64",
            "name": "Ubuntu",
            "platform": "ubuntu",
            "type": "linux",
            "version": "20.04.6 LTS (Focal Fossa)"
        }
    },
    "metricset": {
        "name": "jmx",
        "period": 60000
    },
    "service": {
        "address": "http://apache-spark-main:7779/jolokia/%3FignoreErrors=true&amp;canonicalNaming=false",
        "type": "jolokia"
    }
}</pre>
</div>
</div>
</details>
<p><span class="strong strong"><strong>ECS Field Reference</strong></span></p>
<p>Please refer to the following <a href="/guide/en/ecs/current/ecs-field-reference.html" class="ulink" target="_top">document</a> for detailed information on ECS fields.</p>
<details>
<summary class="title"><span class="strong strong"><strong>Exported fields</strong></span></summary>
<div class="content">
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
<col class="col_4"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Field</th>
<th align="left" valign="top">Description</th>
<th align="left" valign="top">Type</th>
<th align="left" valign="top">Metric Type</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p>@timestamp</p></td>
<td align="left" valign="top"><p>Event timestamp.</p></td>
<td align="left" valign="top"><p>date</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>agent.id</p></td>
<td align="left" valign="top"><p>Unique identifier of this agent (if one exists). Example: For Beats this would be beat.id.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.application_name</p></td>
<td align="left" valign="top"><p>Name of the application.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.dag_scheduler.job.active</p></td>
<td align="left" valign="top"><p>Number of active jobs.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.dag_scheduler.job.all</p></td>
<td align="left" valign="top"><p>Total number of jobs.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.dag_scheduler.stages.failed</p></td>
<td align="left" valign="top"><p>Number of failed stages.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.dag_scheduler.stages.running</p></td>
<td align="left" valign="top"><p>Number of running stages.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.dag_scheduler.stages.waiting</p></td>
<td align="left" valign="top"><p>Number of waiting stages</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.disk.space_used</p></td>
<td align="left" valign="top"><p>Amount of the disk space utilized in MB.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.executor_metrics.gc.major.count</p></td>
<td align="left" valign="top"><p>Total major GC count. For example, the garbage collector is one of MarkSweepCompact, PS MarkSweep, ConcurrentMarkSweep, G1 Old Generation and so on.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.executor_metrics.gc.major.time</p></td>
<td align="left" valign="top"><p>Elapsed total major GC time. The value is expressed in milliseconds.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.executor_metrics.gc.minor.count</p></td>
<td align="left" valign="top"><p>Total minor GC count. For example, the garbage collector is one of Copy, PS Scavenge, ParNew, G1 Young Generation and so on.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.executor_metrics.gc.minor.time</p></td>
<td align="left" valign="top"><p>Elapsed total minor GC time. The value is expressed in milliseconds.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.executor_metrics.heap_memory.off.execution</p></td>
<td align="left" valign="top"><p>Peak off heap execution memory in use, in bytes.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.executor_metrics.heap_memory.off.storage</p></td>
<td align="left" valign="top"><p>Peak off heap storage memory in use, in bytes.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.executor_metrics.heap_memory.off.unified</p></td>
<td align="left" valign="top"><p>Peak off heap memory (execution and storage).</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.executor_metrics.heap_memory.on.execution</p></td>
<td align="left" valign="top"><p>Peak on heap execution memory in use, in bytes.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.executor_metrics.heap_memory.on.storage</p></td>
<td align="left" valign="top"><p>Peak on heap storage memory in use, in bytes.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.executor_metrics.heap_memory.on.unified</p></td>
<td align="left" valign="top"><p>Peak on heap memory (execution and storage).</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.executor_metrics.memory.direct_pool</p></td>
<td align="left" valign="top"><p>Peak memory that the JVM is using for direct buffer pool.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.executor_metrics.memory.jvm.heap</p></td>
<td align="left" valign="top"><p>Peak memory usage of the heap that is used for object allocation.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.executor_metrics.memory.jvm.off_heap</p></td>
<td align="left" valign="top"><p>Peak memory usage of non-heap memory that is used by the Java virtual machine.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.executor_metrics.memory.mapped_pool</p></td>
<td align="left" valign="top"><p>Peak memory that the JVM is using for mapped buffer pool</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.executor_metrics.process_tree.jvm.rss_memory</p></td>
<td align="left" valign="top"><p>Resident Set Size: number of pages the process has in real memory. This is just the pages which count toward text, data, or stack space. This does not include pages which have not been demand-loaded in, or which are swapped out.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.executor_metrics.process_tree.jvm.v_memory</p></td>
<td align="left" valign="top"><p>Virtual memory size in bytes.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.executor_metrics.process_tree.other.rss_memory</p></td>
<td align="left" valign="top"><p></p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.executor_metrics.process_tree.other.v_memory</p></td>
<td align="left" valign="top"><p></p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.executor_metrics.process_tree.python.rss_memory</p></td>
<td align="left" valign="top"><p></p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.executor_metrics.process_tree.python.v_memory</p></td>
<td align="left" valign="top"><p></p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.executors.all</p></td>
<td align="left" valign="top"><p>Total number of executors.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.executors.decommission_unfinished</p></td>
<td align="left" valign="top"><p>Total number of decommissioned unfinished executors.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.executors.exited_unexpectedly</p></td>
<td align="left" valign="top"><p>Total number of executors exited unexpectedly.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.executors.gracefully_decommissioned</p></td>
<td align="left" valign="top"><p>Total number of executors gracefully decommissioned.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.executors.killed_by_driver</p></td>
<td align="left" valign="top"><p>Total number of executors killed by driver.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.executors.max_needed</p></td>
<td align="left" valign="top"><p>Maximum number of executors needed.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.executors.pending_to_remove</p></td>
<td align="left" valign="top"><p>Total number of executors pending to be removed.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.executors.target</p></td>
<td align="left" valign="top"><p>Total number of target executors.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.executors.to_add</p></td>
<td align="left" valign="top"><p>Total number of executors to be added.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.hive_external_catalog.file_cache_hits</p></td>
<td align="left" valign="top"><p>Total number of file cache hits.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.hive_external_catalog.files_discovered</p></td>
<td align="left" valign="top"><p>Total number of files discovered.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.hive_external_catalog.hive_client_calls</p></td>
<td align="left" valign="top"><p>Total number of Hive Client calls.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.hive_external_catalog.parallel_listing_job.count</p></td>
<td align="left" valign="top"><p>Number of jobs running parallely.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.hive_external_catalog.partitions_fetched</p></td>
<td align="left" valign="top"><p>Number of partitions fetched.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.job_duration</p></td>
<td align="left" valign="top"><p>Duration of the job.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.jobs.failed</p></td>
<td align="left" valign="top"><p>Number of failed jobs.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.jobs.succeeded</p></td>
<td align="left" valign="top"><p>Number of successful jobs.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.jvm.cpu.time</p></td>
<td align="left" valign="top"><p>Elapsed CPU time the JVM spent.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.mbean</p></td>
<td align="left" valign="top"><p>The name of the jolokia mbean.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.memory.max_mem</p></td>
<td align="left" valign="top"><p>Maximum amount of memory available for storage, in MB.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.memory.off_heap.max</p></td>
<td align="left" valign="top"><p>Maximum amount of off heap memory available, in MB.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.memory.off_heap.remaining</p></td>
<td align="left" valign="top"><p>Remaining amount of off heap memory, in MB.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.memory.off_heap.used</p></td>
<td align="left" valign="top"><p>Total amount of off heap memory used, in MB.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.memory.on_heap.max</p></td>
<td align="left" valign="top"><p>Maximum amount of on heap memory available, in MB.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.memory.on_heap.remaining</p></td>
<td align="left" valign="top"><p>Remaining amount of on heap memory, in MB.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.memory.on_heap.used</p></td>
<td align="left" valign="top"><p>Total amount of on heap memory used, in MB.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.memory.remaining</p></td>
<td align="left" valign="top"><p>Remaining amount of storage memory, in MB.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.memory.used</p></td>
<td align="left" valign="top"><p>Total amount of memory used for storage, in MB.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.spark.streaming.event_time.watermark</p></td>
<td align="left" valign="top"><p></p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.spark.streaming.input_rate.total</p></td>
<td align="left" valign="top"><p>Total rate of the input.</p></td>
<td align="left" valign="top"><p>double</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.spark.streaming.latency</p></td>
<td align="left" valign="top"><p></p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.spark.streaming.processing_rate.total</p></td>
<td align="left" valign="top"><p>Total rate of processing.</p></td>
<td align="left" valign="top"><p>double</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.spark.streaming.states.rows.total</p></td>
<td align="left" valign="top"><p>Total number of rows.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.spark.streaming.states.used_bytes</p></td>
<td align="left" valign="top"><p>Total number of bytes utilized.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.stages.completed_count</p></td>
<td align="left" valign="top"><p>Total number of completed stages.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.stages.failed_count</p></td>
<td align="left" valign="top"><p>Total number of failed stages.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.stages.skipped_count</p></td>
<td align="left" valign="top"><p>Total number of skipped stages.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.tasks.completed</p></td>
<td align="left" valign="top"><p>Number of completed tasks.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.tasks.executors.black_listed</p></td>
<td align="left" valign="top"><p>Number of blacklisted executors for the tasks.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.tasks.executors.excluded</p></td>
<td align="left" valign="top"><p>Number of excluded executors for the tasks.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.tasks.executors.unblack_listed</p></td>
<td align="left" valign="top"><p>Number of unblacklisted executors for the tasks.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.tasks.executors.unexcluded</p></td>
<td align="left" valign="top"><p>Number of unexcluded executors for the tasks.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.tasks.failed</p></td>
<td align="left" valign="top"><p>Number of failed tasks.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.tasks.killed</p></td>
<td align="left" valign="top"><p>Number of killed tasks.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.driver.tasks.skipped</p></td>
<td align="left" valign="top"><p>Number of skipped tasks.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>cloud.account.id</p></td>
<td align="left" valign="top"><p>The cloud account or organization id used to identify different entities in a multi-tenant environment. Examples: AWS account id, Google Cloud ORG Id, or other unique identifier.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>cloud.availability_zone</p></td>
<td align="left" valign="top"><p>Availability zone in which this host, resource, or service is located.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>cloud.instance.id</p></td>
<td align="left" valign="top"><p>Instance ID of the host machine.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>cloud.provider</p></td>
<td align="left" valign="top"><p>Name of the cloud provider. Example values are aws, azure, gcp, or digitalocean.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>cloud.region</p></td>
<td align="left" valign="top"><p>Region in which this host, resource, or service is located.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>container.id</p></td>
<td align="left" valign="top"><p>Unique container id.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>data_stream.dataset</p></td>
<td align="left" valign="top"><p>Data stream dataset.</p></td>
<td align="left" valign="top"><p>constant_keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>data_stream.namespace</p></td>
<td align="left" valign="top"><p>Data stream namespace.</p></td>
<td align="left" valign="top"><p>constant_keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>data_stream.type</p></td>
<td align="left" valign="top"><p>Data stream type.</p></td>
<td align="left" valign="top"><p>constant_keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>host.name</p></td>
<td align="left" valign="top"><p>Name of the host. It can contain what hostname returns on Unix systems, the fully qualified domain name (FQDN), or a name specified by the user. The recommended value is the lowercase FQDN of the host.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>service.address</p></td>
<td align="left" valign="top"><p>Address where data about this service was collected from. This should be a URI, network address (ipv4:port or [ipv6]:port) or a resource path (sockets).</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
</tbody>
</table>
</div>
</div>
</details>
<div class="position-relative"><h5><a id="apache_spark-executor"></a>Executor</h5><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/integration-docs/edit/main/dist/apache_spark/index.asciidoc">edit</a></div>
<p>The <code class="literal">executor</code> data stream collects metrics related to the operations, memory usage, garbage collection, file handling, and threadpool activity.</p>
<details>
<summary class="title"><span class="strong strong"><strong>Example</strong></span></summary>
<div class="content">
<p>An example event for <code class="literal">executor</code> looks as following:</p>
<div class="pre_wrapper lang-json">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-json">{
    "@timestamp": "2023-09-28T09:26:45.771Z",
    "agent": {
        "ephemeral_id": "3a3db920-eb4b-4045-b351-33526910ae8a",
        "id": "a6bdbb4a-4bac-4243-83cb-dba157f24987",
        "name": "docker-fleet-agent",
        "type": "metricbeat",
        "version": "8.8.0"
    },
    "apache_spark": {
        "executor": {
            "application_name": "app-20230928092630-0000",
            "id": "0",
            "jvm": {
                "cpu_time": 20010000000
            },
            "mbean": "metrics:name=app-20230928092630-0000.0.JVMCPU.jvmCpuTime,type=gauges"
        }
    },
    "data_stream": {
        "dataset": "apache_spark.executor",
        "namespace": "ep",
        "type": "metrics"
    },
    "ecs": {
        "version": "8.11.0"
    },
    "elastic_agent": {
        "id": "a6bdbb4a-4bac-4243-83cb-dba157f24987",
        "snapshot": false,
        "version": "8.8.0"
    },
    "event": {
        "agent_id_status": "verified",
        "dataset": "apache_spark.executor",
        "duration": 2849184715,
        "ingested": "2023-09-28T09:26:49Z",
        "kind": "metric",
        "module": "apache_spark",
        "type": [
            "info"
        ]
    },
    "host": {
        "architecture": "x86_64",
        "containerized": true,
        "hostname": "docker-fleet-agent",
        "id": "e8978f2086c14e13b7a0af9ed0011d19",
        "ip": [
            "172.20.0.7"
        ],
        "mac": [
            "02-42-AC-14-00-07"
        ],
        "name": "docker-fleet-agent",
        "os": {
            "codename": "focal",
            "family": "debian",
            "kernel": "3.10.0-1160.90.1.el7.x86_64",
            "name": "Ubuntu",
            "platform": "ubuntu",
            "type": "linux",
            "version": "20.04.6 LTS (Focal Fossa)"
        }
    },
    "metricset": {
        "name": "jmx",
        "period": 60000
    },
    "service": {
        "address": "http://apache-spark-main:7780/jolokia/%3FignoreErrors=true&amp;canonicalNaming=false",
        "type": "jolokia"
    }
}</pre>
</div>
</div>
</details>
<p><span class="strong strong"><strong>ECS Field Reference</strong></span></p>
<p>Please refer to the following <a href="/guide/en/ecs/current/ecs-field-reference.html" class="ulink" target="_top">document</a> for detailed information on ECS fields.</p>
<details>
<summary class="title"><span class="strong strong"><strong>Exported fields</strong></span></summary>
<div class="content">
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
<col class="col_4"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Field</th>
<th align="left" valign="top">Description</th>
<th align="left" valign="top">Type</th>
<th align="left" valign="top">Metric Type</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p>@timestamp</p></td>
<td align="left" valign="top"><p>Event timestamp.</p></td>
<td align="left" valign="top"><p>date</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>agent.id</p></td>
<td align="left" valign="top"><p>Unique identifier of this agent (if one exists). Example: For Beats this would be beat.id.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.application_name</p></td>
<td align="left" valign="top"><p>Name of application.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.bytes.read</p></td>
<td align="left" valign="top"><p>Total number of bytes read.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.bytes.written</p></td>
<td align="left" valign="top"><p>Total number of bytes written.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.disk_bytes_spilled</p></td>
<td align="left" valign="top"><p>Total number of disk bytes spilled.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.file_cache_hits</p></td>
<td align="left" valign="top"><p>Total number of file cache hits.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.files_discovered</p></td>
<td align="left" valign="top"><p>Total number of files discovered.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.filesystem.file.large_read_ops</p></td>
<td align="left" valign="top"><p>Total number of large read operations from the files.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.filesystem.file.read_bytes</p></td>
<td align="left" valign="top"><p>Total number of bytes read from the files.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.filesystem.file.read_ops</p></td>
<td align="left" valign="top"><p>Total number of read operations from the files.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.filesystem.file.write_bytes</p></td>
<td align="left" valign="top"><p>Total number of bytes written from the files.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.filesystem.file.write_ops</p></td>
<td align="left" valign="top"><p>Total number of write operations from the files.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.filesystem.hdfs.large_read_ops</p></td>
<td align="left" valign="top"><p>Total number of large read operations from HDFS.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.filesystem.hdfs.read_bytes</p></td>
<td align="left" valign="top"><p>Total number of read bytes from HDFS.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.filesystem.hdfs.read_ops</p></td>
<td align="left" valign="top"><p>Total number of read operations from HDFS.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.filesystem.hdfs.write_bytes</p></td>
<td align="left" valign="top"><p>Total number of write bytes from HDFS.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.filesystem.hdfs.write_ops</p></td>
<td align="left" valign="top"><p>Total number of write operations from HDFS.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.gc.major.count</p></td>
<td align="left" valign="top"><p>Total major GC count. For example, the garbage collector is one of MarkSweepCompact, PS MarkSweep, ConcurrentMarkSweep, G1 Old Generation and so on.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.gc.major.time</p></td>
<td align="left" valign="top"><p>Elapsed total major GC time. The value is expressed in milliseconds.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.gc.minor.count</p></td>
<td align="left" valign="top"><p>Total minor GC count. For example, the garbage collector is one of Copy, PS Scavenge, ParNew, G1 Young Generation and so on.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.gc.minor.time</p></td>
<td align="left" valign="top"><p>Elapsed total minor GC time. The value is expressed in milliseconds.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.heap_memory.off.execution</p></td>
<td align="left" valign="top"><p>Peak off heap execution memory in use, in bytes.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.heap_memory.off.storage</p></td>
<td align="left" valign="top"><p>Peak off heap storage memory in use, in bytes.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.heap_memory.off.unified</p></td>
<td align="left" valign="top"><p>Peak off heap memory (execution and storage).</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.heap_memory.on.execution</p></td>
<td align="left" valign="top"><p>Peak on heap execution memory in use, in bytes.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.heap_memory.on.storage</p></td>
<td align="left" valign="top"><p>Peak on heap storage memory in use, in bytes.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.heap_memory.on.unified</p></td>
<td align="left" valign="top"><p>Peak on heap memory (execution and storage).</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.hive_client_calls</p></td>
<td align="left" valign="top"><p>Total number of Hive Client calls.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.id</p></td>
<td align="left" valign="top"><p>ID of executor.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.jvm.cpu_time</p></td>
<td align="left" valign="top"><p>Elapsed CPU time the JVM spent.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.jvm.gc_time</p></td>
<td align="left" valign="top"><p>Elapsed time the JVM spent in garbage collection while executing this task.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.mbean</p></td>
<td align="left" valign="top"><p>The name of the jolokia mbean.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.memory.direct_pool</p></td>
<td align="left" valign="top"><p>Peak memory that the JVM is using for direct buffer pool.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.memory.jvm.heap</p></td>
<td align="left" valign="top"><p>Peak memory usage of the heap that is used for object allocation.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.memory.jvm.off_heap</p></td>
<td align="left" valign="top"><p>Peak memory usage of non-heap memory that is used by the Java virtual machine.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.memory.mapped_pool</p></td>
<td align="left" valign="top"><p>Peak memory that the JVM is using for mapped buffer pool</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.memory_bytes_spilled</p></td>
<td align="left" valign="top"><p>The number of in-memory bytes spilled by this task.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.parallel_listing_job_count</p></td>
<td align="left" valign="top"><p>Number of jobs running parallely.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.partitions_fetched</p></td>
<td align="left" valign="top"><p>Number of partitions fetched.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.process_tree.jvm.rss_memory</p></td>
<td align="left" valign="top"><p>Resident Set Size: number of pages the process has in real memory. This is just the pages which count toward text, data, or stack space. This does not include pages which have not been demand-loaded in, or which are swapped out.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.process_tree.jvm.v_memory</p></td>
<td align="left" valign="top"><p>Virtual memory size in bytes.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.process_tree.other.rss_memory</p></td>
<td align="left" valign="top"><p>Resident Set Size for other kind of process.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.process_tree.other.v_memory</p></td>
<td align="left" valign="top"><p>Virtual memory size for other kind of process in bytes.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.process_tree.python.rss_memory</p></td>
<td align="left" valign="top"><p>Resident Set Size for Python.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.process_tree.python.v_memory</p></td>
<td align="left" valign="top"><p>Virtual memory size for Python in bytes.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.records.read</p></td>
<td align="left" valign="top"><p>Total number of records read.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.records.written</p></td>
<td align="left" valign="top"><p>Total number of records written.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.result.serialization_time</p></td>
<td align="left" valign="top"><p>Elapsed time spent serializing the task result. The value is expressed in milliseconds.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.result.size</p></td>
<td align="left" valign="top"><p>The number of bytes this task transmitted back to the driver as the TaskResult.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.run_time</p></td>
<td align="left" valign="top"><p>Elapsed time in the running this task</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.shuffle.bytes_written</p></td>
<td align="left" valign="top"><p>Number of bytes written in shuffle operations.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.shuffle.client.used.direct_memory</p></td>
<td align="left" valign="top"><p>Amount of direct memory used by the shuffle client.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.shuffle.client.used.heap_memory</p></td>
<td align="left" valign="top"><p>Amount of heap memory used by the shuffle client.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.shuffle.fetch_wait_time</p></td>
<td align="left" valign="top"><p>Time the task spent waiting for remote shuffle blocks.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.shuffle.local.blocks_fetched</p></td>
<td align="left" valign="top"><p>Number of local (as opposed to read from a remote executor) blocks fetched in shuffle operations.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.shuffle.local.bytes_read</p></td>
<td align="left" valign="top"><p>Number of bytes read in shuffle operations from local disk (as opposed to read from a remote executor).</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.shuffle.records.read</p></td>
<td align="left" valign="top"><p>Number of records read in shuffle operations.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.shuffle.records.written</p></td>
<td align="left" valign="top"><p>Number of records written in shuffle operations.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.shuffle.remote.blocks_fetched</p></td>
<td align="left" valign="top"><p>Number of remote blocks fetched in shuffle operations.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.shuffle.remote.bytes_read</p></td>
<td align="left" valign="top"><p>Number of remote bytes read in shuffle operations.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.shuffle.remote.bytes_read_to_disk</p></td>
<td align="left" valign="top"><p>Number of remote bytes read to disk in shuffle operations. Large blocks are fetched to disk in shuffle read operations, as opposed to being read into memory, which is the default behavior.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.shuffle.server.used.direct_memory</p></td>
<td align="left" valign="top"><p>Amount of direct memory used by the shuffle server.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.shuffle.server.used.heap_memory</p></td>
<td align="left" valign="top"><p>Amount of heap memory used by the shuffle server.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.shuffle.total.bytes_read</p></td>
<td align="left" valign="top"><p>Number of bytes read in shuffle operations (both local and remote)</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.shuffle.write.time</p></td>
<td align="left" valign="top"><p>Time spent blocking on writes to disk or buffer cache. The value is expressed in nanoseconds.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.succeeded_tasks</p></td>
<td align="left" valign="top"><p>The number of tasks succeeded.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.threadpool.active_tasks</p></td>
<td align="left" valign="top"><p>Number of tasks currently executing.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.threadpool.complete_tasks</p></td>
<td align="left" valign="top"><p>Number of tasks that have completed in this executor.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.threadpool.current_pool_size</p></td>
<td align="left" valign="top"><p>The size of the current thread pool of the executor.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.threadpool.max_pool_size</p></td>
<td align="left" valign="top"><p>The maximum size of the thread pool of the executor.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.executor.threadpool.started_tasks</p></td>
<td align="left" valign="top"><p>The number of tasks started in the thread pool of the executor.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>counter</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>cloud.account.id</p></td>
<td align="left" valign="top"><p>The cloud account or organization id used to identify different entities in a multi-tenant environment. Examples: AWS account id, Google Cloud ORG Id, or other unique identifier.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>cloud.availability_zone</p></td>
<td align="left" valign="top"><p>Availability zone in which this host, resource, or service is located.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>cloud.instance.id</p></td>
<td align="left" valign="top"><p>Instance ID of the host machine.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>cloud.provider</p></td>
<td align="left" valign="top"><p>Name of the cloud provider. Example values are aws, azure, gcp, or digitalocean.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>cloud.region</p></td>
<td align="left" valign="top"><p>Region in which this host, resource, or service is located.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>container.id</p></td>
<td align="left" valign="top"><p>Unique container id.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>data_stream.dataset</p></td>
<td align="left" valign="top"><p>Data stream dataset.</p></td>
<td align="left" valign="top"><p>constant_keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>data_stream.namespace</p></td>
<td align="left" valign="top"><p>Data stream namespace.</p></td>
<td align="left" valign="top"><p>constant_keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>data_stream.type</p></td>
<td align="left" valign="top"><p>Data stream type.</p></td>
<td align="left" valign="top"><p>constant_keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>host.name</p></td>
<td align="left" valign="top"><p>Name of the host. It can contain what hostname returns on Unix systems, the fully qualified domain name (FQDN), or a name specified by the user. The recommended value is the lowercase FQDN of the host.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>service.address</p></td>
<td align="left" valign="top"><p>Address where data about this service was collected from. This should be a URI, network address (ipv4:port or [ipv6]:port) or a resource path (sockets).</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
</tbody>
</table>
</div>
</div>
</details>
<div class="position-relative"><h5><a id="apache_spark-node"></a>Node</h5><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/integration-docs/edit/main/dist/apache_spark/index.asciidoc">edit</a></div>
<p>The <code class="literal">node</code> data stream collects metrics related to the application count, waiting applications, worker metrics, executor count, core usage, and memory usage.</p>
<details>
<summary class="title"><span class="strong strong"><strong>Example</strong></span></summary>
<div class="content">
<p>An example event for <code class="literal">node</code> looks as following:</p>
<div class="pre_wrapper lang-json">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-json">{
    "@timestamp": "2022-04-12T04:42:49.581Z",
    "agent": {
        "ephemeral_id": "ae57925e-eeca-4bf4-ae20-38f82db1378b",
        "id": "f051059f-86be-46d5-896d-ff1b2cdab179",
        "name": "docker-fleet-agent",
        "type": "metricbeat",
        "version": "8.1.0"
    },
    "apache_spark": {
        "node": {
            "main": {
                "applications": {
                    "count": 0,
                    "waiting": 0
                },
                "workers": {
                    "alive": 0,
                    "count": 0
                }
            }
        }
    },
    "data_stream": {
        "dataset": "apache_spark.node",
        "namespace": "ep",
        "type": "metrics"
    },
    "ecs": {
        "version": "8.11.0"
    },
    "elastic_agent": {
        "id": "f051059f-86be-46d5-896d-ff1b2cdab179",
        "snapshot": false,
        "version": "8.1.0"
    },
    "event": {
        "agent_id_status": "verified",
        "dataset": "apache_spark.node",
        "duration": 8321835,
        "ingested": "2022-04-12T04:42:53Z",
        "kind": "metric",
        "module": "apache_spark",
        "type": [
            "info"
        ]
    },
    "host": {
        "architecture": "x86_64",
        "containerized": true,
        "hostname": "docker-fleet-agent",
        "ip": [
            "192.168.32.5"
        ],
        "mac": [
            "02-42-AC-14-00-07"
        ],
        "name": "docker-fleet-agent",
        "os": {
            "codename": "focal",
            "family": "debian",
            "kernel": "5.4.0-107-generic",
            "name": "Ubuntu",
            "platform": "ubuntu",
            "type": "linux",
            "version": "20.04.3 LTS (Focal Fossa)"
        }
    },
    "metricset": {
        "name": "jmx",
        "period": 60000
    },
    "service": {
        "address": "http://apache-spark-main:7777/jolokia/%3FignoreErrors=true&amp;canonicalNaming=false",
        "type": "jolokia"
    }
}</pre>
</div>
</div>
</details>
<p><span class="strong strong"><strong>ECS Field Reference</strong></span></p>
<p>Please refer to the following <a href="/guide/en/ecs/current/ecs-field-reference.html" class="ulink" target="_top">document</a> for detailed information on ECS fields.</p>
<details>
<summary class="title"><span class="strong strong"><strong>Exported fields</strong></span></summary>
<div class="content">
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
<col class="col_4"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Field</th>
<th align="left" valign="top">Description</th>
<th align="left" valign="top">Type</th>
<th align="left" valign="top">Metric Type</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p>@timestamp</p></td>
<td align="left" valign="top"><p>Event timestamp.</p></td>
<td align="left" valign="top"><p>date</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>agent.id</p></td>
<td align="left" valign="top"><p>Unique identifier of this agent (if one exists). Example: For Beats this would be beat.id.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.node.main.applications.count</p></td>
<td align="left" valign="top"><p>Total number of apps.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.node.main.applications.waiting</p></td>
<td align="left" valign="top"><p>Number of apps waiting.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.node.main.workers.alive</p></td>
<td align="left" valign="top"><p>Number of alive workers.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.node.main.workers.count</p></td>
<td align="left" valign="top"><p>Total number of workers.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.node.worker.cores.free</p></td>
<td align="left" valign="top"><p>Number of cores free.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.node.worker.cores.used</p></td>
<td align="left" valign="top"><p>Number of cores used.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.node.worker.executors</p></td>
<td align="left" valign="top"><p>Number of executors.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.node.worker.memory.free</p></td>
<td align="left" valign="top"><p>Number of cores free.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>apache_spark.node.worker.memory.used</p></td>
<td align="left" valign="top"><p>Amount of memory utilized in MB.</p></td>
<td align="left" valign="top"><p>long</p></td>
<td align="left" valign="top"><p>gauge</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>cloud.account.id</p></td>
<td align="left" valign="top"><p>The cloud account or organization id used to identify different entities in a multi-tenant environment. Examples: AWS account id, Google Cloud ORG Id, or other unique identifier.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>cloud.availability_zone</p></td>
<td align="left" valign="top"><p>Availability zone in which this host, resource, or service is located.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>cloud.instance.id</p></td>
<td align="left" valign="top"><p>Instance ID of the host machine.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>cloud.provider</p></td>
<td align="left" valign="top"><p>Name of the cloud provider. Example values are aws, azure, gcp, or digitalocean.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>cloud.region</p></td>
<td align="left" valign="top"><p>Region in which this host, resource, or service is located.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>container.id</p></td>
<td align="left" valign="top"><p>Unique container id.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>data_stream.dataset</p></td>
<td align="left" valign="top"><p>Data stream dataset.</p></td>
<td align="left" valign="top"><p>constant_keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>data_stream.namespace</p></td>
<td align="left" valign="top"><p>Data stream namespace.</p></td>
<td align="left" valign="top"><p>constant_keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>data_stream.type</p></td>
<td align="left" valign="top"><p>Data stream type.</p></td>
<td align="left" valign="top"><p>constant_keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>host.name</p></td>
<td align="left" valign="top"><p>Name of the host. It can contain what hostname returns on Unix systems, the fully qualified domain name (FQDN), or a name specified by the user. The recommended value is the lowercase FQDN of the host.</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p>service.address</p></td>
<td align="left" valign="top"><p>Address where data about this service was collected from. This should be a URI, network address (ipv4:port or [ipv6]:port) or a resource path (sockets).</p></td>
<td align="left" valign="top"><p>keyword</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
</tbody>
</table>
</div>
</div>
</details>
<div class="position-relative"><h4><a id="apache_spark-changelog"></a>Changelog</h4><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/integration-docs/edit/main/dist/apache_spark/index.asciidoc">edit</a></div>
<details>
<summary class="title"><span class="strong strong"><strong>Changelog</strong></span></summary>
<div class="content">
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Version</th>
<th align="left" valign="top">Details</th>
<th align="left" valign="top">Kibana version(s)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>1.3.1</strong></span></p></td>
<td align="left" valign="top">
<p><span class="eui-icon icon-bug"></span> <span class="strong strong"><strong>Bug fix</strong></span> (<a href="https://github.com/elastic/integrations/pull/12145" class="ulink" target="_top">View pull request</a>)<br>
Update links to getting started docs</p>
</td>
<td align="left" valign="top"><p>8.13.0 or higher</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>1.3.0</strong></span></p></td>
<td align="left" valign="top">
<p><span class="eui-icon icon-checkInCircleFilled"></span> <span class="strong strong"><strong>Enhancement</strong></span> (<a href="https://github.com/elastic/integrations/pull/10409" class="ulink" target="_top">View pull request</a>)<br>
Add processor support for application, driver, executor and node data streams.</p>
</td>
<td align="left" valign="top"><p>8.13.0 or higher</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>1.2.0</strong></span></p></td>
<td align="left" valign="top">
<p><span class="eui-icon icon-checkInCircleFilled"></span> <span class="strong strong"><strong>Enhancement</strong></span> (<a href="https://github.com/elastic/integrations/pull/10162" class="ulink" target="_top">View pull request</a>)<br>
ECS version updated to 8.11.0. Update the kibana constraint to ^8.13.0. Modified the field definitions to remove ECS fields made redundant by the ecs@mappings component template.</p>
</td>
<td align="left" valign="top"><p>8.13.0 or higher</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>1.1.0</strong></span></p></td>
<td align="left" valign="top">
<p><span class="eui-icon icon-checkInCircleFilled"></span> <span class="strong strong"><strong>Enhancement</strong></span> (<a href="https://github.com/elastic/integrations/pull/9768" class="ulink" target="_top">View pull request</a>)<br>
Add global filter on data_stream.dataset to improve performance.</p>
</td>
<td align="left" valign="top"><p>8.8.0 or higher</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>1.0.3</strong></span></p></td>
<td align="left" valign="top">
<p><span class="eui-icon icon-checkInCircleFilled"></span> <span class="strong strong"><strong>Enhancement</strong></span> (<a href="https://github.com/elastic/integrations/pull/9068" class="ulink" target="_top">View pull request</a>)<br>
Update README to follow documentation guidelines.</p>
</td>
<td align="left" valign="top"><p>8.8.0 or higher</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>1.0.2</strong></span></p></td>
<td align="left" valign="top">
<p><span class="eui-icon icon-checkInCircleFilled"></span> <span class="strong strong"><strong>Enhancement</strong></span> (<a href="https://github.com/elastic/integrations/pull/8423" class="ulink" target="_top">View pull request</a>)<br>
Inline "by reference" visualizations</p>
</td>
<td align="left" valign="top"><p>8.8.0 or higher</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>1.0.1</strong></span></p></td>
<td align="left" valign="top">
<p><span class="eui-icon icon-bug"></span> <span class="strong strong"><strong>Bug fix</strong></span> (<a href="https://github.com/elastic/integrations/pull/9021" class="ulink" target="_top">View pull request</a>)<br>
Update the link to the correct reindexing procedure.</p>
</td>
<td align="left" valign="top"><p>8.8.0 or higher</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>1.0.0</strong></span></p></td>
<td align="left" valign="top">
<p><span class="eui-icon icon-checkInCircleFilled"></span> <span class="strong strong"><strong>Enhancement</strong></span> (<a href="https://github.com/elastic/integrations/pull/7650" class="ulink" target="_top">View pull request</a>)<br>
Make Apache Spark GA.</p>
</td>
<td align="left" valign="top"><p>8.8.0 or higher</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>0.8.0</strong></span></p></td>
<td align="left" valign="top">
<p><span class="eui-icon icon-checkInCircleFilled"></span> <span class="strong strong"><strong>Enhancement</strong></span> (<a href="https://github.com/elastic/integrations/pull/8170" class="ulink" target="_top">View pull request</a>)<br>
Update the package format_version to 3.0.0.</p>
</td>
<td align="left" valign="top"><p>&mdash;</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>0.7.9</strong></span></p></td>
<td align="left" valign="top">
<p><span class="eui-icon icon-bug"></span> <span class="strong strong"><strong>Bug fix</strong></span> (<a href="https://github.com/elastic/integrations/pull/8117" class="ulink" target="_top">View pull request</a>)<br>
Add filters in visualizations.</p>
</td>
<td align="left" valign="top"><p>&mdash;</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>0.7.8</strong></span></p></td>
<td align="left" valign="top">
<p><span class="eui-icon icon-checkInCircleFilled"></span> <span class="strong strong"><strong>Enhancement</strong></span> (<a href="https://github.com/elastic/integrations/pull/8112" class="ulink" target="_top">View pull request</a>)<br>
Enable time series data streams for the metrics datasets. This dramatically reduces storage for metrics and is expected to progressively improve query performance. For more details, see <a href="/guide/en/elasticsearch/reference/current/tsds.html" class="ulink" target="_top">https://www.elastic.co/guide/en/elasticsearch/reference/current/tsds.html</a>.</p>
</td>
<td align="left" valign="top"><p>&mdash;</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>0.7.7</strong></span></p></td>
<td align="left" valign="top">
<p><span class="eui-icon icon-checkInCircleFilled"></span> <span class="strong strong"><strong>Enhancement</strong></span> (<a href="https://github.com/elastic/integrations/pull/7794" class="ulink" target="_top">View pull request</a>)<br>
Add metric_type for node data stream.</p>
</td>
<td align="left" valign="top"><p>&mdash;</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>0.7.6</strong></span></p></td>
<td align="left" valign="top">
<p><span class="eui-icon icon-checkInCircleFilled"></span> <span class="strong strong"><strong>Enhancement</strong></span> (<a href="https://github.com/elastic/integrations/pull/7996" class="ulink" target="_top">View pull request</a>)<br>
Added dimension mapping for Node datastream.</p>
</td>
<td align="left" valign="top"><p>&mdash;</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>0.7.5</strong></span></p></td>
<td align="left" valign="top">
<p><span class="eui-icon icon-checkInCircleFilled"></span> <span class="strong strong"><strong>Enhancement</strong></span> (<a href="https://github.com/elastic/integrations/pull/7831" class="ulink" target="_top">View pull request</a>)<br>
Add metric_type mappings for executor data stream.</p>
</td>
<td align="left" valign="top"><p>&mdash;</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>0.7.4</strong></span></p></td>
<td align="left" valign="top">
<p><span class="eui-icon icon-checkInCircleFilled"></span> <span class="strong strong"><strong>Enhancement</strong></span> (<a href="https://github.com/elastic/integrations/pull/7993" class="ulink" target="_top">View pull request</a>)<br>
Added dimension mapping for Executor datastream.</p>
</td>
<td align="left" valign="top"><p>&mdash;</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>0.7.3</strong></span></p></td>
<td align="left" valign="top">
<p><span class="eui-icon icon-checkInCircleFilled"></span> <span class="strong strong"><strong>Enhancement</strong></span> (<a href="https://github.com/elastic/integrations/pull/8110" class="ulink" target="_top">View pull request</a>)<br>
Add metric_type mapping for driver datastream.</p>
</td>
<td align="left" valign="top"><p>&mdash;</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>0.7.2</strong></span></p></td>
<td align="left" valign="top">
<p><span class="eui-icon icon-checkInCircleFilled"></span> <span class="strong strong"><strong>Enhancement</strong></span> (<a href="https://github.com/elastic/integrations/pull/8111" class="ulink" target="_top">View pull request</a>)<br>
Added dimension mapping for driver datastream.</p>
</td>
<td align="left" valign="top"><p>&mdash;</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>0.7.1</strong></span></p></td>
<td align="left" valign="top">
<p><span class="eui-icon icon-checkInCircleFilled"></span> <span class="strong strong"><strong>Enhancement</strong></span> (<a href="https://github.com/elastic/integrations/pull/7795" class="ulink" target="_top">View pull request</a>)<br>
Add metric type for application data stream.</p>
</td>
<td align="left" valign="top"><p>&mdash;</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>0.7.0</strong></span></p></td>
<td align="left" valign="top">
<p><span class="eui-icon icon-checkInCircleFilled"></span> <span class="strong strong"><strong>Enhancement</strong></span> (<a href="https://github.com/elastic/integrations/pull/7991" class="ulink" target="_top">View pull request</a>)<br>
Added dimension mapping for Application datastream.</p>
</td>
<td align="left" valign="top"><p>&mdash;</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>0.6.4</strong></span></p></td>
<td align="left" valign="top">
<p><span class="eui-icon icon-bug"></span> <span class="strong strong"><strong>Bug fix</strong></span> (<a href="https://github.com/elastic/integrations/pull/8061" class="ulink" target="_top">View pull request</a>)<br>
Fix the metric type of input_rate field for driver datastream.</p>
</td>
<td align="left" valign="top"><p>&mdash;</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>0.6.3</strong></span></p></td>
<td align="left" valign="top">
<p><span class="eui-icon icon-checkInCircleFilled"></span> <span class="strong strong"><strong>Enhancement</strong></span> (<a href="https://github.com/elastic/integrations/pull/7830" class="ulink" target="_top">View pull request</a>)<br>
Update Apache Spark logo.</p>
</td>
<td align="left" valign="top"><p>&mdash;</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>0.6.2</strong></span></p></td>
<td align="left" valign="top">
<p><span class="eui-icon icon-bug"></span> <span class="strong strong"><strong>Bug fix</strong></span> (<a href="https://github.com/elastic/integrations/pull/7468" class="ulink" target="_top">View pull request</a>)<br>
Resolve the conflicts in host.ip field</p>
</td>
<td align="left" valign="top"><p>&mdash;</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>0.6.1</strong></span></p></td>
<td align="left" valign="top">
<p><span class="eui-icon icon-bug"></span> <span class="strong strong"><strong>Bug fix</strong></span> (<a href="https://github.com/elastic/integrations/pull/7467" class="ulink" target="_top">View pull request</a>)<br>
Remove incorrect filter from the visualizations</p>
</td>
<td align="left" valign="top"><p>&mdash;</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>0.6.0</strong></span></p></td>
<td align="left" valign="top">
<p><span class="eui-icon icon-checkInCircleFilled"></span> <span class="strong strong"><strong>Enhancement</strong></span> (<a href="https://github.com/elastic/integrations/pull/6298" class="ulink" target="_top">View pull request</a>)<br>
Rename ownership from obs-service-integrations to obs-infraobs-integrations</p>
</td>
<td align="left" valign="top"><p>&mdash;</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>0.5.0</strong></span></p></td>
<td align="left" valign="top">
<p><span class="eui-icon icon-checkInCircleFilled"></span> <span class="strong strong"><strong>Enhancement</strong></span> (<a href="https://github.com/elastic/integrations/pull/5492" class="ulink" target="_top">View pull request</a>)<br>
Migrate visualizations to lens.</p>
</td>
<td align="left" valign="top"><p>&mdash;</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>0.4.1</strong></span></p></td>
<td align="left" valign="top">
<p><span class="eui-icon icon-checkInCircleFilled"></span> <span class="strong strong"><strong>Enhancement</strong></span> (<a href="https://github.com/elastic/integrations/pull/5123" class="ulink" target="_top">View pull request</a>)<br>
Added categories and/or subcategories.</p>
</td>
<td align="left" valign="top"><p>&mdash;</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>0.4.0</strong></span></p></td>
<td align="left" valign="top">
<p><span class="eui-icon icon-checkInCircleFilled"></span> <span class="strong strong"><strong>Enhancement</strong></span> (<a href="https://github.com/elastic/integrations/pull/5037" class="ulink" target="_top">View pull request</a>)<br>
Update ECS version to 8.5.1</p>
</td>
<td align="left" valign="top"><p>&mdash;</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>0.3.0</strong></span></p></td>
<td align="left" valign="top">
<p><span class="eui-icon icon-checkInCircleFilled"></span> <span class="strong strong"><strong>Enhancement</strong></span> (<a href="https://github.com/elastic/integrations/pull/4585" class="ulink" target="_top">View pull request</a>)<br>
Update readme</p>
</td>
<td align="left" valign="top"><p>&mdash;</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>0.2.1</strong></span></p></td>
<td align="left" valign="top">
<p><span class="eui-icon icon-bug"></span> <span class="strong strong"><strong>Bug fix</strong></span> (<a href="https://github.com/elastic/integrations/pull/3853" class="ulink" target="_top">View pull request</a>)<br>
Remove unnecessary fields from fields.yml</p>
</td>
<td align="left" valign="top"><p>&mdash;</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>0.2.0</strong></span></p></td>
<td align="left" valign="top">
<p><span class="eui-icon icon-checkInCircleFilled"></span> <span class="strong strong"><strong>Enhancement</strong></span> (<a href="https://github.com/elastic/integrations/pull/3020" class="ulink" target="_top">View pull request</a>)<br>
Add dashboards and visualizations</p>
</td>
<td align="left" valign="top"><p>&mdash;</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>0.1.1</strong></span></p></td>
<td align="left" valign="top">
<p><span class="eui-icon icon-checkInCircleFilled"></span> <span class="strong strong"><strong>Enhancement</strong></span> (<a href="https://github.com/elastic/integrations/pull/3070" class="ulink" target="_top">View pull request</a>)<br>
Refactor the "nodes" data stream to adjust its name to "node" (singular)</p>
</td>
<td align="left" valign="top"><p>&mdash;</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>0.1.0</strong></span></p></td>
<td align="left" valign="top">
<p><span class="eui-icon icon-checkInCircleFilled"></span> <span class="strong strong"><strong>Enhancement</strong></span> (<a href="https://github.com/elastic/integrations/pull/2943" class="ulink" target="_top">View pull request</a>)<br>
Implement "executor" data stream</p>
<p><span class="eui-icon icon-checkInCircleFilled"></span> <span class="strong strong"><strong>Enhancement</strong></span> (<a href="https://github.com/elastic/integrations/pull/2945" class="ulink" target="_top">View pull request</a>)<br>
Implement "driver" data stream</p>
<p><span class="eui-icon icon-checkInCircleFilled"></span> <span class="strong strong"><strong>Enhancement</strong></span> (<a href="https://github.com/elastic/integrations/pull/2941" class="ulink" target="_top">View pull request</a>)<br>
Implement "application" data stream</p>
<p><span class="eui-icon icon-checkInCircleFilled"></span> <span class="strong strong"><strong>Enhancement</strong></span> (<a href="https://github.com/elastic/integrations/pull/2939" class="ulink" target="_top">View pull request</a>)<br>
Implement "nodes" data stream</p>
</td>
<td align="left" valign="top"><p>&mdash;</p></td>
</tr>
</tbody>
</table>
</div>
</div>
</details>
</div>
</div>
</div><div class="navfooter">
<span class="prev">
<a href="apache.html">« Apache Integration</a>
</span>
<span class="next">
<a href="apache_tomcat.html">Apache Tomcat Integration »</a>
</span>
</div>
</body>
</html>
