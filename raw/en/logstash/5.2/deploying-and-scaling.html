<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Deploying and Scaling Logstash | Logstash Reference [5.2] | Elastic</title>
<link rel="home" href="index.html" title="Logstash Reference [5.2]"/>
<link rel="up" href="index.html" title="Logstash Reference [5.2]"/>
<link rel="prev" href="glob-support.html" title="Glob Pattern Support"/>
<link rel="next" href="performance-tuning.html" title="Performance Tuning"/>
<meta name="DC.type" content="Learn/Docs/Logstash/Reference/5.2"/>
<meta name="DC.subject" content="Logstash"/>
<meta name="DC.identifier" content="5.2"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body><div class="page_header">
<strong>IMPORTANT</strong>: No additional bug fixes or documentation updates
will be released for this version. For the latest information, see the
<a href="../current/index.html">current release documentation</a>.
</div>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="index.html">Logstash Reference [5.2]</a></span>
»
<span class="breadcrumb-node">Deploying and Scaling Logstash</span>
</div>
<div class="navheader">
<span class="prev">
<a href="glob-support.html">« Glob Pattern Support</a>
</span>
<span class="next">
<a href="performance-tuning.html">Performance Tuning »</a>
</span>
</div>
<div class="chapter">
<div class="titlepage"><div><div>
<h1 class="title"><a id="deploying-and-scaling"></a>Deploying and Scaling Logstash<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/5.2/docs/static/deploying.asciidoc">edit</a></h1>
</div></div></div>
<p>As your use case for Logstash evolves, the preferred architecture at a given scale will change. This section discusses
a range of Logstash architectures in increasing order of complexity, starting from a minimal installation and adding
elements to the system. The example deployments in this section write to an Elasticsearch cluster, but Logstash can
write to a large variety of <a href="/guide/en/logstash/5.2/output-plugins.html" class="ulink" target="_top">endpoints</a>.</p>
<h3><a id="deploying-minimal-install"></a>The Minimal Installation<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/5.2/docs/static/deploying.asciidoc">edit</a></h3>
<p>The minimal Logstash installation has one Logstash instance and one Elasticsearch instance. These instances are
directly connected. Logstash uses an <a href="/guide/en/logstash/5.2/input-plugins.html" class="ulink" target="_top"><em>input plugin</em></a> to ingest data and an
Elasticsearch <a href="/guide/en/logstash/5.2/output-plugins.html" class="ulink" target="_top"><em>output plugin</em></a> to index the data in Elasticsearch, following the Logstash
<a href="/guide/en/logstash/5.2/pipeline.html" class="ulink" target="_top"><em>processing pipeline</em></a>. A Logstash instance has a fixed pipeline constructed at startup,
based on the instance’s configuration file. You must specify an input plugin. Output defaults to <code class="literal">stdout</code>, and the
filtering section of the pipeline, which is discussed in the next section, is optional.</p>
<div class="imageblock">
<div class="content">
<img src="static/images/deploy_1.png" alt="deploy 1">
</div>
</div>
<h3><a id="deploying-filter-threads"></a>Using Filters<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/5.2/docs/static/deploying.asciidoc">edit</a></h3>
<p>Log data is typically unstructured, often contains extraneous information that isn’t relevant to your use case, and
sometimes is missing relevant information that can be derived from the log contents. You can use a
<a href="/guide/en/logstash/5.2/filter-plugins.html" class="ulink" target="_top">filter plugin</a> to parse the log into fields, remove unnecessary information, and derive
additional information from the existing fields. For example, filters can derive geolocation information from an IP
address and add that information to the logs, or parse and structure arbitrary text with the
<a href="/guide/en/logstash/5.2/plugins-filters-grok.html" class="ulink" target="_top">grok</a> filter.</p>
<p>Adding a filter plugin can significantly affect performance, depending on the amount of computation the filter plugin
performs, as well as on the volume of the logs being processed. The <code class="literal">grok</code> filter’s regular expression computation is
particularly resource-intensive. One way to address this increased demand for computing resources is to use
parallel processing on multicore machines. Use the <code class="literal">-w</code> switch to set the number of execution threads for Logstash
filtering tasks. For example the <code class="literal">bin/logstash -w 8</code> command uses eight different threads for filter processing.</p>
<div class="imageblock">
<div class="content">
<img src="static/images/deploy_2.png" alt="deploy 2">
</div>
</div>
<h3><a id="deploying-filebeat"></a>Using Filebeat<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/5.2/docs/static/deploying.asciidoc">edit</a></h3>
<p><a href="/guide/en/beats/filebeat/current/index.html" class="ulink" target="_top">Filebeat</a> is a lightweight, resource-friendly tool
written in Go that collects logs from files on the server and forwards these logs to other machines for processing.
Filebeat uses the <a href="/guide/en/beats/libbeat/current/index.html" class="ulink" target="_top">Beats</a> protocol to communicate with a
centralized Logstash instance. Configure the Logstash instances that receive Beats data to use the
<a href="/guide/en/logstash/5.2/plugins-inputs-beats.html" class="ulink" target="_top">Beats input plugin</a>.</p>
<p>Filebeat uses the computing resources of the machine hosting the source data, and the Beats input plugin minimizes the
resource demands on the Logstash instance, making this architecture attractive for use cases with resource constraints.</p>
<div class="imageblock">
<div class="content">
<img src="static/images/deploy_3.png" alt="deploy 3">
</div>
</div>
<h3><a id="deploying-larger-cluster"></a>Scaling to a Larger Elasticsearch Cluster<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/5.2/docs/static/deploying.asciidoc">edit</a></h3>
<p>Typically, Logstash does not communicate with a single Elasticsearch node, but with a cluster that comprises several
nodes. By default, Logstash uses the HTTP protocol to move data into the cluster.</p>
<p>You can use the Elasticsearch HTTP REST APIs to index data into the Elasticsearch cluster. These APIs represent the
indexed data in JSON. Using the REST APIs does not require the Java client classes or any additional JAR
files and has no performance disadvantages compared to the transport or node protocols. You can secure communications
that use the HTTP REST APIs by using <a href="/guide/en/x-pack/5.2/xpack-security.html" class="ulink" target="_top">X-Pack Security</a>, which supports SSL and HTTP basic authentication.</p>
<p>When you use the HTTP protocol, you can configure the Logstash Elasticsearch output plugin to automatically
load-balance indexing requests across a
specified set of hosts in the Elasticsearch cluster. Specifying multiple Elasticsearch nodes also provides high availability for the Elasticsearch cluster by routing traffic to active Elasticsearch nodes.</p>
<p>You can also use the Elasticsearch Java APIs to serialize the data into a binary representation, using
the transport protocol. The transport protocol can sniff the endpoint of the request and select an
arbitrary client or data node in the Elasticsearch cluster.</p>
<p>Using the HTTP or transport protocols keep your Logstash instances separate from the Elasticsearch cluster. The node
protocol, by contrast, has the machine running the Logstash instance join the Elasticsearch cluster, running an
Elasticsearch instance. The data that needs indexing propagates from this node to the rest of the cluster. Since the
machine is part of the cluster, the cluster topology is available, making the node protocol a good fit for use cases
that use a relatively small number of persistent connections.</p>
<p>You can also use a third-party hardware or software load balancer to handle connections between Logstash and
external applications.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>Make sure that your Logstash configuration does not connect directly to Elasticsearch dedicated
<a href="/guide/en/elasticsearch/reference/5.2/modules-node.html" class="ulink" target="_top">master nodes</a>, which perform dedicated cluster management. Connect Logstash to client or data
nodes to protect the stability of your Elasticsearch cluster.</p>
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="static/images/deploy_4.png" alt="deploy 4">
</div>
</div>
<h3><a id="deploying-message-queueing"></a>Managing Throughput Spikes with Message Queueing<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/5.2/docs/static/deploying.asciidoc">edit</a></h3>
<p>When the data coming into a Logstash pipeline exceeds the Elasticsearch cluster&#8217;s ability to ingest the data, you can
use a message broker as a buffer. By default, Logstash throttles incoming events when
indexer consumption rates fall below incoming data rates. Since this throttling can lead to events being buffered at
the data source, preventing back pressure with message brokers becomes an important part of managing your deployment.</p>
<p>Adding a message broker to your Logstash deployment also provides a level of protection from data loss. When a Logstash
instance that has consumed data from the message broker fails, the data can be replayed from the message broker to an
active Logstash instance.</p>
<p>Several third-party message brokers exist, such as Redis, Kafka, or RabbitMQ. Logstash provides input and output plugins
to integrate with several of these third-party message brokers. When your Logstash deployment has a message broker
configured, Logstash functionally exists in two phases: shipping instances, which handles data ingestion and storage in
the message broker, and indexing instances, which retrieve the data from the message broker, apply any configured
filtering, and write the filtered data to an Elasticsearch index.</p>
<div class="imageblock">
<div class="content">
<img src="static/images/deploy_5.png" alt="deploy 5">
</div>
</div>
<h3><a id="deploying-logstash-ha"></a>Multiple Connections for Logstash High Availability<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/5.2/docs/static/deploying.asciidoc">edit</a></h3>
<p>To make your Logstash deployment more resilient to individual instance failures, you can set up a load balancer between
your data source machines and the Logstash cluster. The load balancer handles the individual connections to the
Logstash instances to ensure continuity of data ingestion and processing even when an individual instance is unavailable.</p>
<div class="imageblock">
<div class="content">
<img src="static/images/deploy_6.png" alt="deploy 6">
</div>
</div>
<p>The architecture in the previous diagram is unable to process input from a specific type, such as an RSS feed or a
file, if the Logstash instance dedicated to that input type becomes unavailable. For more robust input processing,
configure each Logstash instance for multiple inputs, as in the following diagram:</p>
<div class="imageblock">
<div class="content">
<img src="static/images/deploy_7.png" alt="deploy 7">
</div>
</div>
<p>This architecture parallelizes the Logstash workload based on the inputs you configure. With more inputs, you can add
more Logstash instances to scale horizontally. Separate parallel pipelines also increases the reliability of your stack
by eliminating single points of failure.</p>
<h3><a id="deploying-scaling"></a>Scaling Logstash<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/5.2/docs/static/deploying.asciidoc">edit</a></h3>
<p>A mature Logstash deployment typically has the following pipeline:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
The <em>input</em> tier consumes data from the source, and consists of Logstash instances with the proper input plugins.
</li>
<li class="listitem">
The <em>message broker</em> serves as a buffer to hold ingested data and serve as failover protection.
</li>
<li class="listitem">
The <em>filter</em> tier applies parsing and other processing to the data consumed from the message broker.
</li>
<li class="listitem">
The <em>indexing</em> tier moves the processed data into Elasticsearch.
</li>
</ul>
</div>
<p>Any of these layers can be scaled by adding computing resources. Examine the performance of these components regularly
as your use case evolves and add resources as needed. When Logstash routinely throttles incoming events, consider
adding storage for your message broker. Alternately, increase the Elasticsearch cluster&#8217;s rate of data consumption by
adding more Logstash indexing instances.</p>
</div>
<div class="navfooter">
<span class="prev">
<a href="glob-support.html">« Glob Pattern Support</a>
</span>
<span class="next">
<a href="performance-tuning.html">Performance Tuning »</a>
</span>
</div>
</div>
</body>
</html>
