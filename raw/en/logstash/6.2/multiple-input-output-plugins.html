<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Stitching Together Multiple Input and Output Plugins | Logstash Reference [6.2] | Elastic</title>
<link rel="home" href="index.html" title="Logstash Reference [6.2]"/>
<link rel="up" href="getting-started-with-logstash.html" title="Getting Started with Logstash"/>
<link rel="prev" href="advanced-pipeline.html" title="Parsing Logs with Logstash"/>
<link rel="next" href="pipeline.html" title="How Logstash Works"/>
<meta name="DC.type" content="Learn/Docs/Logstash/Reference/6.2"/>
<meta name="DC.subject" content="Logstash"/>
<meta name="DC.identifier" content="6.2"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body><div class="page_header">
<strong>IMPORTANT</strong>: No additional bug fixes or documentation updates
will be released for this version. For the latest information, see the
<a href="../current/index.html">current release documentation</a>.
</div>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="index.html">Logstash Reference [6.2]</a></span>
»
<span class="breadcrumb-link"><a href="getting-started-with-logstash.html">Getting Started with Logstash</a></span>
»
<span class="breadcrumb-node">Stitching Together Multiple Input and Output Plugins</span>
</div>
<div class="navheader">
<span class="prev">
<a href="advanced-pipeline.html">« Parsing Logs with Logstash</a>
</span>
<span class="next">
<a href="pipeline.html">How Logstash Works »</a>
</span>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="multiple-input-output-plugins"></a>Stitching Together Multiple Input and Output Plugins<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/6.2/docs/static/advanced-pipeline.asciidoc">edit</a></h2>
</div></div></div>
<p>The information you need to manage often comes from several disparate sources, and use cases can require multiple
destinations for your data. Your Logstash pipeline can use multiple input and output plugins to handle these
requirements.</p>
<p>In this section, you create a Logstash pipeline that takes input from a Twitter feed and the Filebeat client, then
sends the information to an Elasticsearch cluster as well as writing the information directly to a file.</p>
<h4><a id="twitter-configuration"></a>Reading from a Twitter Feed<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/6.2/docs/static/advanced-pipeline.asciidoc">edit</a></h4>
<p>To add a Twitter feed, you use the <a href="/guide/en/logstash/6.2/plugins-inputs-twitter.html" class="ulink" target="_top"><code class="literal">twitter</code></a> input plugin. To
configure the plugin, you need several pieces of information:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
A <em>consumer key</em>, which uniquely identifies your Twitter app.
</li>
<li class="listitem">
A <em>consumer secret</em>, which serves as the password for your Twitter app.
</li>
<li class="listitem">
One or more <em>keywords</em> to search in the incoming feed. The example shows using "cloud" as a keyword, but you can use whatever you want.
</li>
<li class="listitem">
An <em>oauth token</em>, which identifies the Twitter account using this app.
</li>
<li class="listitem">
An <em>oauth token secret</em>, which serves as the password of the Twitter account.
</li>
</ul>
</div>
<p>Visit <a href="https://dev.twitter.com/apps" class="ulink" target="_top">https://dev.twitter.com/apps</a> to set up a Twitter account and generate your consumer
key and secret, as well as your access token and secret. See the docs for the <a href="/guide/en/logstash/6.2/plugins-inputs-twitter.html" class="ulink" target="_top"><code class="literal">twitter</code></a> input plugin if you&#8217;re not sure how to generate these keys.</p>
<p>Like you did earlier when you worked on <a class="xref" href="advanced-pipeline.html" title="Parsing Logs with Logstash">Parsing Logs with Logstash</a>, create a config file (called <code class="literal">second-pipeline.conf</code>) that
contains the skeleton of a configuration pipeline. If you want, you can reuse the file you created earlier, but make
sure you pass in the correct config file name when you run Logstash.</p>
<p>Add the following lines to the <code class="literal">input</code> section of the <code class="literal">second-pipeline.conf</code> file, substituting your values for the
placeholder values shown here:</p>
<div class="pre_wrapper lang-json">
<pre class="programlisting prettyprint lang-json">    twitter {
        consumer_key =&gt; "enter_your_consumer_key_here"
        consumer_secret =&gt; "enter_your_secret_here"
        keywords =&gt; ["cloud"]
        oauth_token =&gt; "enter_your_access_token_here"
        oauth_token_secret =&gt; "enter_your_access_token_secret_here"
    }</pre>
</div>
<h4><a id="configuring-lsf"></a>Configuring Filebeat to Send Log Lines to Logstash<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/6.2/docs/static/advanced-pipeline.asciidoc">edit</a></h4>
<p>As you learned earlier in <a class="xref" href="advanced-pipeline.html#configuring-filebeat" title="Configuring Filebeat to Send Log Lines to Logstash">Configuring Filebeat to Send Log Lines to Logstash</a>, the <a href="https://github.com/elastic/beats/tree/master/filebeat" class="ulink" target="_top">Filebeat</a>
client is a lightweight, resource-friendly tool that collects logs from files on the server and forwards these logs to your
Logstash instance for processing.</p>
<p>After installing Filebeat, you need to configure it. Open the <code class="literal">filebeat.yml</code> file located in your Filebeat installation
directory, and replace the contents with the following lines. Make sure <code class="literal">paths</code> points to your syslog:</p>
<div class="pre_wrapper lang-shell">
<pre class="programlisting prettyprint lang-shell">filebeat.prospectors:
- type: log
  paths:
    - /var/log/*.log <a id="CO2-1"></a><i class="conum" data-value="1"></i>
  fields:
    type: syslog <a id="CO2-2"></a><i class="conum" data-value="2"></i>
output.logstash:
  hosts: ["localhost:5044"]</pre>
</div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO2-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>Absolute path to the file or files that Filebeat processes.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO2-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p>Adds a field called <code class="literal">type</code> with the value <code class="literal">syslog</code> to the event.</p>
</td>
</tr>
</table>
</div>
<p>Save your changes.</p>
<p>To keep the configuration simple, you won&#8217;t specify TLS/SSL settings as you would in a real world
scenario.</p>
<p>Configure your Logstash instance to use the Filebeat input plugin by adding the following lines to the <code class="literal">input</code> section
of the <code class="literal">second-pipeline.conf</code> file:</p>
<div class="pre_wrapper lang-json">
<pre class="programlisting prettyprint lang-json">    beats {
        port =&gt; "5044"
    }</pre>
</div>
<h4><a id="logstash-file-output"></a>Writing Logstash Data to a File<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/6.2/docs/static/advanced-pipeline.asciidoc">edit</a></h4>
<p>You can configure your Logstash pipeline to write data directly to a file with the
<a href="/guide/en/logstash/6.2/plugins-outputs-file.html" class="ulink" target="_top"><code class="literal">file</code></a> output plugin.</p>
<p>Configure your Logstash instance to use the <code class="literal">file</code> output plugin by adding the following lines to the <code class="literal">output</code> section
of the <code class="literal">second-pipeline.conf</code> file:</p>
<div class="pre_wrapper lang-json">
<pre class="programlisting prettyprint lang-json">    file {
        path =&gt; "/path/to/target/file"
    }</pre>
</div>
<h4><a id="multiple-es-nodes"></a>Writing to Multiple Elasticsearch Nodes<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/6.2/docs/static/advanced-pipeline.asciidoc">edit</a></h4>
<p>Writing to multiple Elasticsearch nodes lightens the resource demands on a given Elasticsearch node, as well as
providing redundant points of entry into the cluster when a particular node is unavailable.</p>
<p>To configure your Logstash instance to write to multiple Elasticsearch nodes, edit the <code class="literal">output</code> section of the <code class="literal">second-pipeline.conf</code> file to read:</p>
<div class="pre_wrapper lang-json">
<pre class="programlisting prettyprint lang-json">output {
    elasticsearch {
        hosts =&gt; ["IP Address 1:port1", "IP Address 2:port2", "IP Address 3"]
    }
}</pre>
</div>
<p>Use the IP addresses of three non-master nodes in your Elasticsearch cluster in the host line. When the <code class="literal">hosts</code>
parameter lists multiple IP addresses, Logstash load-balances requests across the list of addresses. Also note that
the default port for Elasticsearch is <code class="literal">9200</code> and can be omitted in the configuration above.</p>
<h5><a id="testing-second-pipeline"></a>Testing the Pipeline<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/6.2/docs/static/advanced-pipeline.asciidoc">edit</a></h5>
<p>At this point, your <code class="literal">second-pipeline.conf</code> file looks like this:</p>
<div class="pre_wrapper lang-json">
<pre class="programlisting prettyprint lang-json">input {
    twitter {
        consumer_key =&gt; "enter_your_consumer_key_here"
        consumer_secret =&gt; "enter_your_secret_here"
        keywords =&gt; ["cloud"]
        oauth_token =&gt; "enter_your_access_token_here"
        oauth_token_secret =&gt; "enter_your_access_token_secret_here"
    }
    beats {
        port =&gt; "5044"
    }
}
output {
    elasticsearch {
        hosts =&gt; ["IP Address 1:port1", "IP Address 2:port2", "IP Address 3"]
    }
    file {
        path =&gt; "/path/to/target/file"
    }
}</pre>
</div>
<p>Logstash is consuming data from the Twitter feed you configured, receiving data from Filebeat, and
indexing this information to three nodes in an Elasticsearch cluster as well as writing to a file.</p>
<p>At the data source machine, run Filebeat with the following command:</p>
<div class="pre_wrapper lang-shell">
<pre class="programlisting prettyprint lang-shell">sudo ./filebeat -e -c filebeat.yml -d "publish"</pre>
</div>
<p>Filebeat will attempt to connect on port 5044. Until Logstash starts with an active Beats plugin, there
won’t be any answer on that port, so any messages you see regarding failure to connect on that port are normal for now.</p>
<p>To verify your configuration, run the following command:</p>
<div class="pre_wrapper lang-shell">
<pre class="programlisting prettyprint lang-shell">bin/logstash -f second-pipeline.conf --config.test_and_exit</pre>
</div>
<p>The <code class="literal">--config.test_and_exit</code> option parses your configuration file and reports any errors. When the configuration file
passes the configuration test, start Logstash with the following command:</p>
<div class="pre_wrapper lang-shell">
<pre class="programlisting prettyprint lang-shell">bin/logstash -f second-pipeline.conf</pre>
</div>
<p>Use the <code class="literal">grep</code> utility to search in the target file to verify that information is present:</p>
<div class="pre_wrapper lang-shell">
<pre class="programlisting prettyprint lang-shell">grep syslog /path/to/target/file</pre>
</div>
<p>Run an Elasticsearch query to find the same information in the Elasticsearch cluster:</p>
<div class="pre_wrapper lang-shell">
<pre class="programlisting prettyprint lang-shell">curl -XGET 'localhost:9200/logstash-$DATE/_search?pretty&amp;q=fields.type:syslog'</pre>
</div>
<p>Replace $DATE with the current date, in YYYY.MM.DD format.</p>
<p>To see data from the Twitter feed, try this query:</p>
<div class="pre_wrapper lang-shell">
<pre class="programlisting prettyprint lang-shell">curl -XGET 'http://localhost:9200/logstash-$DATE/_search?pretty&amp;q=client:iphone'</pre>
</div>
<p>Again, remember to replace $DATE with the current date, in YYYY.MM.DD format.</p>
</div>
<div class="navfooter">
<span class="prev">
<a href="advanced-pipeline.html">« Parsing Logs with Logstash</a>
</span>
<span class="next">
<a href="pipeline.html">How Logstash Works »</a>
</span>
</div>
</div>
</body>
</html>
