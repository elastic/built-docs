<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Deserializing Data | Logstash Reference [7.11] | Elastic</title>
<link rel="home" href="index.html" title="Logstash Reference [7.11]"/>
<link rel="up" href="transformation.html" title="Transforming Data"/>
<link rel="prev" href="core-operations.html" title="Performing Core Operations"/>
<link rel="next" href="field-extraction.html" title="Extracting Fields and Wrangling Data"/>
<meta name="DC.type" content="Learn/Docs/Logstash/Reference/7.11"/>
<meta name="DC.subject" content="Logstash"/>
<meta name="DC.identifier" content="7.11"/>
</head>
<body><div class="page_header">
You are looking at preliminary documentation for a future release.
Not what you want? See the
<a href="../current/index.html">current release documentation</a>.
</div>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="index.html">Logstash Reference [7.11]</a></span>
»
<span class="breadcrumb-link"><a href="transformation.html">Transforming Data</a></span>
»
<span class="breadcrumb-node">Deserializing Data</span>
</div>
<div class="navheader">
<span class="prev">
<a href="core-operations.html">« Performing Core Operations</a>
</span>
<span class="next">
<a href="field-extraction.html">Extracting Fields and Wrangling Data »</a>
</span>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="data-deserialization"></a>Deserializing Data<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/logstash/edit/7.11/docs/static/transforming-data.asciidoc">edit</a></h2>
</div></div></div>
<p>The plugins described in this section are useful for deserializing data into
Logstash events.</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<a class="xref" href="plugins-codecs-avro.html" title="Avro codec plugin">avro codec</a>
</span>
</dt>
<dd>
<p>
Reads serialized Avro records as Logstash events. This plugin deserializes
individual Avro records. It is not for reading Avro files. Avro files have a
unique format that must be handled upon input.
</p>
<p>The following config deserializes input from Kafka:</p>
<div class="pre_wrapper lang-json">
<pre class="programlisting prettyprint lang-json">input {
  kafka {
    codec =&gt; {
      avro =&gt; {
        schema_uri =&gt; "/tmp/schema.avsc"
      }
    }
  }
}
...</pre>
</div>
</dd>
<dt>
<span class="term">
<a class="xref" href="plugins-filters-csv.html" title="Csv filter plugin">csv filter</a>
</span>
</dt>
<dd>
<p>
Parses comma-separated value data into individual fields. By default, the
filter autogenerates field names (column1, column2, and so on), or you can specify
a list of names. You can also change the column separator.
</p>
<p>The following config parses CSV data into the field names specified in the
<code class="literal">columns</code> field:</p>
<div class="pre_wrapper lang-json">
<pre class="programlisting prettyprint lang-json">filter {
  csv {
    separator =&gt; ","
    columns =&gt; [ "Transaction Number", "Date", "Description", "Amount Debit", "Amount Credit", "Balance" ]
  }
}</pre>
</div>
</dd>
<dt>
<span class="term">
<a class="xref" href="plugins-codecs-fluent.html" title="Fluent codec plugin">fluent codec</a>
</span>
</dt>
<dd>
<p>
Reads the Fluentd <code class="literal">msgpack</code> schema.
</p>
<p>The following config decodes logs received from <code class="literal">fluent-logger-ruby</code>:</p>
<div class="pre_wrapper lang-json">
<pre class="programlisting prettyprint lang-json">input {
  tcp {
    codec =&gt; fluent
    port =&gt; 4000
  }
}</pre>
</div>
</dd>
<dt>
<span class="term">
<a class="xref" href="plugins-codecs-json.html" title="Json codec plugin">json codec</a>
</span>
</dt>
<dd>
<p>
Decodes (via inputs) and encodes (via outputs) JSON formatted content, creating
one event per element in a JSON array.
</p>
<p>The following config decodes the JSON formatted content in a file:</p>
<div class="pre_wrapper lang-json">
<pre class="programlisting prettyprint lang-json">input {
  file {
    path =&gt; "/path/to/myfile.json"
    codec =&gt;"json"
}</pre>
</div>
</dd>
<dt>
<span class="term">
<a class="xref" href="plugins-codecs-protobuf.html" title="Protobuf codec plugin">protobuf codec</a>
</span>
</dt>
<dd>
<p>
Reads protobuf encoded messages and converts them to Logstash events. Requires
the protobuf definitions to be compiled as Ruby files. You can compile them by
using the
<a href="https://github.com/codekitchen/ruby-protocol-buffers" class="ulink" target="_top">ruby-protoc compiler</a>.
</p>
<p>The following config decodes events from a Kafka stream:</p>
<div class="pre_wrapper lang-json">
<pre class="programlisting prettyprint lang-json">input
  kafka {
    zk_connect =&gt; "127.0.0.1"
    topic_id =&gt; "your_topic_goes_here"
    codec =&gt; protobuf {
      class_name =&gt; "Animal::Unicorn"
      include_path =&gt; ['/path/to/protobuf/definitions/UnicornProtobuf.pb.rb']
    }
  }
}</pre>
</div>
</dd>
<dt>
<span class="term">
<a class="xref" href="plugins-filters-xml.html" title="Xml filter plugin">xml filter</a>
</span>
</dt>
<dd>
<p>
Parses XML into fields.
</p>
<p>The following config parses the whole XML document stored in the <code class="literal">message</code> field:</p>
<div class="pre_wrapper lang-json">
<pre class="programlisting prettyprint lang-json">filter {
  xml {
    source =&gt; "message"
  }
}</pre>
</div>
</dd>
</dl>
</div>
</div>
<div class="navfooter">
<span class="prev">
<a href="core-operations.html">« Performing Core Operations</a>
</span>
<span class="next">
<a href="field-extraction.html">Extracting Fields and Wrangling Data »</a>
</span>
</div>
</div>
</body>
</html>
