<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Use Logstash pipelines for parsing | Logstash Reference [7.7] | Elastic</title>
<link rel="home" href="index.html" title="Logstash Reference [7.7]"/>
<link rel="up" href="filebeat-modules.html" title="Working with Filebeat Modules"/>
<link rel="prev" href="use-ingest-pipelines.html" title="Use ingest pipelines for parsing"/>
<link rel="next" href="use-filebeat-modules-kafka.html" title="Example: Set up Filebeat modules to work with Kafka and Logstash"/>
<meta name="DC.type" content="Learn/Docs/Logstash/Reference/7.7"/>
<meta name="DC.subject" content="Logstash"/>
<meta name="DC.identifier" content="7.7"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body><div class="page_header">
<strong>IMPORTANT</strong>: No additional bug fixes or documentation updates
will be released for this version. For the latest information, see the
<a href="../current/index.html">current release documentation</a>.
</div>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="index.html">Logstash Reference [7.7]</a></span>
»
<span class="breadcrumb-link"><a href="filebeat-modules.html">Working with Filebeat Modules</a></span>
»
<span class="breadcrumb-node">Use Logstash pipelines for parsing</span>
</div>
<div class="navheader">
<span class="prev">
<a href="use-ingest-pipelines.html">« Use ingest pipelines for parsing</a>
</span>
<span class="next">
<a href="use-filebeat-modules-kafka.html">Example: Set up Filebeat modules to work with Kafka and Logstash »</a>
</span>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="logstash-config-for-filebeat-modules"></a>Use Logstash pipelines for parsing<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/7.7/docs/static/filebeat-modules.asciidoc">edit</a></h2>
</div></div></div>
<p>The examples in this section show how to build Logstash pipeline configurations that
replace the ingest pipelines provided with Filebeat modules. The pipelines
take the data collected by Filebeat modules, parse it into fields expected by
the Filebeat index, and send the fields to Elasticsearch so that you can visualize the
data in the pre-built dashboards provided by Filebeat.</p>
<p>This approach is more time consuming than using the existing ingest pipelines to
parse the data, but it gives you more control over how the data is processed.
By writing your own pipeline configurations, you can do additional processing,
such as dropping fields, after the fields are extracted, or you can move your
load from Elasticsearch ingest nodes to Logstash nodes.</p>
<p>Before deciding to replaced the ingest pipelines with Logstash configurations,
read <a class="xref" href="use-ingest-pipelines.html" title="Use ingest pipelines for parsing">Use ingest pipelines for parsing</a>.</p>
<p>Here are some examples that show how to implement Logstash configurations to replace
ingest pipelines:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="logstash-config-for-filebeat-modules.html#parsing-apache2" title="Apache 2 Logs">Apache 2 Logs</a>
</li>
<li class="listitem">
<a class="xref" href="logstash-config-for-filebeat-modules.html#parsing-mysql" title="MySQL Logs">MySQL Logs</a>
</li>
<li class="listitem">
<a class="xref" href="logstash-config-for-filebeat-modules.html#parsing-nginx" title="Nginx Logs">Nginx Logs</a>
</li>
<li class="listitem">
<a class="xref" href="logstash-config-for-filebeat-modules.html#parsing-system" title="System Logs">System Logs</a>
</li>
</ul>
</div>
<div class="tip admon">
<div class="icon"></div>
<div class="admon_content">
<p>Logstash provides an <a class="xref" href="ingest-converter.html" title="Converting Ingest Node Pipelines">ingest pipeline conversion tool</a>
to help you migrate ingest pipeline definitions to Logstash configs. The tool does
not currently support all the processors that are available for ingest node, but
it&#8217;s a good starting point.</p>
</div>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="parsing-apache2"></a>Apache 2 Logs<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/7.7/docs/static/filebeat-modules.asciidoc">edit</a></h3>
</div></div></div>
<p>The Logstash pipeline configuration in this example shows how to ship and parse
access and error logs collected by the
<a href="/guide/en/beats/filebeat/7.7/filebeat-module-apache.html" class="ulink" target="_top"><code class="literal">apache</code> Filebeat module</a>.</p>
<div class="pre_wrapper lang-json">
<pre class="programlisting prettyprint lang-json">input {
  beats {
    port =&gt; 5044
    host =&gt; "0.0.0.0"
  }
}
filter {
  if [fileset][module] == "apache2" {
    if [fileset][name] == "access" {
      grok {
        match =&gt; { "message" =&gt; ["%{IPORHOST:[apache2][access][remote_ip]} - %{DATA:[apache2][access][user_name]} \[%{HTTPDATE:[apache2][access][time]}\] \"%{WORD:[apache2][access][method]} %{DATA:[apache2][access][url]} HTTP/%{NUMBER:[apache2][access][http_version]}\" %{NUMBER:[apache2][access][response_code]} %{NUMBER:[apache2][access][body_sent][bytes]}( \"%{DATA:[apache2][access][referrer]}\")?( \"%{DATA:[apache2][access][agent]}\")?",
          "%{IPORHOST:[apache2][access][remote_ip]} - %{DATA:[apache2][access][user_name]} \\[%{HTTPDATE:[apache2][access][time]}\\] \"-\" %{NUMBER:[apache2][access][response_code]} -" ] }
        remove_field =&gt; "message"
      }
      mutate {
        add_field =&gt; { "read_timestamp" =&gt; "%{@timestamp}" }
      }
      date {
        match =&gt; [ "[apache2][access][time]", "dd/MMM/YYYY:H:m:s Z" ]
        remove_field =&gt; "[apache2][access][time]"
      }
      useragent {
        source =&gt; "[apache2][access][agent]"
        target =&gt; "[apache2][access][user_agent]"
        remove_field =&gt; "[apache2][access][agent]"
      }
      geoip {
        source =&gt; "[apache2][access][remote_ip]"
        target =&gt; "[apache2][access][geoip]"
      }
    }
    else if [fileset][name] == "error" {
      grok {
        match =&gt; { "message" =&gt; ["\[%{APACHE_TIME:[apache2][error][timestamp]}\] \[%{LOGLEVEL:[apache2][error][level]}\]( \[client %{IPORHOST:[apache2][error][client]}\])? %{GREEDYDATA:[apache2][error][message]}",
          "\[%{APACHE_TIME:[apache2][error][timestamp]}\] \[%{DATA:[apache2][error][module]}:%{LOGLEVEL:[apache2][error][level]}\] \[pid %{NUMBER:[apache2][error][pid]}(:tid %{NUMBER:[apache2][error][tid]})?\]( \[client %{IPORHOST:[apache2][error][client]}\])? %{GREEDYDATA:[apache2][error][message1]}" ] }
        pattern_definitions =&gt; {
          "APACHE_TIME" =&gt; "%{DAY} %{MONTH} %{MONTHDAY} %{TIME} %{YEAR}"
        }
        remove_field =&gt; "message"
      }
      mutate {
        rename =&gt; { "[apache2][error][message1]" =&gt; "[apache2][error][message]" }
      }
      date {
        match =&gt; [ "[apache2][error][timestamp]", "EEE MMM dd H:m:s YYYY", "EEE MMM dd H:m:s.SSSSSS YYYY" ]
        remove_field =&gt; "[apache2][error][timestamp]"
      }
    }
  }
}
output {
  elasticsearch {
    hosts =&gt; localhost
    manage_template =&gt; false
    index =&gt; "%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}"
  }
}</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="parsing-mysql"></a>MySQL Logs<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/7.7/docs/static/filebeat-modules.asciidoc">edit</a></h3>
</div></div></div>
<p>The Logstash pipeline configuration in this example shows how to ship and parse
error and slowlog logs collected by the
<a href="/guide/en/beats/filebeat/7.7/filebeat-module-mysql.html" class="ulink" target="_top"><code class="literal">mysql</code> Filebeat module</a>.</p>
<div class="pre_wrapper lang-json">
<pre class="programlisting prettyprint lang-json">input {
  beats {
    port =&gt; 5044
    host =&gt; "0.0.0.0"
  }
}
filter {
  if [fileset][module] == "mysql" {
    if [fileset][name] == "error" {
      grok {
        match =&gt; { "message" =&gt; ["%{LOCALDATETIME:[mysql][error][timestamp]} (\[%{DATA:[mysql][error][level]}\] )?%{GREEDYDATA:[mysql][error][message]}",
          "%{TIMESTAMP_ISO8601:[mysql][error][timestamp]} %{NUMBER:[mysql][error][thread_id]} \[%{DATA:[mysql][error][level]}\] %{GREEDYDATA:[mysql][error][message1]}",
          "%{GREEDYDATA:[mysql][error][message2]}"] }
        pattern_definitions =&gt; {
          "LOCALDATETIME" =&gt; "[0-9]+ %{TIME}"
        }
        remove_field =&gt; "message"
      }
      mutate {
        rename =&gt; { "[mysql][error][message1]" =&gt; "[mysql][error][message]" }
      }
      mutate {
        rename =&gt; { "[mysql][error][message2]" =&gt; "[mysql][error][message]" }
      }
      date {
        match =&gt; [ "[mysql][error][timestamp]", "ISO8601", "YYMMdd H:m:s" ]
        remove_field =&gt; "[mysql][error][time]"
      }
    }
    else if [fileset][name] == "slowlog" {
      grok {
        match =&gt; { "message" =&gt; ["^# User@Host: %{USER:[mysql][slowlog][user]}(\[[^\]]+\])? @ %{HOSTNAME:[mysql][slowlog][host]} \[(IP:[mysql][slowlog][ip])?\](\s*Id:\s* %{NUMBER:[mysql][slowlog][id]})?\n# Query_time: %{NUMBER:[mysql][slowlog][query_time][sec]}\s* Lock_time: %{NUMBER:[mysql][slowlog][lock_time][sec]}\s* Rows_sent: %{NUMBER:[mysql][slowlog][rows_sent]}\s* Rows_examined: %{NUMBER:[mysql][slowlog][rows_examined]}\n(SET timestamp=%{NUMBER:[mysql][slowlog][timestamp]};\n)?%{GREEDYMULTILINE:[mysql][slowlog][query]}"] }
        pattern_definitions =&gt; {
          "GREEDYMULTILINE" =&gt; "(.|\n)*"
        }
        remove_field =&gt; "message"
      }
      date {
        match =&gt; [ "[mysql][slowlog][timestamp]", "UNIX" ]
      }
      mutate {
        gsub =&gt; ["[mysql][slowlog][query]", "\n# Time: [0-9]+ [0-9][0-9]:[0-9][0-9]:[0-9][0-9](\\.[0-9]+)?$", ""]
      }
    }
  }
}
output {
  elasticsearch {
    hosts =&gt; localhost
    manage_template =&gt; false
    index =&gt; "%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}"
  }
}</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="parsing-nginx"></a>Nginx Logs<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/7.7/docs/static/filebeat-modules.asciidoc">edit</a></h3>
</div></div></div>
<p>The Logstash pipeline configuration in this example shows how to ship and parse
access and error logs collected by the
<a href="/guide/en/beats/filebeat/7.7/filebeat-module-nginx.html" class="ulink" target="_top"><code class="literal">nginx</code> Filebeat module</a>.</p>
<div class="pre_wrapper lang-json">
<pre class="programlisting prettyprint lang-json">input {
  beats {
    port =&gt; 5044
    host =&gt; "0.0.0.0"
  }
}
filter {
  if [fileset][module] == "nginx" {
    if [fileset][name] == "access" {
      grok {
        match =&gt; { "message" =&gt; ["%{IPORHOST:[nginx][access][remote_ip]} - %{DATA:[nginx][access][user_name]} \[%{HTTPDATE:[nginx][access][time]}\] \"%{WORD:[nginx][access][method]} %{DATA:[nginx][access][url]} HTTP/%{NUMBER:[nginx][access][http_version]}\" %{NUMBER:[nginx][access][response_code]} %{NUMBER:[nginx][access][body_sent][bytes]} \"%{DATA:[nginx][access][referrer]}\" \"%{DATA:[nginx][access][agent]}\""] }
        remove_field =&gt; "message"
      }
      mutate {
        add_field =&gt; { "read_timestamp" =&gt; "%{@timestamp}" }
      }
      date {
        match =&gt; [ "[nginx][access][time]", "dd/MMM/YYYY:H:m:s Z" ]
        remove_field =&gt; "[nginx][access][time]"
      }
      useragent {
        source =&gt; "[nginx][access][agent]"
        target =&gt; "[nginx][access][user_agent]"
        remove_field =&gt; "[nginx][access][agent]"
      }
      geoip {
        source =&gt; "[nginx][access][remote_ip]"
        target =&gt; "[nginx][access][geoip]"
      }
    }
    else if [fileset][name] == "error" {
      grok {
        match =&gt; { "message" =&gt; ["%{DATA:[nginx][error][time]} \[%{DATA:[nginx][error][level]}\] %{NUMBER:[nginx][error][pid]}#%{NUMBER:[nginx][error][tid]}: (\*%{NUMBER:[nginx][error][connection_id]} )?%{GREEDYDATA:[nginx][error][message]}"] }
        remove_field =&gt; "message"
      }
      mutate {
        rename =&gt; { "@timestamp" =&gt; "read_timestamp" }
      }
      date {
        match =&gt; [ "[nginx][error][time]", "YYYY/MM/dd H:m:s" ]
        remove_field =&gt; "[nginx][error][time]"
      }
    }
  }
}
output {
  elasticsearch {
    hosts =&gt; localhost
    manage_template =&gt; false
    index =&gt; "%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}"
  }
}</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="parsing-system"></a>System Logs<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/7.7/docs/static/filebeat-modules.asciidoc">edit</a></h3>
</div></div></div>
<p>The Logstash pipeline configuration in this example shows how to ship and parse
system logs collected by the
<a href="/guide/en/beats/filebeat/7.7/filebeat-module-system.html" class="ulink" target="_top"><code class="literal">system</code> Filebeat module</a>.</p>
<div class="pre_wrapper lang-json">
<pre class="programlisting prettyprint lang-json">input {
  beats {
    port =&gt; 5044
    host =&gt; "0.0.0.0"
  }
}
filter {
  if [fileset][module] == "system" {
    if [fileset][name] == "auth" {
      grok {
        match =&gt; { "message" =&gt; ["%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} sshd(?:\[%{POSINT:[system][auth][pid]}\])?: %{DATA:[system][auth][ssh][event]} %{DATA:[system][auth][ssh][method]} for (invalid user )?%{DATA:[system][auth][user]} from %{IPORHOST:[system][auth][ssh][ip]} port %{NUMBER:[system][auth][ssh][port]} ssh2(: %{GREEDYDATA:[system][auth][ssh][signature]})?",
                  "%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} sshd(?:\[%{POSINT:[system][auth][pid]}\])?: %{DATA:[system][auth][ssh][event]} user %{DATA:[system][auth][user]} from %{IPORHOST:[system][auth][ssh][ip]}",
                  "%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} sshd(?:\[%{POSINT:[system][auth][pid]}\])?: Did not receive identification string from %{IPORHOST:[system][auth][ssh][dropped_ip]}",
                  "%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} sudo(?:\[%{POSINT:[system][auth][pid]}\])?: \s*%{DATA:[system][auth][user]} :( %{DATA:[system][auth][sudo][error]} ;)? TTY=%{DATA:[system][auth][sudo][tty]} ; PWD=%{DATA:[system][auth][sudo][pwd]} ; USER=%{DATA:[system][auth][sudo][user]} ; COMMAND=%{GREEDYDATA:[system][auth][sudo][command]}",
                  "%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} groupadd(?:\[%{POSINT:[system][auth][pid]}\])?: new group: name=%{DATA:system.auth.groupadd.name}, GID=%{NUMBER:system.auth.groupadd.gid}",
                  "%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} useradd(?:\[%{POSINT:[system][auth][pid]}\])?: new user: name=%{DATA:[system][auth][useradd][name]}, UID=%{NUMBER:[system][auth][useradd][uid]}, GID=%{NUMBER:[system][auth][useradd][gid]}, home=%{DATA:[system][auth][useradd][home]}, shell=%{DATA:[system][auth][useradd][shell]}$",
                  "%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} %{DATA:[system][auth][program]}(?:\[%{POSINT:[system][auth][pid]}\])?: %{GREEDYMULTILINE:[system][auth][message]}"] }
        pattern_definitions =&gt; {
          "GREEDYMULTILINE"=&gt; "(.|\n)*"
        }
        remove_field =&gt; "message"
      }
      date {
        match =&gt; [ "[system][auth][timestamp]", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
      }
      geoip {
        source =&gt; "[system][auth][ssh][ip]"
        target =&gt; "[system][auth][ssh][geoip]"
      }
    }
    else if [fileset][name] == "syslog" {
      grok {
        match =&gt; { "message" =&gt; ["%{SYSLOGTIMESTAMP:[system][syslog][timestamp]} %{SYSLOGHOST:[system][syslog][hostname]} %{DATA:[system][syslog][program]}(?:\[%{POSINT:[system][syslog][pid]}\])?: %{GREEDYMULTILINE:[system][syslog][message]}"] }
        pattern_definitions =&gt; { "GREEDYMULTILINE" =&gt; "(.|\n)*" }
        remove_field =&gt; "message"
      }
      date {
        match =&gt; [ "[system][syslog][timestamp]", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
      }
    }
  }
}
output {
  elasticsearch {
    hosts =&gt; localhost
    manage_template =&gt; false
    index =&gt; "%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}"
  }
}</pre>
</div>
</div>

</div>
<div class="navfooter">
<span class="prev">
<a href="use-ingest-pipelines.html">« Use ingest pipelines for parsing</a>
</span>
<span class="next">
<a href="use-filebeat-modules-kafka.html">Example: Set up Filebeat modules to work with Kafka and Logstash »</a>
</span>
</div>
</div>
</body>
</html>
