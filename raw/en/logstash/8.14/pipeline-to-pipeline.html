<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Pipeline-to-pipeline communication | Logstash Reference [8.14] | Elastic</title>
<meta class="elastic" name="content" content="Pipeline-to-pipeline communication | Logstash Reference [8.14]">

<link rel="home" href="index.html" title="Logstash Reference [8.14]"/>
<link rel="up" href="configuration-advanced.html" title="Advanced Logstash Configurations"/>
<link rel="prev" href="multiple-pipelines.html" title="Multiple Pipelines"/>
<link rel="next" href="reloading-config.html" title="Reloading the Config File"/>
<meta class="elastic" name="product_version" content="8.14"/>
<meta class="elastic" name="product_name" content="Logstash"/>
<meta class="elastic" name="website_area" content="documentation"/>
<meta name="DC.type" content="Learn/Docs/Logstash/Reference/8.14"/>
<meta name="DC.subject" content="Logstash"/>
<meta name="DC.identifier" content="8.14"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body>
<div class="page_header">
<strong>IMPORTANT</strong>: No additional bug fixes or documentation updates
will be released for this version. For the latest information, see the
<a href="../current/index.html">current release documentation</a>.
</div>
<div class="navheader">
<span class="prev">
<a href="multiple-pipelines.html">« Multiple Pipelines</a>
</span>
<span class="next">
<a href="reloading-config.html">Reloading the Config File »</a>
</span>
</div>
<div class="book" lang="en">
<div class="titlepage">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="index.html">Logstash Reference [8.14]</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="configuration-advanced.html">Advanced Logstash Configurations</a></span>
</div>
<div>
<div><h1 class="title"><a id="id-1"></a>Pipeline-to-pipeline communication</h1><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/8.14/docs/static/pipeline-pipeline-config.asciidoc">edit</a></div>
</div>
<!--EXTRA-->
</div>
<div id="content">
<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="pipeline-to-pipeline"></a>Pipeline-to-pipeline communication<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/8.14/docs/static/pipeline-pipeline-config.asciidoc">edit</a></h2>
</div></div></div>
<p>When using the multiple pipeline feature of Logstash, you may want to connect multiple pipelines within the same Logstash instance. This configuration can be useful to isolate the execution of these pipelines, as well as to help break-up the logic of complex pipelines. The <code class="literal">pipeline</code> input/output enables a number of advanced architectural patterns discussed later in this document.</p>
<p>If you need to set up communication <em>between</em> Logstash instances, use either <a href="/guide/en/logstash/8.14/ls-to-ls.html" class="ulink" target="_top">Logstash-to-Logstash</a> communications, or an intermediary queue, such as Kafka or Redis.</p>
<div class="tip admon">
<div class="icon"></div>
<div class="admon_content">
<p>Persistent queues (PQs) can help keep data moving through pipelines.
See <a class="xref" href="persistent-queues.html#pq-pline-pline" title="PQs and pipeline-to-pipeline communication">PQs and pipeline-to-pipeline communication</a> to learn how PQs can enhance your
pipeline-to-pipeline communication strategy.</p>
</div>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="pipeline-to-pipeline-overview"></a>Configuration overview<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/8.14/docs/static/pipeline-pipeline-config.asciidoc">edit</a></h3>
</div></div></div>
<p>Use the <code class="literal">pipeline</code> input and <code class="literal">pipeline</code> output to connect two pipelines running within the same Logstash instance. These inputs use a client-server approach, where the <code class="literal">pipeline</code> input registers a virtual address that a <code class="literal">pipeline</code> output can connect to.</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Create a <em>downstream</em> pipeline that listens for events on a virtual address.
</li>
<li class="listitem">
Create an <em>upstream</em> pipeline that produces events, sending them through a <code class="literal">pipeline</code> output to one or more virtual addresses.
</li>
</ol>
</div>
<p>Here is a simple example of this configuration.</p>
<div class="pre_wrapper lang-yaml">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-yaml"># config/pipelines.yml
- pipeline.id: upstream
  config.string: input { stdin {} } output { pipeline { send_to =&gt; [myVirtualAddress] } }
- pipeline.id: downstream
  config.string: input { pipeline { address =&gt; myVirtualAddress } }</pre>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="how-pipeline-to-pipeline-works"></a>How it works<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/8.14/docs/static/pipeline-pipeline-config.asciidoc">edit</a></h4>
</div></div></div>
<p>The <code class="literal">pipeline</code> input acts as a virtual server listening on a single virtual address in the local process. Only <code class="literal">pipeline</code> outputs running on the same local Logstash can send events to this address. Pipeline <code class="literal">outputs</code> can send events to a list of virtual addresses. A <code class="literal">pipeline</code> output will be blocked if the downstream pipeline is blocked or unavailable.</p>
<p>When events are sent across pipelines, their data is fully copied. Modifications to an event in a downstream pipeline do not affect that event in any upstream pipelines.</p>
<p>The <code class="literal">pipeline</code> plugin may be the most efficient way to communicate between pipelines, but it still incurs a performance cost. Logstash must duplicate each event in full on the Java heap for each downstream pipeline. Using this feature may affect the heap memory utilization of Logstash.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="delivery-guarantees"></a>Delivery guarantees<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/8.14/docs/static/pipeline-pipeline-config.asciidoc">edit</a></h4>
</div></div></div>
<p>In its standard configuration the <code class="literal">pipeline</code> input/output has at-least-once delivery guarantees. The output will be
blocked if the address is blocked or unavailable.</p>
<p>By default, the <code class="literal">ensure_delivery</code> option on the <code class="literal">pipeline</code> output is set to <code class="literal">true.</code> If you change the
<code class="literal">ensure_delivery</code> flag to <code class="literal">false</code>, an <em>unavailable</em> downstream pipeline causes the sent message to be discarded. Note
that a pipeline is considered unavailable only when it is starting up or reloading, not when any of the plugins it
may contain are blocked. A <em>blocked</em> downstream pipeline blocks the sending output/pipeline regardless of the value of
the <code class="literal">ensure_delivery</code> flag. Use <code class="literal">ensure_delivery =&gt; false</code> when you want the ability to temporarily disable a
downstream pipeline without blocking any upstream pipelines sending to it.</p>
<p>These delivery guarantees also inform the shutdown behavior of this feature. When performing a pipeline reload, changes
will be made immediately as the user requests, even if that means removing a downstream pipeline receiving events from
an upstream pipeline. This will cause the upstream pipeline to block. You must restore the downstream pipeline to
cleanly shut down Logstash. You may issue a force kill, but inflight events may be lost unless the persistent queue is
enabled for that pipeline.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="avoid-cycles"></a>Avoid cycles<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/8.14/docs/static/pipeline-pipeline-config.asciidoc">edit</a></h4>
</div></div></div>
<p>When you connect pipelines, keep the data flowing in one direction. Looping data or connecting the pipelines into a cyclic graph can cause problems. Logstash waits for each pipeline&#8217;s work to complete before shutting down. Pipeline loops can prevent Logstash from shutting down cleanly.</p>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="architectural-patterns"></a>Architectural patterns<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/8.14/docs/static/pipeline-pipeline-config.asciidoc">edit</a></h3>
</div></div></div>
<p>You can use the <code class="literal">pipeline</code> input and output to better organize code, streamline control flow, and isolate the performance of complex configurations. There are infinite ways to connect pipelines. The ones presented here offer some ideas.</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="pipeline-to-pipeline.html#distributor-pattern" title="The distributor pattern">The distributor pattern</a>
</li>
<li class="listitem">
<a class="xref" href="pipeline-to-pipeline.html#output-isolator-pattern" title="The output isolator pattern">The output isolator pattern</a>
</li>
<li class="listitem">
<a class="xref" href="pipeline-to-pipeline.html#forked-path-pattern" title="The forked path pattern">The forked path pattern</a>
</li>
<li class="listitem">
<a class="xref" href="pipeline-to-pipeline.html#collector-pattern" title="The collector pattern">The collector pattern</a>
</li>
</ul>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>These examples use <code class="literal">config.string</code> to illustrate the flows.
You can also use configuration files for pipeline-to-pipeline communication.</p>
</div>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="distributor-pattern"></a>The distributor pattern<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/8.14/docs/static/pipeline-pipeline-config.asciidoc">edit</a></h4>
</div></div></div>
<p>You can use the distributor pattern in situations where there are multiple types of data coming through a single input, each with its own complex set of processing rules. With the distributor pattern one pipeline is used to route data to other pipelines based on type. Each type is routed to a pipeline with only the logic for handling that type. In this way each type&#8217;s logic can be isolated.</p>
<p>As an example, in many organizations a single beats input may be used to receive traffic from a variety of sources, each with its own processing logic. A common way to deal with this type of data is to have a number of <code class="literal">if</code> conditions separating the traffic and processing each type differently. This approach can quickly get messy when configs are long and complex.</p>
<p>Here is an example distributor pattern configuration.</p>
<div class="pre_wrapper lang-yaml">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-yaml"># config/pipelines.yml
- pipeline.id: beats-server
  config.string: |
    input { beats { port =&gt; 5044 } }
    output {
        if [type] == apache {
          pipeline { send_to =&gt; weblogs }
        } else if [type] == system {
          pipeline { send_to =&gt; syslog }
        } else {
          pipeline { send_to =&gt; fallback }
        }
    }
- pipeline.id: weblog-processing
  config.string: |
    input { pipeline { address =&gt; weblogs } }
    filter {
       # Weblog filter statements here...
    }
    output {
      elasticsearch { hosts =&gt; [es_cluster_a_host] }
    }
- pipeline.id: syslog-processing
  config.string: |
    input { pipeline { address =&gt; syslog } }
    filter {
       # Syslog filter statements here...
    }
    output {
      elasticsearch { hosts =&gt; [es_cluster_b_host] }
    }
- pipeline.id: fallback-processing
    config.string: |
    input { pipeline { address =&gt; fallback } }
    output { elasticsearch { hosts =&gt; [es_cluster_b_host] } }</pre>
</div>
<p>Notice how following the flow of data is a simple due to the fact that each pipeline only works on a single specific task.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="output-isolator-pattern"></a>The output isolator pattern<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/8.14/docs/static/pipeline-pipeline-config.asciidoc">edit</a></h4>
</div></div></div>
<p>You can use the output isolator pattern to prevent Logstash from becoming blocked if one of multiple outputs experiences a temporary failure. Logstash, by default, is blocked when any single output is down. This behavior is important in guaranteeing at-least-once delivery of data.</p>
<p>For example, a server might be configured to send log data to both Elasticsearch and an HTTP endpoint. The HTTP endpoint might be frequently unavailable due to regular service or other reasons. In this scenario, data would be paused from sending to Elasticsearch any time the HTTP endpoint is down.</p>
<p>Using the output isolator pattern and persistent queues, we can continue sending to Elasticsearch, even when one output is down.</p>
<p>Here is an example of this scenario using the output isolator pattern.</p>
<div class="pre_wrapper lang-yaml">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-yaml"># config/pipelines.yml
- pipeline.id: intake
  config.string: |
    input { beats { port =&gt; 5044 } }
    output { pipeline { send_to =&gt; [es, http] } }
- pipeline.id: buffered-es
  queue.type: persisted
  config.string: |
    input { pipeline { address =&gt; es } }
    output { elasticsearch { } }
- pipeline.id: buffered-http
  queue.type: persisted
  config.string: |
    input { pipeline { address =&gt; http } }
    output { http { } }</pre>
</div>
<p>In this architecture, each output has its own queue with its own tuning and settings. Note that this approach uses up to twice as much disk space and incurs three times as much serialization/deserialization cost as a single pipeline.</p>
<p>If any of the persistent queues of the downstream pipelines (in the example above, <code class="literal">buffered-es</code> and <code class="literal">buffered-http</code>) become full, both outputs will stop.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="forked-path-pattern"></a>The forked path pattern<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/8.14/docs/static/pipeline-pipeline-config.asciidoc">edit</a></h4>
</div></div></div>
<p>You can use the forked path pattern for situations where a single event must be processed more than once according to different sets of rules. Before the <code class="literal">pipeline</code> input and output were available, this need was commonly addressed through creative use of the <code class="literal">clone</code> filter and <code class="literal">if/else</code> rules.</p>
<p>Let&#8217;s imagine a use case where we receive data and index the full event in our own systems, but publish a redacted version of the data to a partner&#8217;s S3 bucket. We might use the output isolator pattern described above to decouple our writes to either system. The distinguishing feature of the forked path pattern is the existence of additional rules in the downstream pipelines.</p>
<p>Here is an example of the forked path configuration.</p>
<div class="pre_wrapper lang-yaml">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-yaml"># config/pipelines.yml
- pipeline.id: intake
  queue.type: persisted
  config.string: |
    input { beats { port =&gt; 5044 } }
    output { pipeline { send_to =&gt; ["internal-es", "partner-s3"] } }
- pipeline.id: buffered-es
  queue.type: persisted
  config.string: |
    input { pipeline { address =&gt; "internal-es" } }
    # Index the full event
    output { elasticsearch { } }
- pipeline.id: partner
  queue.type: persisted
  config.string: |
    input { pipeline { address =&gt; "partner-s3" } }
    filter {
      # Remove the sensitive data
      mutate { remove_field =&gt; 'sensitive-data' }
    }
    output { s3 { } } # Output to partner's bucket</pre>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="collector-pattern"></a>The collector pattern<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/8.14/docs/static/pipeline-pipeline-config.asciidoc">edit</a></h4>
</div></div></div>
<p>You can use the collector pattern when you want to define a common set of outputs and pre-output filters that many disparate pipelines might use. This pattern is the opposite of the distributor pattern. In this pattern many pipelines flow in to a single pipeline where they share outputs and processing. This pattern simplifies configuration at the cost of reducing isolation, since all data is sent through a single pipeline.</p>
<p>Here is an example of the collector pattern.</p>
<div class="pre_wrapper lang-yaml">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-yaml"># config/pipelines.yml
- pipeline.id: beats
  config.string: |
    input { beats { port =&gt; 5044 } }
    output { pipeline { send_to =&gt; [commonOut] } }
- pipeline.id: kafka
  config.string: |
    input { kafka { ... } }
    output { pipeline { send_to =&gt; [commonOut] } }
- pipeline.id: partner
  # This common pipeline enforces the same logic whether data comes from Kafka or Beats
  config.string: |
    input { pipeline { address =&gt; commonOut } }
    filter {
      # Always remove sensitive data from all input sources
      mutate { remove_field =&gt; 'sensitive-data' }
    }
    output { elasticsearch { } }</pre>
</div>
</div>

</div>

</div>
</div>
</div><div class="navfooter">
<span class="prev">
<a href="multiple-pipelines.html">« Multiple Pipelines</a>
</span>
<span class="next">
<a href="reloading-config.html">Reloading the Config File »</a>
</span>
</div>
</body>
</html>
