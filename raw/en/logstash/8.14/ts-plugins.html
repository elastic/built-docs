<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Troubleshooting specific plugins | Logstash Reference [8.14] | Elastic</title>
<meta class="elastic" name="content" content="Troubleshooting specific plugins | Logstash Reference [8.14]">

<link rel="home" href="index.html" title="Logstash Reference [8.14]"/>
<link rel="up" href="troubleshooting.html" title="Troubleshooting"/>
<link rel="prev" href="ts-plugins-general.html" title="Troubleshooting plugins"/>
<link rel="next" href="contributing-to-logstash.html" title="Contributing to Logstash"/>
<meta class="elastic" name="product_version" content="8.14"/>
<meta class="elastic" name="product_name" content="Logstash"/>
<meta class="elastic" name="website_area" content="documentation"/>
<meta name="DC.type" content="Learn/Docs/Logstash/Reference/8.14"/>
<meta name="DC.subject" content="Logstash"/>
<meta name="DC.identifier" content="8.14"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body>
<div class="page_header">
<strong>IMPORTANT</strong>: No additional bug fixes or documentation updates
will be released for this version. For the latest information, see the
<a href="../current/index.html">current release documentation</a>.
</div>
<div class="navheader">
<span class="prev">
<a href="ts-plugins-general.html">« Troubleshooting plugins</a>
</span>
<span class="next">
<a href="contributing-to-logstash.html">Contributing to Logstash »</a>
</span>
</div>
<div class="book" lang="en">
<div class="titlepage">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="index.html">Logstash Reference [8.14]</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="troubleshooting.html">Troubleshooting</a></span>
</div>
<div>
<div><h1 class="title"><a id="id-1"></a>Troubleshooting specific plugins</h1><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/8.14/docs/static/troubleshoot/ts-plugins.asciidoc">edit</a></div>
</div>
<!--EXTRA-->
</div>
<div id="content">
<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="ts-plugins"></a>Troubleshooting specific plugins<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/8.14/docs/static/troubleshoot/ts-plugins.asciidoc">edit</a></h2>
</div></div></div>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="ts-kafka"></a>Kafka issues and solutions<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/8.14/docs/static/troubleshoot/ts-kafka.asciidoc">edit</a></h3>
</div></div></div>
<h5><a id="ts-kafka-timeout"></a>Kafka session timeout issues (input)<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/8.14/docs/static/troubleshoot/ts-kafka.asciidoc">edit</a></h5>
<p><span class="strong strong"><strong>Symptoms</strong></span></p>
<p>Throughput issues and duplicate event processing Logstash logs warnings:</p>
<pre class="screen">[2017-10-18T03:37:59,302][WARN][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
Auto offset commit failed for group clap_tx1: Commit cannot be completed since
the group has already rebalanced and assigned the partitions to another member.</pre>
<p>The time between subsequent calls to <code class="literal">poll()</code> was longer than the
configured <code class="literal">session.timeout.ms</code>, which typically implies that the poll loop is
spending too much time processing messages. You can address this by
increasing the session timeout or by reducing the maximum size of batches
returned in <code class="literal">poll()</code> with <code class="literal">max.poll.records</code>.</p>
<pre class="screen">[INFO][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator] Revoking
previously assigned partitions [] for group log-ronline-node09
`[2018-01-29T14:54:06,485][INFO]`[org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
Setting newly assigned partitions [elk-pmbr-9] for group log-pmbr</pre>
<p><span class="strong strong"><strong>Background</strong></span></p>
<p>Kafka tracks the individual consumers in a consumer group (for example, a number
of Logstash instances) and tries to give each consumer one or more specific
partitions of data in the topic they’re consuming. In order to achieve this,
Kafka tracks whether or not a consumer (Logstash Kafka input thread) is making
progress on their assigned partition, and reassigns partitions that have not
made progress in a set timeframe.</p>
<p>When Logstash requests more events from the Kafka Broker than it can process within
the timeout, it triggers reassignment of partitions. Reassignment of partitions
takes time, and can cause duplicate processing of events and significant
throughput problems.</p>
<p><span class="strong strong"><strong>Possible solutions</strong></span></p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Reduce the number of records per request that Logstash polls from the Kafka Broker in one request,
</li>
<li class="listitem">
Reduce the number of Kafka input threads, and/or
</li>
<li class="listitem">
Increase the relevant timeouts in the Kafka Consumer configuration.
</li>
</ul>
</div>
<p><span class="strong strong"><strong>Details</strong></span></p>
<p>The <code class="literal">max_poll_records</code> option sets the number of records to be pulled in one request.
If it exceeds the default value of 500, try reducing it.</p>
<p>The <code class="literal">consumer_threads</code> option sets the number of input threads. If the value exceeds
the number of pipeline workers configured in the <code class="literal">logstash.yml</code> file, it should
certainly be reduced.
If the value is greater than 4, try reducing it to <code class="literal">4</code> or less if the client has
the time/resources for it. Try starting with a value of <code class="literal">1</code>, and then
incrementing from there to find the optimal performance.</p>
<p>The <code class="literal">session_timeout_ms</code> option sets the relevant timeout. Set it to a value
that ensures that the number of events in <code class="literal">max_poll_records</code> can be safely
processed within the time limit.</p>
<pre class="screen">EXAMPLE
Pipeline throughput is `10k/s` and `max_poll_records` is set to 1k =&gt;. The value
must be at least 100ms if `consumer_threads` is set to `1`. If it is set to a
higher value `n`, then the minimum session timeout increases proportionally to
`n * 100ms`.</pre>
<p>In practice the value must be set much higher than the theoretical value because
the behavior of the outputs and filters in a pipeline follows a distribution.
The value should also be higher than the maximum time you expect your outputs to
stall. The default setting is <code class="literal">10s == 10000ms</code>. If you are experiencing
periodic problems with an output that can stall because of load or similar
effects (such as the Elasticsearch output), there is little downside to
increasing this value significantly to say <code class="literal">60s</code>.</p>
<p>From a performance perspective, decreasing the <code class="literal">max_poll_records</code> value is preferable
to increasing the timeout value. Increasing the timeout is your only option if the
client’s issues are caused by periodically stalling outputs. Check logs for
evidence of stalling outputs, such as <code class="literal">ES output logging status 429</code>.</p>
<h5><a id="ts-schema-registry"></a>Kafka input plugin crashes when using schema registry<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/8.14/docs/static/troubleshoot/ts-kafka.asciidoc">edit</a></h5>
<p>By default, the kafka input plugin checks connectivity and validates the schema registry during plugin registration before events are processed.
In some circumstances, this process may fail when it tries to validate an authenticated schema registry, causing the plugin to crash.</p>
<p>The plugin offers a <code class="literal">schema_registry_validation</code> setting to change the default behavior.
This setting allows the plugin to skip validation during registration, which allows the plugin to continue and events to be processed.
See the <a class="xref" href="plugins-inputs-kafka.html#plugins-inputs-kafka-schema_registry_validation" title="schema_registry_validation">kafka input plugin documentation</a> for more information about the plugin and other configuration options.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>An incorrectly configured schema registry will still stop the plugin from processing events.</p>
</div>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>The default setting of <code class="literal">auto</code> is the best option for most circumstances and should not need to be changed.</p>
</div>
</div>
<h5><a id="ts-kafka-many-offset-commits"></a>Large number of offset commits (input)<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/8.14/docs/static/troubleshoot/ts-kafka.asciidoc">edit</a></h5>
<p><span class="strong strong"><strong>Symptoms</strong></span></p>
<p>Logstash’s Kafka Input is causing a much higher number of commits to
the offset topic than expected. Often the complaint also mentions redundant
offset commits where the same offset is committed repeatedly.</p>
<p><span class="strong strong"><strong>Solution</strong></span></p>
<p>For Kafka Broker versions 0.10.2.1 to 1.0.x: The problem is caused by a bug in
Kafka. <a href="https://issues.apache.org/jira/browse/KAFKA-6362" class="ulink" target="_top">https://issues.apache.org/jira/browse/KAFKA-6362</a> The client’s best option
is upgrading their Kafka Brokers to version 1.1 or newer.</p>
<p>For older versions of
Kafka or if the above does not fully resolve the issue: The problem can also be
caused by setting the value for <code class="literal">poll_timeout_ms</code> too low relative to the rate
at which the Kafka Brokers receive events themselves (or if Brokers periodically
idle between receiving bursts of events). Increasing the value set for
<code class="literal">poll_timeout_ms</code> proportionally decreases the number of offsets commits in
this scenario. For example, raising it by 10x will lead to 10x fewer offset commits.</p>
<h5><a id="ts-kafka-codec-errors-input"></a>Codec Errors in Kafka Input (before Plugin Version 6.3.4 only)<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/8.14/docs/static/troubleshoot/ts-kafka.asciidoc">edit</a></h5>
<p><span class="strong strong"><strong>Symptoms</strong></span></p>
<p>Logstash Kafka input randomly logs errors from the configured codec and/or reads
events incorrectly (partial reads, mixing data between multiple events etc.).</p>
<pre class="screen">Log example:  [2018-02-05T13:51:25,773][FATAL][logstash.runner          ] An
unexpected error occurred! {:error=&gt;#&lt;TypeError: can't convert nil into String&gt;,
:backtrace=&gt;["org/jruby/RubyArray.java:1892:in `join'",
"org/jruby/RubyArray.java:1898:in `join'",
"/usr/share/logstash/logstash-core/lib/logstash/util/buftok.rb:87:in `extract'",
"/usr/share/logstash/vendor/bundle/jruby/1.9/gems/logstash-codec-line-3.0.8/lib/logstash/codecs/line.rb:38:in
`decode'",
"/usr/share/logstash/vendor/bundle/jruby/1.9/gems/logstash-input-kafka-5.1.11/lib/logstash/inputs/kafka.rb:241:in
`thread_runner'",
"file:/usr/share/logstash/vendor/jruby/lib/jruby.jar!/jruby/java/java_ext/java.lang.rb:12:in
`each'",
"/usr/share/logstash/vendor/bundle/jruby/1.9/gems/logstash-input-kafka-5.1.11/lib/logstash/inputs/kafka.rb:240:in
`thread_runner'"]}</pre>
<p><span class="strong strong"><strong>Background</strong></span></p>
<p>There was a bug in the way the Kafka Input plugin was handling codec instances
when running on multiple threads (<code class="literal">consumer_threads</code> set to &gt; 1).
<a href="https://github.com/logstash-plugins/logstash-input-kafka/issues/210" class="ulink" target="_top">https://github.com/logstash-plugins/logstash-input-kafka/issues/210</a></p>
<p><span class="strong strong"><strong>Solution</strong></span></p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Upgrade Kafka Input plugin to v. 6.3.4 or later.
</li>
<li class="listitem">
If (and only if) upgrading is not possible, set <code class="literal">consumer_threads</code> to <code class="literal">1</code>.
</li>
</ul>
</div>
<h5><a id="ts-kafka-kerberos-debug"></a>Setting up debugging for Kerberos SASL<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/8.14/docs/static/troubleshoot/ts-kafka.asciidoc">edit</a></h5>
<p>You can set up your machine to help you troubleshoot authentication failures in the Kafka client.</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<p>In <code class="literal">config/jvm.options</code>, add:</p>
<div class="pre_wrapper lang-txt">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-txt">-Dsun.security.krb5.debug=true</pre>
</div>
</li>
<li class="listitem">
<p>In <code class="literal">config/log4j2.properties</code>, add:</p>
<div class="pre_wrapper lang-txt">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-txt">logger.kafkainput.name = logstash.inputs.kafka
logger.kafkainput.level = debug
logger.kafkaoutput.name = logstash.outputs.kafka
logger.kafkaoutput.level = debug
logger.kafka.name = org.apache.kafka
logger.kafka.level = debug</pre>
</div>
</li>
</ul>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>Logging entries for Kerberos are NOT sent through Log4j but go directly to the console.</p>
</div>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="ts-azure"></a>Azure Event Hub issues and solutions<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/8.14/docs/static/troubleshoot/ts-azure.asciidoc">edit</a></h3>
</div></div></div>
<h5><a id="ts-azure-http"></a>Event Hub plugin can&#8217;t connect to Storage blob (input)<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/8.14/docs/static/troubleshoot/ts-azure.asciidoc">edit</a></h5>
<p><span class="strong strong"><strong>Symptoms</strong></span></p>
<p>Azure EventHub can&#8217;t connect to blob storage:</p>
<pre class="screen">[2024-01-01T13:13:13,123][ERROR][com.microsoft.azure.eventprocessorhost.AzureStorageCheckpointLeaseManager][azure_eventhub_pipeline][eh_input_plugin] host logstash-a0a00a00-0aa0-0000-aaaa-0a00a0a0aaaa: Failure while creating lease store
com.microsoft.azure.storage.StorageException: The client could not finish the operation within specified maximum execution timeout.</pre>
<p>Plugin can&#8217;t complete registration phase because it can&#8217;t connect to Azure Blob Storage configured
in the plugin <code class="literal">storage_connection</code> setting.</p>
<p><span class="strong strong"><strong>Background</strong></span></p>
<p>Azure Event Hub plugin can share the offset position of a consumer group with
other consumers only if Blob Storage connection settings are configured.
EventHub uses the AMQP protocol to transfer data, but Blob storage uses a
library which leverages the JDK&#8217;s http client, <code class="literal">HttpURLConnection</code>.
To troubleshoot HTTP connection problems, which may be related to proxy settings, the
logging level for this part of the JDK has to be increased. The problem is that
JDK uses Java Util Logging for its internal logging needs, which is not configurable
with the standard <code class="literal">log4j2.properties</code> shipped with Logstash.</p>
<p><span class="strong strong"><strong>Possible solutions</strong></span></p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Configure Logstash settings to enable the JDK logging.
</li>
</ul>
</div>
<p><span class="strong strong"><strong>Details</strong></span></p>
<p>Steps to enable JDK logging on Logstash:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Create a properties file with the logging definitions for Java Util Logging (JUL).
</li>
<li class="listitem">
Configure a JVM property to inform JUL to use such definitions file.
</li>
</ul>
</div>
<p><span class="strong strong"><strong>JUL definitions</strong></span></p>
<p>Create a file that you can use to define logging levels, handlers and loggers.
For example, <code class="literal">&lt;LS_HOME&gt;/conf/jul.properties</code>.</p>
<div class="pre_wrapper lang-txt">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-txt">handlers= java.util.logging.ConsoleHandler,java.util.logging.FileHandler
.level= ALL
java.util.logging.FileHandler.pattern = &lt;USER's LOGS FOLDER&gt;/logs/jul_http%u.log <a id="CO51-1"></a><i class="conum" data-value="1"></i>
java.util.logging.FileHandler.limit = 50000
java.util.logging.FileHandler.count = 1
java.util.logging.FileHandler.level=ALL
java.util.logging.FileHandler.maxLocks = 100
java.util.logging.FileHandler.formatter = java.util.logging.SimpleFormatter

java.util.logging.ConsoleHandler.level = INFO # or put FINE
java.util.logging.ConsoleHandler.formatter = java.util.logging.SimpleFormatter

# defines the logger we are interested in
sun.net.www.protocol.http.HttpURLConnection.level = ALL <a id="CO51-2"></a><i class="conum" data-value="2"></i> <a id="CO51-3"></a><i class="conum" data-value="3"></i></pre>
</div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO51-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>The log file will be created in a path defined by the user (<code class="literal">&lt;USER's LOGS FOLDER&gt;/logs/</code>)</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO51-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p>This configuration enables the <code class="literal">sun.net.www.protocol.http.HttpURLConnection</code> logger, and</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO51-3"><i class="conum" data-value="3"></i></a></p>
</td>
<td align="left" valign="top">
<p>sets the logging level to <code class="literal">ALL</code>. It will log all messages directed to it, from highest to lowest priority.</p>
</td>
</tr>
</table>
</div>
<p><span class="strong strong"><strong>JVM property</strong></span></p>
<p>To inform the JUL framework of the selected definitions file a property (<code class="literal">java.util.logging.config.file</code>) has to be
evaluated, this is where Logstash&#8217;s <code class="literal">config/jvm.properties</code> come in handy.
Edit the file adding the property, pointing to the path where the JUL definitions file was created:</p>
<div class="pre_wrapper lang-txt">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-txt">-Djava.util.logging.config.file=&lt;LS_HOME&gt;/conf/jul.properties</pre>
</div>
<p>The logs could contain sensible information, such credentials, and could be verbose but should give
hits on the connection problem at HTTP level with the Azure Blob Storage.</p>
<h2><a id="ts-other"></a>Other issues<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/logstash/edit/8.14/docs/static/troubleshoot/ts-other-issues.asciidoc">edit</a></h2>
<p>Coming soon, and you can help! If you have something to add, please:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
create an issue at
<a href="https://github.com/elastic/logstash/issues" class="ulink" target="_top">https://github.com/elastic/logstash/issues</a>, or
</li>
<li class="listitem">
create a pull request with your proposed changes at <a href="https://github.com/elastic/logstash" class="ulink" target="_top">https://github.com/elastic/logstash</a>.
</li>
</ul>
</div>
<p>Also check out the <a href="https://discuss.elastic.co/c/logstash" class="ulink" target="_top">Logstash discussion
forum</a>.</p>
</div>

</div>
</div>
</div><div class="navfooter">
<span class="prev">
<a href="ts-plugins-general.html">« Troubleshooting plugins</a>
</span>
<span class="next">
<a href="contributing-to-logstash.html">Contributing to Logstash »</a>
</span>
</div>
</body>
</html>
