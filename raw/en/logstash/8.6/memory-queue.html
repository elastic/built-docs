<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Memory queue | Logstash Reference [8.6] | Elastic</title>
<link rel="home" href="index.html" title="Logstash Reference [8.6]"/>
<link rel="up" href="resiliency.html" title="Queues and data resiliency"/>
<link rel="prev" href="resiliency.html" title="Queues and data resiliency"/>
<link rel="next" href="persistent-queues.html" title="Persistent queues (PQ)"/>
<meta name="DC.type" content="Learn/Docs/Logstash/Reference/8.6"/>
<meta name="DC.subject" content="Logstash"/>
<meta name="DC.identifier" content="8.6"/>
</head>
<body>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="index.html">Logstash Reference [8.6]</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="resiliency.html">Queues and data resiliency</a></span>
</div>
<div class="navheader">
<span class="prev">
<a href="resiliency.html">« Queues and data resiliency</a>
</span>
<span class="next">
<a href="persistent-queues.html">Persistent queues (PQ) »</a>
</span>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="memory-queue"></a>Memory queue<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/logstash/edit/8.6/docs/static/mem-queue.asciidoc">edit</a></h2>
</div></div></div>
<p>By default, Logstash uses in-memory bounded queues between pipeline stages (inputs → pipeline workers) to buffer events.
If Logstash experiences a temporary machine failure, the contents of the memory queue will be lost.
Temporary machine failures are scenarios where Logstash or its host machine are terminated abnormally, but are capable of being restarted.</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="mem-queue-benefits"></a>Benefits of memory queues<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/logstash/edit/8.6/docs/static/mem-queue.asciidoc">edit</a></h3>
</div></div></div>
<p>The memory queue might be a good choice if you value throughput over data resiliency.</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Easier configuration
</li>
<li class="listitem">
Easier management and administration
</li>
<li class="listitem">
Faster throughput
</li>
</ul>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="mem-queue-limitations"></a>Limitations of memory queues<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/logstash/edit/8.6/docs/static/mem-queue.asciidoc">edit</a></h3>
</div></div></div>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Can lose data in abnormal termination
</li>
<li class="listitem">
Don&#8217;t do well handling sudden bursts of data, where extra capacity in needed for Logstash to catch up
</li>
</ul>
</div>
<div class="tip admon">
<div class="icon"></div>
<div class="admon_content">
<p>Consider using <a class="xref" href="persistent-queues.html" title="Persistent queues (PQ)">persistent queues</a> to avoid these limitations.</p>
</div>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="sizing-mem-queue"></a>Memory queue size<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/logstash/edit/8.6/docs/static/mem-queue.asciidoc">edit</a></h3>
</div></div></div>
<p>Memory queue size is not configured directly.
Instead, it depends on how you have Logstash tuned.</p>
<p>Its upper bound is defined by <code class="literal">pipeline.workers</code> (default: number of CPUs) times the <code class="literal">pipeline.batch.size</code> (default: 125) events.
This value, called the "inflight count," determines maximum number of events that can be held in each memory queue.</p>
<p>Doubling the number of workers OR doubling the batch size will effectively double the memory queue&#8217;s capacity (and memory usage).
Doubling both will <em>quadruple</em> the capacity (and usage).</p>
<div class="important admon">
<div class="icon"></div>
<div class="admon_content">
<p>Each pipeline has its own queue.</p>
</div>
</div>
<p>See <a class="xref" href="tuning-logstash.html" title="Tuning and Profiling Logstash Performance">Tuning and Profiling Logstash Performance</a> for more info on the effects of adjusting <code class="literal">pipeline.batch.size</code> and <code class="literal">pipeline.workers</code>.</p>
<p>If you need to absorb bursts of traffic, consider using <a class="xref" href="persistent-queues.html" title="Persistent queues (PQ)">persistent queues</a> instead.
Persistent queues are bound to allocated capacity on disk.</p>
<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="mq-settings"></a>Settings that affect queue size<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/logstash/edit/8.6/docs/static/mem-queue.asciidoc">edit</a></h4>
</div></div></div>
<p>These values can be configured in <code class="literal">logstash.yml</code> and <code class="literal">pipelines.yml</code>.</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
pipeline.batch.size
</span>
</dt>
<dd>
Number events to retrieve from inputs before sending to filters+workers
The default is 125.
</dd>
<dt>
<span class="term">
pipelines.workers
</span>
</dt>
<dd>
Number of workers that will, in parallel, execute the filters+outputs stage of the pipeline.
This value defaults to the number of the host&#8217;s CPU cores.
</dd>
</dl>
</div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="backpressure-mem-queue"></a>Back pressure<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/logstash/edit/8.6/docs/static/mem-queue.asciidoc">edit</a></h3>
</div></div></div>
<p>When the queue is full, Logstash puts back pressure on the inputs to stall data
flowing into Logstash.
This mechanism helps Logstash control the rate of data flow at the input stage
without overwhelming outputs like Elasticsearch.</p>
<p>Each input handles back pressure independently.</p>
</div>

</div>
<div class="navfooter">
<span class="prev">
<a href="resiliency.html">« Queues and data resiliency</a>
</span>
<span class="next">
<a href="persistent-queues.html">Persistent queues (PQ) »</a>
</span>
</div>
</div>
</body>
</html>
