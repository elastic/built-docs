<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Feature importance | Machine Learning in the Elastic Stack [7.10] | Elastic</title>
<link rel="home" href="index.html" title="Machine Learning in the Elastic Stack [7.10]"/>
<link rel="up" href="ml-dfa-concepts.html" title="Concepts"/>
<link rel="prev" href="ml-feature-encoding.html" title="Feature encoding"/>
<link rel="next" href="hyperparameters.html" title="Hyperparameter optimization"/>
<meta name="DC.type" content="Learn/Docs/Elastic Stack/Machine learning/7.10"/>
<meta name="DC.subject" content="Machine learning"/>
<meta name="DC.identifier" content="7.10"/>
</head>
<body>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="index.html">Machine Learning in the Elastic Stack [7.10]</a></span>
»
<span class="breadcrumb-link"><a href="ml-dfanalytics.html">Data frame analytics</a></span>
»
<span class="breadcrumb-link"><a href="ml-dfa-concepts.html">Concepts</a></span>
»
<span class="breadcrumb-node">Feature importance</span>
</div>
<div class="navheader">
<span class="prev">
<a href="ml-feature-encoding.html">« Feature encoding</a>
</span>
<span class="next">
<a href="hyperparameters.html">Hyperparameter optimization »</a>
</span>
</div>
<div class="section xpack">
<div class="titlepage"><div><div>
<h2 class="title"><a id="ml-feature-importance"></a>Feature importance<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/7.10/docs/en/stack/ml/df-analytics/ml-feature-importance.asciidoc">edit</a><a class="xpack_tag" href="/subscriptions"></a></h2>
</div></div></div>
<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<p>This functionality is experimental and may be changed or removed completely in a future release. Elastic will take a best effort approach to fix any issues, but experimental features are not subject to the support SLA of official GA features.</p>
</div>
</div>
<p>Feature importance values indicate which fields had the biggest impact on each
prediction that is generated by classification or regression analysis. Each
feature importance value has both a magnitude and a direction (positive or negative),
which indicate how each field (or <em>feature</em> of a data point) affects a
particular prediction.</p>
<p>The purpose of feature importance is to help you determine whether the predictions are
sensible. Is the relationship between the dependent variable and the important
features supported by your domain knowledge? The lessons you learn about the
importance of specific features might also affect your decision to include them
in future iterations of your trained model.</p>
<p>You can see the average magnitude of the feature importance values for each field across
all the training data in Kibana or by using the
<a href="/guide/en/elasticsearch/reference/7.10/get-inference.html" class="ulink" target="_top">get trained model API</a>. For example, Kibana shows the
total feature importance for each field in regression or binary
classification analysis results as follows:</p>
<div class="imageblock screenshot">
<div class="content">
<img src="images/flights-regression-total-importance.jpg" alt="Total feature importance values for a regression data frame analytics job in Kibana">
</div>
</div>
<p>If the classification analysis involves more than two classes, Kibana uses colors to show
how the impact of each field varies by class. For example:</p>
<div class="imageblock screenshot">
<div class="content">
<img src="images/diamonds-classification-total-importance.png" alt="Total feature importance values for a classification data frame analytics job in Kibana">
</div>
</div>
<p>You can also examine the feature importance values for each individual
prediction. In Kibana, you can see these values in JSON objects or decision plots:</p>
<div class="imageblock screenshot">
<div class="content">
<img src="images/flights-regression-decision-plot.png" alt="Feature importance values for a regression data frame analytics job in Kibana">
</div>
</div>
<p>For regression analysis, each decision plot starts at a shared baseline, which is
the average of the prediction values for all the data points in the training
data set. When you add all of the feature importance values for a particular
data point to that baseline, you arrive at the numeric prediction value. If a
feature importance value is negative, it reduces the prediction value. If a feature importance
value is positive, it increases the prediction value.</p>
<p>By default, feature importance values are not calculated. To generate this information,
when you create a data frame analytics job you must specify the
<code class="literal">num_top_feature_importance_values</code> property. For example, see
<a class="xref" href="flightdata-regression.html" title="Predicting flight delays with regression analysis">Predicting flight delays with regression analysis</a> and <a class="xref" href="flightdata-classification.html" title="Predicting delayed flights with classification analysis">Predicting delayed flights with classification analysis</a>.</p>
<p>The feature importance values are stored in the machine learning results field for each document in
the destination index. The number of feature importance values for each document might
be less than the <code class="literal">num_top_feature_importance_values</code> property value. For example,
it returns only features that had a positive or negative effect on the
prediction.</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="ml-feature-importance-readings"></a>Further reading<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/7.10/docs/en/stack/ml/df-analytics/ml-feature-importance.asciidoc">edit</a></h3>
</div></div></div>
<p>Feature importance in the Elastic Stack is calculated using the SHAP (SHapley Additive
exPlanations) method as described in
<a href="https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf" class="ulink" target="_top">Lundberg, S. M., &amp; Lee, S.-I. A Unified Approach to Interpreting Model Predictions. In NeurIPS 2017</a>.</p>
<p>See also
<a href="/blog/feature-importance-for-data-frame-analytics-with-elastic-machine-learning" class="ulink" target="_top">Feature importance for data frame analytics with Elastic machine learning</a>.</p>
</div>

</div>
<div class="navfooter">
<span class="prev">
<a href="ml-feature-encoding.html">« Feature encoding</a>
</span>
<span class="next">
<a href="hyperparameters.html">Hyperparameter optimization »</a>
</span>
</div>
</div>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  messageStyle: "none",
  tex2jax: {
    inlineMath: [["\\(", "\\)"]],
    displayMath: [["\\[", "\\]"]],
    ignoreClass: "nostem|nolatexmath"
  },
  asciimath2jax: {
    delimiters: [["\\$", "\\$"]],
    ignoreClass: "nostem|noasciimath"
  },
  TeX: { equationNumbers: { autoNumber: "none" } }
})
MathJax.Hub.Register.StartupHook("AsciiMath Jax Ready", function () {
  MathJax.InputJax.AsciiMath.postfilterHooks.Add(function (data, node) {
    if ((node = data.script.parentNode) && (node = node.parentNode) && node.classList.contains('stemblock')) {
      data.math.root.display = "block"
    }
    return data
  })
})
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>
</body>
</html>
