<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="description" content="An introduction to machine learning data frame analytics, which enables you to analyze your data using classification, regression, and outlier detection algorithms and to generate trained models for predictions on new data.">
<meta name="keywords" content="ML, Elastic Stack, data frame analytics, overview">
<title>Outlier detection | Machine Learning in the Elastic Stack [7.17] | Elastic</title>
<meta class="elastic" name="content" content="Outlier detection | Machine Learning in the Elastic Stack [7.17]">

<link rel="home" href="index.html" title="Machine Learning in the Elastic Stack [7.17]"/>
<link rel="up" href="ml-dfa-concepts.html" title="Concepts"/>
<link rel="prev" href="ml-dfa-concepts.html" title="Concepts"/>
<link rel="next" href="dfa-regression.html" title="Regression"/>
<meta class="elastic" name="product_version" content="7.17"/>
<meta class="elastic" name="product_name" content="Machine Learning"/>
<meta class="elastic" name="website_area" content="documentation"/>
<meta name="DC.type" content="Learn/Docs/Elastic Stack/Machine Learning/7.17"/>
<meta name="DC.subject" content="Machine Learning"/>
<meta name="DC.identifier" content="7.17"/>
</head>
<body><div class="page_header">
A newer version is available. For the latest information, see the
<a href="../current/index.html">current release documentation</a>.
</div>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="index.html">Machine Learning in the Elastic Stack [7.17]</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="ml-dfanalytics.html">Data frame analytics</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="ml-dfa-concepts.html">Concepts</a></span>
</div>
<div class="navheader">
<span class="prev">
<a href="ml-dfa-concepts.html">« Concepts</a>
</span>
<span class="next">
<a href="dfa-regression.html">Regression »</a>
</span>
</div>
<div class="section xpack">
<div class="titlepage"><div><div>
<h2 class="title"><a id="dfa-outlier-detection"></a>Outlier detection<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/7.17/docs/en/stack/ml/df-analytics/dfa-outlier-detection.asciidoc">edit</a></h2>
</div></div></div>
<p>Outlier detection is an analysis for identifying data points (outliers) whose
feature values are different from those of the normal data points in a
particular data set. Outliers may denote errors or unusual behavior.</p>
<p>We use unsupervised outlier detection which means there is no need to provide a
training data set to teach outlier detection to recognize outliers. Unsupervised
outlier detection uses various machine learning techniques to find which data points
are unusual compared to the majority of the data points.</p>
<p>You can create outlier detection data frame analytics jobs in Kibana or by using the
<a href="/guide/en/elasticsearch/reference/7.17/put-dfanalytics.html" class="ulink" target="_top">create data frame analytics jobs API</a>.</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="dfa-outlier-algorithms"></a>Outlier detection algorithms<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/7.17/docs/en/stack/ml/df-analytics/dfa-outlier-detection.asciidoc">edit</a></h3>
</div></div></div>
<p>In the Elastic Stack, we use an ensemble of four different distance and density based
outlier detection methods:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
distance of K<sup>th</sup> nearest neighbor
</li>
<li class="listitem">
distance of K-nearest neighbors
</li>
<li class="listitem">
local outlier factor (<code class="literal">lof</code>)
</li>
<li class="listitem">
local distance-based outlier factor (<code class="literal">ldof</code>).
</li>
</ul>
</div>
<p>By default, you don&#8217;t need to select the methods or
provide any parameters, but you can override the default behavior if you like.
The basic assumption of the <span class="strong strong"><strong>distance based methods</strong></span> is that normal data
points – in other words, points that are not outliers – have a lot of neighbors
nearby, because we expect that in a population the majority of the data points
have similar feature values, while the minority of the data points – the
outliers – have different feature values and will, therefore, be far away from
the normal points.</p>
<p>The distance of K<sup>th</sup> nearest neighbor method (<code class="literal">distance_kth_nn</code>) computes the
distance of the data point to its K<sup>th</sup> nearest neighbor where K is a small
number and usually independent of the total number of data points. The higher
this distance the more the data point is an outlier.</p>
<p>The distance of K-nearest neighbors method (<code class="literal">distance_knn</code>) calculates the
average distance of the data points to their nearest neighbors. Points with the
largest average distance will be the most outlying.</p>
<p>While the results of the distance based methods are easy to interpret, their
drawback is that they don&#8217;t take into account the density variations of a
data set. This is the point where <span class="strong strong"><strong>density based methods</strong></span> come into the
picture, they are used for mitigating this problem. These methods take into
account not only the distance of the points to their K nearest neighbors but
also the distance of these neighbors to their neighbors.</p>
<p>Based on this approach, a metric is computed called local outlier factor
(<code class="literal">lof</code>) for each data point. The higher the local outlier factor, the more
outlying is the data point.</p>
<p>The other density based method that outlier detection uses is the local
distance-based outlier factor (<code class="literal">ldof</code>). Ldof is a ratio of two measures: the
first computes the average distance of the data point to its K nearest
neighbors; the second computes the average of the pairwise distances of the
neighbors themselves. Again, the higher the value the more the data point is an
outlier.</p>
<p>As you can see, these four algorithms work differently, so they don&#8217;t always
agree on which points are outliers. By default, we use all these methods during
outlier detection, then normalize and combine their results and give every datapoint
in the index an outlier score. The outlier score ranges from 0 to 1, where the higher
number represents the chance that the data point is an outlier compared to the
other data points in the index.</p>
<div class="important admon">
<div class="icon"></div>
<div class="admon_content">
<p>Outlier detection is a batch analysis, it runs against your data
once. If new data comes into the index, you need to do the analysis again on the
altered data.</p>
</div>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="dfa-feature-influence"></a>Feature influence<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/7.17/docs/en/stack/ml/df-analytics/dfa-outlier-detection.asciidoc">edit</a></h3>
</div></div></div>
<p>Besides the outlier score, another value is calculated during outlier detection:
the feature influence score. As we mentioned, there are multiple features of a
data point that are analyzed during outlier detection. An influential feature is a
feature of a data point that is responsible for the point being an outlier. The
value of feature influence provides a relative ranking of features by their
contribution to a point being an outlier. Therefore, while outlier score tells us
whether a data point is an outlier, feature influence shows which features make
the point an outlier. By doing this, this value provides context to help
understand more about the reasons for the data point being unusual and can drive
visualizations.</p>
</div>

</div>
<div class="navfooter">
<span class="prev">
<a href="ml-dfa-concepts.html">« Concepts</a>
</span>
<span class="next">
<a href="dfa-regression.html">Regression »</a>
</span>
</div>
</div>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  messageStyle: "none",
  tex2jax: {
    inlineMath: [["\\(", "\\)"]],
    displayMath: [["\\[", "\\]"]],
    ignoreClass: "nostem|nolatexmath"
  },
  asciimath2jax: {
    delimiters: [["\\$", "\\$"]],
    ignoreClass: "nostem|noasciimath"
  },
  TeX: { equationNumbers: { autoNumber: "none" } }
})
MathJax.Hub.Register.StartupHook("AsciiMath Jax Ready", function () {
  MathJax.InputJax.AsciiMath.postfilterHooks.Add(function (data, node) {
    if ((node = data.script.parentNode) && (node = node.parentNode) && node.classList.contains('stemblock')) {
      data.math.root.display = "block"
    }
    return data
  })
})
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>
</body>
</html>
