<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Evaluating data frame analytics | Machine Learning in the Elastic Stack [7.x] | Elastic</title>
<link rel="home" href="index.html" title="Machine Learning in the Elastic Stack [7.x]"/>
<link rel="up" href="ml-dfa-concepts.html" title="Concepts"/>
<link rel="prev" href="ml-inference.html" title="Inference"/>
<link rel="next" href="ml-feature-encoding.html" title="Feature encoding"/>
<meta name="DC.type" content="Learn/Docs/Elastic Stack/Machine learning/7.x"/>
<meta name="DC.subject" content="Machine learning"/>
<meta name="DC.identifier" content="7.x"/>
</head>
<body><div class="page_header">
You are looking at preliminary documentation for a future release.
Not what you want? See the
<a href="../current/index.html">current release documentation</a>.
</div>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="index.html">Machine Learning in the Elastic Stack [7.x]</a></span>
»
<span class="breadcrumb-link"><a href="ml-dfanalytics.html">Data frame analytics</a></span>
»
<span class="breadcrumb-link"><a href="ml-dfa-concepts.html">Concepts</a></span>
»
<span class="breadcrumb-node">Evaluating data frame analytics</span>
</div>
<div class="navheader">
<span class="prev">
<a href="ml-inference.html">« Inference</a>
</span>
<span class="next">
<a href="ml-feature-encoding.html">Feature encoding »</a>
</span>
</div>
<div class="section xpack">
<div class="titlepage"><div><div>
<h2 class="title"><a id="ml-dfanalytics-evaluate"></a>Evaluating data frame analytics<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/7.x/docs/en/stack/ml/df-analytics/ml-dfanalytics-evaluate.asciidoc">edit</a><a class="xpack_tag" href="/subscriptions"></a></h2>
</div></div></div>
<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<p>This functionality is experimental and may be changed or removed completely in a future release. Elastic will take a best effort approach to fix any issues, but experimental features are not subject to the support SLA of official GA features.</p>
</div>
</div>
<p>Using the data frame analytics features to gain insights from a data set is an
iterative process. You might need to experiment with different analyses,
parameters, and ways to transform data before you arrive at a result that satisfies
your use case. A valuable companion to this process is the
<a href="/guide/en/elasticsearch/reference/7.x/evaluate-dfanalytics.html" class="ulink" target="_top">evaluate data frame analytics API</a>, which enables you to evaluate
the data frame analytics performance against a marked up data set. It helps you
understand error distributions and identifies the points where the data frame analytics
model performs well or less trustworthily.</p>
<p>The evaluate data frame analytics API is designed for providing a general evaluation mechanism
for the different kinds of data frame analytics.</p>
<p>To evaluate the data frame analytics with this API, you need to annotate your index
that contains the results of the analysis with a field that marks each
document with the ground truth. For example, in case of outlier detection,
the field must indicate whether the given data point really is an outlier or
not. The evaluate data frame analytics API evaluates the performance of the data frame analytics against
this manually provided ground truth.</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="ml-dfanalytics-outlier-detection"></a>Outlier detection evaluation<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/7.x/docs/en/stack/ml/df-analytics/ml-dfanalytics-evaluate.asciidoc">edit</a></h3>
</div></div></div>
<p>This evaluation type is suitable for analyses which calculate a probability that
each data point in a data set is a member of a class or not. It offers the
following metrics to evaluate the model performance:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
confusion matrix
</li>
<li class="listitem">
precision
</li>
<li class="listitem">
recall
</li>
<li class="listitem">
receiver operating characteristic (ROC) curve.
</li>
</ul>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="ml-dfanalytics-confusion-matrix"></a>Confusion matrix<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/7.x/docs/en/stack/ml/df-analytics/ml-dfanalytics-evaluate.asciidoc">edit</a></h4>
</div></div></div>
<p>A confusion matrix provides four measures of how well the data frame analytics worked
on your data set:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
True positives (TP): Class members that the analysis identified as class
members.
</li>
<li class="listitem">
True negatives (TN): Not class members that the analysis identified as not
class members.
</li>
<li class="listitem">
False positives (FP): Not class members that the analysis misidentified as
class members.
</li>
<li class="listitem">
False negatives (FN): Class members that the analysis misidentified as not
class members.
</li>
</ul>
</div>
<p>Although, the evaluate data frame analytics API can compute the confusion matrix out of the
analysis results, these results are not binary values (class member/not
class member), but a number between 0 and 1 (which called the outlier score in case
of outlier detection). This value captures how likely it is for a data
point to be a member of a certain class. It means that it is up to the user who
is evaluating the results to decide what is the threshold or cutoff point at
which the data point will be considered as a member of the given class. For
example, in the case of outlier detection the user can say that all the data points
with an outlier score higher than 0.5 will be considered as outliers.</p>
<p>To take this complexity into account, the evaluate data frame analytics API returns the confusion
matrix at different thresholds (by default, 0.25, 0.5, and 0.75).</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="ml-dfanalytics-precision-recall"></a>Precision and recall<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/7.x/docs/en/stack/ml/df-analytics/ml-dfanalytics-evaluate.asciidoc">edit</a></h4>
</div></div></div>
<p>A confusion matrix is a useful measure, but it could be hard to compare the
results across the different algorithms. Precision and recall values
summarize the algorithm performance as a single number that makes it easier to
compare the evaluation results.</p>
<p>Precision shows how many of the data points that the algorithm identified as
class members were actually class members. It is the number of true positives
divided by the sum of the true positives and false positives (TP/(TP+FP)).</p>
<p>Recall answers a slightly different question. This value shows how many of the
data points that are actual class members were identified correctly as class
members. It is the number of true positives divided by the sum of the true
positives and false negatives (TP/(TP+FN)).</p>
<p>As was the case for the confusion matrix, you also need to define different
threshold levels for computing precision and recall.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="ml-dfanalytics-roc"></a>Receiver operating characteristic curve<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/7.x/docs/en/stack/ml/df-analytics/ml-dfanalytics-evaluate.asciidoc">edit</a></h4>
</div></div></div>
<p>The receiver operating characteristic (ROC) curve is a plot that represents the
performance of the binary classification process at different thresholds. It
compares the rate of true positives against the rate of false positives at the
different threshold levels to create the curve. From this plot, you can compute
the area under the curve (AUC) value, which is a number between 0 and 1. The
closer to 1, the better the algorithm performance.</p>
<p>The evaluate data frame analytics API can return the false positive rate (<code class="literal">fpr</code>) and the true
positive rate (<code class="literal">tpr</code>) at the different threshold levels, so you can visualize
the algorithm performance by using these values.</p>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="ml-dfanalytics-regression-evaluation"></a>Regression evaluation<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/7.x/docs/en/stack/ml/df-analytics/ml-dfanalytics-evaluate.asciidoc">edit</a></h3>
</div></div></div>
<p>This evaluation type is suitable for evaluating regression models. The
regression evaluation type offers the following metrics to evaluate the model
performance:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Mean squared error (MSE)
</li>
<li class="listitem">
R-squared (R<sup>2</sup>)
</li>
<li class="listitem">
Pseudo-Huber loss
</li>
<li class="listitem">
Mean squared logarithmic error (MSLE)
</li>
</ul>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="ml-dfanalytics-mse"></a>Mean squared error<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/7.x/docs/en/stack/ml/df-analytics/ml-dfanalytics-evaluate.asciidoc">edit</a></h4>
</div></div></div>
<p>The API provides a MSE by computing the average squared sum of the difference
between the true value and the value that the regression model predicted.
(Avg (predicted value-actual value)<sup>2</sup>). You can use the MSE to measure how well
the regression analysis model is performing.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="ml-dfanalytics-r-sqared"></a>R-squared<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/7.x/docs/en/stack/ml/df-analytics/ml-dfanalytics-evaluate.asciidoc">edit</a></h4>
</div></div></div>
<p>Another evaluation metric for regression analysis is R-squared (R<sup>2</sup>). It represents
the goodness of fit and measures how much of the variation in the data the
predictions are able to explain. The value of R<sup>2</sup> are less than or equal to 1,
where 1 indicates that the predictions and true values are equal. A value of 0
is obtained when all the predictions are set to the mean of the true values. A
value of 0.5 for R<sup>2</sup> would indicate that, the predictions are 1 - 0.5<sup>(1/2)</sup>
(about 30%) closer to true values than their mean.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="ml-dfanalytics-huber"></a>Pseudo-Huber loss<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/7.x/docs/en/stack/ml/df-analytics/ml-dfanalytics-evaluate.asciidoc">edit</a></h4>
</div></div></div>
<p><a href="https://en.wikipedia.org/wiki/Huber_loss#Pseudo-Huber_loss_function" class="ulink" target="_top">Pseudo-Huber loss metric</a>
behaves as mean absolute error (MAE) for errors larger than a predefined value
(defaults to <code class="literal">1</code>) and as mean squared error (MSE) for errors smaller than the
predefined value. This loss function uses the <code class="literal">delta</code> parameter to define the
transition point between MAE and MSE. Consult the
<a class="xref" href="dfa-regression.html#dfa-regression-lossfunction" title="Loss functions for regression analyses">Loss functions for regression analyses</a> page to learn more about loss functions.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="ml-dfanalytics-msle"></a>Mean squared logarithmic error<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/7.x/docs/en/stack/ml/df-analytics/ml-dfanalytics-evaluate.asciidoc">edit</a></h4>
</div></div></div>
<p>This evaluation metric is a variation of mean squared error. It can be used for
cases when the target values are positive and distributed with a long tail such
as data on prices or population. Consult the <a class="xref" href="dfa-regression.html#dfa-regression-lossfunction" title="Loss functions for regression analyses">Loss functions for regression analyses</a>
page to learn more about loss functions.</p>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title"><a id="ml-dfanalytics-classification"></a>Classification evaluation<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/7.x/docs/en/stack/ml/df-analytics/ml-dfanalytics-evaluate.asciidoc">edit</a></h3>
</div></div></div>
<p>This evaluation type is suitable for evaluating classification models. The
classification evaluation offers the following metrics to evaluate the model
performance:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Multiclass confusion matrix
</li>
<li class="listitem">
Area under the curve of receiver operating characteristic (AUC ROC)
</li>
</ul>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="ml-dfanalytics-mccm"></a>Multiclass confusion matrix<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/7.x/docs/en/stack/ml/df-analytics/ml-dfanalytics-evaluate.asciidoc">edit</a></h4>
</div></div></div>
<p>The multiclass confusion matrix provides a summary of the performance of the
classification analysis. It contains the number of occurrences where the analysis
classified data points correctly with their actual class as well as the number
of occurrences where it misclassified them.</p>
<p>Let&#8217;s see two examples of the confusion matrix. The first is a confusion matrix
of a binary problem:</p>
<div class="imageblock text-center">
<div class="content">
<img src="images/confusion-matrix-binary.jpg" alt="Confusion matrix of a binary problem" width="75%">
</div>
</div>
<p>It is a two by two matrix because there are only two classes (<code class="literal">true</code> and
<code class="literal">false</code>). The matrix shows the proportion of data points that is correctly
identified as members of a each class and the proportion that is
misidentified.</p>
<p>As the number of classes in your classification analysis increases, the confusion
matrix also increases in complexity:</p>
<div class="imageblock text-center">
<div class="content">
<img src="images/confusion-matrix-multiclass.jpg" alt="Confusion matrix of a multiclass problem" width="100%">
</div>
</div>
<p>The matrix contains the actual labels on the left side while the predicted
labels are on the top. The proportion of correct and incorrect predictions is
broken down for each class. This enables you to examine how the classification analysis
confused the different classes while it made its predictions.</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title"><a id="ml-dfanalytics-class-aucroc"></a>Area under the curve of receiver operating characteristic (AUC ROC)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/7.x/docs/en/stack/ml/df-analytics/ml-dfanalytics-evaluate.asciidoc">edit</a></h4>
</div></div></div>
<p>The receiver operating characteristic (ROC) curve is a plot that represents the
performance of the classification process at different predicted probability
thresholds. It compares the true positive rate for a specific class against the
rate of all the other classes combined ("one versus all" strategy) at the
different threshold levels to create the curve.</p>
<p>Let&#8217;s see an example. You have three classes: <code class="literal">A</code>, <code class="literal">B</code>, and <code class="literal">C</code>, you want to
calculate AUC ROC for <code class="literal">A</code>. In this case, the number of correctly classified
<code class="literal">A</code>s (true positives) are compared to the number of <code class="literal">B</code>s and <code class="literal">C</code>s that are
misclassified as <code class="literal">A</code>s (false positives).</p>
<p>From this plot, you can compute the area under the curve (AUC) value, which is a
number between 0 and 1. The higher the AUC, the better the model is at
predicting <code class="literal">A</code>s as <code class="literal">A</code>s, in this case.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>To use this evaluation method, you must set <code class="literal">num_top_classes</code> to <code class="literal">-1</code>
or a value greater than or equal to the total number of classes when you create
the data frame analytics job.</p>
</div>
</div>
</div>

</div>

</div>
<div class="navfooter">
<span class="prev">
<a href="ml-inference.html">« Inference</a>
</span>
<span class="next">
<a href="ml-feature-encoding.html">Feature encoding »</a>
</span>
</div>
</div>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  messageStyle: "none",
  tex2jax: {
    inlineMath: [["\\(", "\\)"]],
    displayMath: [["\\[", "\\]"]],
    ignoreClass: "nostem|nolatexmath"
  },
  asciimath2jax: {
    delimiters: [["\\$", "\\$"]],
    ignoreClass: "nostem|noasciimath"
  },
  TeX: { equationNumbers: { autoNumber: "none" } }
})
MathJax.Hub.Register.StartupHook("AsciiMath Jax Ready", function () {
  MathJax.InputJax.AsciiMath.postfilterHooks.Add(function (data, node) {
    if ((node = data.script.parentNode) && (node = node.parentNode) && node.classList.contains('stemblock')) {
      data.math.root.display = "block"
    }
    return data
  })
})
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>
</body>
</html>
