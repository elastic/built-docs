<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="keywords" content="ML, Elastic Stack, natural language processing, text embedding">
<title>Deploy the model in your cluster | Machine Learning in the Elastic Stack [8.14] | Elastic</title>
<meta class="elastic" name="content" content="Deploy the model in your cluster | Machine Learning in the Elastic Stack [8.14]">

<link rel="home" href="index.html" title="Machine Learning in the Elastic Stack [8.14]"/>
<link rel="up" href="ml-nlp-deploy-models.html" title="Deploy trained models"/>
<link rel="prev" href="ml-nlp-import-model.html" title="Import the trained model and vocabulary"/>
<link rel="next" href="ml-nlp-test-inference.html" title="Try it out"/>
<meta class="elastic" name="product_version" content="8.14"/>
<meta class="elastic" name="product_name" content="Machine Learning"/>
<meta class="elastic" name="website_area" content="documentation"/>
<meta name="DC.type" content="Learn/Docs/Elastic Stack/Machine Learning/8.14"/>
<meta name="DC.subject" content="Machine Learning"/>
<meta name="DC.identifier" content="8.14"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body>
<div class="page_header">
<strong>IMPORTANT</strong>: No additional bug fixes or documentation updates
will be released for this version. For the latest information, see the
<a href="../current/index.html">current release documentation</a>.
</div>
<div class="navheader">
<span class="prev">
<a href="ml-nlp-import-model.html">« Import the trained model and vocabulary</a>
</span>
<span class="next">
<a href="ml-nlp-test-inference.html">Try it out »</a>
</span>
</div>
<div class="book" lang="en">
<div class="titlepage">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="index.html">Machine Learning in the Elastic Stack [8.14]</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="ml-nlp.html">Natural language processing</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="ml-nlp-deploy-models.html">Deploy trained models</a></span>
</div>
<div>
<div><h1 class="title"><a id="id-1"></a>Deploy the model in your cluster</h1><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/stack-docs/edit/8.14/docs/en/stack/ml/nlp/ml-nlp-deploy-models.asciidoc">edit</a></div>
</div>
<!--EXTRA-->
</div>
<div id="content">
<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="ml-nlp-deploy-model"></a>Deploy the model in your cluster<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/stack-docs/edit/8.14/docs/en/stack/ml/nlp/ml-nlp-deploy-models.asciidoc">edit</a></h2>
</div></div></div>
<p>After you import the model and vocabulary, you can use Kibana to view and manage
their deployment across your cluster under <span class="strong strong"><strong>Machine Learning</strong></span> &gt; <span class="strong strong"><strong>Model Management</strong></span>.
Alternatively, you can use the
<a href="/guide/en/elasticsearch/reference/8.14/start-trained-model-deployment.html" class="ulink" target="_top">start trained model deployment API</a>.</p>
<p>You can deploy a model multiple times by assigning a unique deployment ID when
starting the deployment. It enables you to have dedicated deployments for
different purposes, such as search and ingest. By doing so, you ensure that the
search speed remains unaffected by ingest workloads, and vice versa. Having
separate deployments for search and ingest mitigates performance issues
resulting from interactions between the two, which can be hard to diagnose.</p>
<div class="imageblock screenshot">
<div class="content">
<img src="images/ml-nlp-deployment-id-elser-v2.png" alt="Model deployment on the Trained Models UI.">
</div>
</div>
<p>It is recommended to fine-tune each deployment based on its specific purpose. To
improve ingest performance, increase throughput by adding more allocations to
the deployment. For improved search speed, increase the number of threads per
allocation.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>Since eland uses APIs to deploy the models, you cannot see the models in
Kibana until the saved objects are synchronized. You can follow the prompts in
Kibana, wait for automatic synchronization, or use the
<a href="/guide/en/kibana/8.14/machine-learning-api-sync.html" class="ulink" target="_top">sync machine learning saved objects API</a>.</p>
</div>
</div>
<p>When you deploy the model, its allocations are distributed across available machine learning
nodes. Model allocations are independent units of work for NLP tasks. To
influence model performance, you can configure the number of allocations and the
number of threads used by each allocation of your deployment.</p>
<div class="important admon">
<div class="icon"></div>
<div class="admon_content">
<p>If your deployed trained model has only one allocation, it&#8217;s likely
that you will experience downtime in the service your trained model performs.
You can reduce or eliminate downtime by adding more allocations to your trained
models.</p>
</div>
</div>
<p>Throughput can be scaled by adding more allocations to the deployment; it
increases the number of inference requests that can be performed in parallel. All
allocations assigned to a node share the same copy of the model in memory. The
model is loaded into memory in a native process that encapsulates <code class="literal">libtorch</code>,
which is the underlying machine learning library of PyTorch. The number of allocations
setting affects the amount of model allocations across all the machine learning nodes. Model
allocations are distributed in such a way that the total number of used threads
does not exceed the allocated processors of a node.</p>
<p>The threads per allocation setting affects the number of threads used by each
model allocation during inference. Increasing the number of threads generally
increases the speed of inference requests. The value of this setting must not
exceed the number of available allocated processors per node.</p>
<p>You can view the allocation status in Kibana or by using the
<a href="/guide/en/elasticsearch/reference/8.14/get-trained-models-stats.html" class="ulink" target="_top">get trained model stats API</a>. If you want to
change the number of allocations, you can use the
<a href="/guide/en/elasticsearch/reference/8.14/update-trained-model-deployment.html" class="ulink" target="_top">update trained model stats API</a> after
the allocation status is <code class="literal">started</code>.</p>
<h4><a id="infer-request-queues"></a>Request queues and search priority<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/stack-docs/edit/8.14/docs/en/stack/ml/nlp/ml-nlp-deploy-models.asciidoc">edit</a></h4>
<p>Each allocation of a model deployment has a dedicated queue to buffer inference
requests. The size of this queue is determined by the <code class="literal">queue_capacity</code> parameter
in the
<a href="/guide/en/elasticsearch/reference/8.14/start-trained-model-deployment.html" class="ulink" target="_top">start trained model deployment API</a>.
When the queue reaches its maximum capacity, new requests are declined until
some of the queued requests are processed, creating available capacity once
again. When multiple ingest pipelines reference the same deployment, the queue
can fill up, resulting in rejected requests. Consider using dedicated
deployments to prevent this situation.</p>
<p>Inference requests originating from search, such as the
<a href="/guide/en/elasticsearch/reference/8.14/query-dsl-text-expansion-query.html" class="ulink" target="_top"><code class="literal">text_expansion</code> query</a>, have a higher
priority compared to non-search requests. The inference ingest processor generates
normal priority requests. If both a search query and an ingest processor use the
same deployment, the search requests with higher priority skip ahead in the
queue for processing before the lower priority ingest requests. This
prioritization accelerates search responses while potentially slowing down
ingest where response time is less critical.</p>
</div>
</div>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  messageStyle: "none",
  tex2jax: {
    inlineMath: [["\\(", "\\)"]],
    displayMath: [["\\[", "\\]"]],
    ignoreClass: "nostem|nolatexmath"
  },
  asciimath2jax: {
    delimiters: [["\\$", "\\$"]],
    ignoreClass: "nostem|noasciimath"
  },
  TeX: { equationNumbers: { autoNumber: "none" } }
})
MathJax.Hub.Register.StartupHook("AsciiMath Jax Ready", function () {
  MathJax.InputJax.AsciiMath.postfilterHooks.Add(function (data, node) {
    if ((node = data.script.parentNode) && (node = node.parentNode) && node.classList.contains('stemblock')) {
      data.math.root.display = "block"
    }
    return data
  })
})
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>
</div><div class="navfooter">
<span class="prev">
<a href="ml-nlp-import-model.html">« Import the trained model and vocabulary</a>
</span>
<span class="next">
<a href="ml-nlp-test-inference.html">Try it out »</a>
</span>
</div>
</body>
</html>
