<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Trained model autoscaling | Machine Learning in the Elastic Stack [8.17] | Elastic</title>
<meta class="elastic" name="content" content="Trained model autoscaling | Machine Learning in the Elastic Stack [8.17]">

<link rel="home" href="index.html" title="Machine Learning in the Elastic Stack [8.17]"/>
<link rel="up" href="ml-nlp.html" title="Natural language processing"/>
<link rel="prev" href="ml-nlp-test-inference.html" title="Try it out"/>
<link rel="next" href="ml-nlp-inference.html" title="Add NLP inference to ingest pipelines"/>
<meta class="elastic" name="product_version" content="8.17"/>
<meta class="elastic" name="product_name" content="Machine Learning"/>
<meta class="elastic" name="website_area" content="documentation"/>
<meta name="DC.type" content="Learn/Docs/Elastic Stack/Machine Learning/8.17"/>
<meta name="DC.subject" content="Machine Learning"/>
<meta name="DC.identifier" content="8.17"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body>
<div class="page_header">
This documentation contains work-in-progress information for future Elastic Stack and Cloud releases. Use the version selector to view supported release docs. It also contains some Elastic Cloud serverless information. Check out our <a href="https://www.elastic.co/docs/current/serverless">serverless docs</a> for more details.
</div>
<div class="navheader">
<span class="prev">
<a href="ml-nlp-test-inference.html">« Try it out</a>
</span>
<span class="next">
<a href="ml-nlp-inference.html">Add NLP inference to ingest pipelines »</a>
</span>
</div>
<div class="book" lang="en">
<div class="titlepage">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="index.html">Machine Learning in the Elastic Stack [8.17]</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="ml-nlp.html">Natural language processing</a></span>
</div>
<div>
<div><h1 class="title"><a id="id-1"></a>Trained model autoscaling</h1><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/stack-docs/edit/8.17/docs/en/stack/ml/nlp/ml-nlp-autoscaling.asciidoc">edit</a></div>
</div>
<!--EXTRA-->
</div>
<div id="content">
<div class="chapter">
<div class="titlepage"><div><div>
<div class="position-relative"><h2 class="title"><a id="ml-nlp-auto-scale"></a>Trained model autoscaling</h2><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/stack-docs/edit/8.17/docs/en/stack/ml/nlp/ml-nlp-autoscaling.asciidoc">edit</a></div>
</div></div></div>
<p>You can enable autoscaling for each of your trained model deployments.
Autoscaling allows Elasticsearch to automatically adjust the resources the model deployment can use based on the workload demand.</p>
<p>There are two ways to enable autoscaling:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
through APIs by enabling adaptive allocations
</li>
<li class="listitem">
in Kibana by enabling adaptive resources
</li>
</ul>
</div>
<div class="important admon">
<div class="icon"></div>
<div class="admon_content">
<p>To fully leverage model autoscaling, it is highly recommended to enable <a href="/guide/en/cloud/current/ec-autoscaling.html" class="ulink" target="_top">Elasticsearch deployment autoscaling</a>.</p>
</div>
</div>
<div class="position-relative"><h3><a id="nlp-model-adaptive-allocations"></a>Enabling autoscaling through APIs - adaptive allocations</h3><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/stack-docs/edit/8.17/docs/en/stack/ml/nlp/ml-nlp-autoscaling.asciidoc">edit</a></div>
<p>Model allocations are independent units of work for NLP tasks.
If you set the numbers of threads and allocations for a model manually, they remain constant even when not all the available resources are fully used or when the load on the model requires more resources.
Instead of setting the number of allocations manually, you can enable adaptive allocations to set the number of allocations based on the load on the process.
This can help you to manage performance and cost more easily.
(Refer to the <a href="https://cloud.elastic.co/pricing" class="ulink" target="_top">pricing calculator</a> to learn more about the possible costs.)</p>
<p>When adaptive allocations are enabled, the number of allocations of the model is set automatically based on the current load.
When the load is high, a new model allocation is automatically created.
When the load is low, a model allocation is automatically removed.
You can explicitely set the minimum and maximum number of allocations; autoscaling will occur within these limits.</p>
<p>You can enable adaptive allocations by using:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
the create inference endpoint API for <a href="/guide/en/elasticsearch/reference/8.17/infer-service-elser.html" class="ulink" target="_top">ELSER</a>, <a href="/guide/en/elasticsearch/reference/8.17/infer-service-elasticsearch.html" class="ulink" target="_top">E5 and models uploaded through Eland</a> that are used as inference services.
</li>
<li class="listitem">
the <a href="/guide/en/elasticsearch/reference/8.17/start-trained-model-deployment.html" class="ulink" target="_top">start trained model deployment</a> or <a href="/guide/en/elasticsearch/reference/8.17/update-trained-model-deployment.html" class="ulink" target="_top">update trained model deployment</a> APIs for trained models that are deployed on machine learning nodes.
</li>
</ul>
</div>
<p>If the new allocations fit on the current machine learning nodes, they are immediately started.
If more resource capacity is needed for creating new model allocations, then your machine learning node will be scaled up if machine learning autoscaling is enabled to provide enough resources for the new allocation.
The number of model allocations can be scaled down to 0.
They cannot be scaled up to more than 32 allocations, unless you explicitly set the maximum number of allocations to more.
Adaptive allocations must be set up independently for each deployment and <a href="/guide/en/elasticsearch/reference/8.17/put-inference-api.html" class="ulink" target="_top">inference endpoint</a>.</p>
<div class="position-relative"><h4><a id="optimize-use-case"></a>Optimizing for typical use cases</h4><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/stack-docs/edit/8.17/docs/en/stack/ml/nlp/ml-nlp-autoscaling.asciidoc">edit</a></div>
<p>You can optimize your model deployment for typical use cases, such as search and ingest.
When you optimize for ingest, the throughput will be higher, which increases the number of inference requests that can be performed in parallel.
When you optimize for search, the latency will be lower during search processes.</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
If you want to optimize for ingest, set the number of threads to <code class="literal">1</code> (<code class="literal">"threads_per_allocation": 1</code>).
</li>
<li class="listitem">
If you want to optimize for search, set the number of threads to greater than <code class="literal">1</code>.
Increasing the number of threads will make the search processes more performant.
</li>
</ul>
</div>
<div class="position-relative"><h3><a id="nlp-model-adaptive-resources"></a>Enabling autoscaling in Kibana - adaptive resources</h3><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/stack-docs/edit/8.17/docs/en/stack/ml/nlp/ml-nlp-autoscaling.asciidoc">edit</a></div>
<p>You can enable adaptive resources for your models when starting or updating the model deployment.
Adaptive resources make it possible for Elasticsearch to scale up or down the available resources based on the load on the process.
This can help you to manage performance and cost more easily.
When adaptive resources are enabled, the number of vCPUs that the model deployment uses is set automatically based on the current load.
When the load is high, the number of vCPUs that the process can use is automatically increased.
When the load is low, the number of vCPUs that the process can use is automatically decreased.</p>
<p>You can choose from three levels of resource usage for your trained model deployment; autoscaling will occur within the selected level&#8217;s range.</p>
<p>Refer to the tables in the <a class="xref" href="ml-nlp-auto-scale.html#auto-scaling-matrix" title="Model deployment resource matrix">Model deployment resource matrix</a> section to find out the setings for the level you selected.</p>
<div class="imageblock screenshot">
<div class="content">
<img src="images/ml-nlp-deployment-id-elser-v2.png" alt="ELSER deployment with adaptive resources enabled." width="640">
</div>
</div>
<div class="position-relative"><h3><a id="auto-scaling-matrix"></a>Model deployment resource matrix</h3><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/stack-docs/edit/8.17/docs/en/stack/ml/nlp/ml-nlp-autoscaling.asciidoc">edit</a></div>
<p>The used resources for trained model deployments depend on three factors:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
your cluster environment (Serverless, Cloud, or on-premises)
</li>
<li class="listitem">
the use case you optimize the model deployment for (ingest or search)
</li>
<li class="listitem">
whether model autoscaling is enabled with adaptive allocations/resources to have dynamic resources, or disabled for static resources
</li>
</ul>
</div>
<p>If you use Elasticsearch on-premises, vCPUs level ranges are derived from the <code class="literal">total_ml_processors</code> and <code class="literal">max_single_ml_node_processors</code> values.
Use the <a href="/guide/en/elasticsearch/reference/8.17/get-ml-info.html" class="ulink" target="_top">get machine learning info API</a> to check these values.
The following tables show you the number of allocations, threads, and vCPUs available in Cloud when adaptive resources are enabled or disabled.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>On Serverless, adaptive allocations are automatically enabled for all project types.
However, the "Adaptive resources" control is not displayed in Kibana for Observability and Security projects.</p>
</div>
</div>
<div class="position-relative"><h4><a id="_deployments_in_cloud_optimized_for_ingest"></a>Deployments in Cloud optimized for ingest</h4><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/stack-docs/edit/8.17/docs/en/stack/ml/nlp/ml-nlp-autoscaling.asciidoc">edit</a></div>
<p>In case of ingest-optimized deployments, we maximize the number of model allocations.</p>
<div class="position-relative"><h5><a id="_adaptive_resources_enabled"></a>Adaptive resources enabled</h5><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/stack-docs/edit/8.17/docs/en/stack/ml/nlp/ml-nlp-autoscaling.asciidoc">edit</a></div>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
<col class="col_4"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Level</th>
<th align="left" valign="top">Allocations</th>
<th align="left" valign="top">Threads</th>
<th align="left" valign="top">vCPUs</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p>Low</p></td>
<td align="left" valign="top"><p>0 to 2 if available, dynamically</p></td>
<td align="left" valign="top"><p>1</p></td>
<td align="left" valign="top"><p>0 to 2 if available, dynamically</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Medium</p></td>
<td align="left" valign="top"><p>1 to 32 dynamically</p></td>
<td align="left" valign="top"><p>1</p></td>
<td align="left" valign="top"><p>1 to the smaller of 32 or the limit set in the Cloud console, dynamically</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>High</p></td>
<td align="left" valign="top"><p>1 to limit set in the Cloud console <sup>*</sup>, dynamically</p></td>
<td align="left" valign="top"><p>1</p></td>
<td align="left" valign="top"><p>1 to limit set in the Cloud console, dynamically</p></td>
</tr>
</tbody>
</table>
</div>
<p><sup>*</sup> The Cloud console doesn&#8217;t directly set an allocations limit; it only sets a vCPU limit.
This vCPU limit indirectly determines the number of allocations, calculated as the vCPU limit divided by the number of threads.</p>
<div class="position-relative"><h5><a id="_adaptive_resources_disabled"></a>Adaptive resources disabled</h5><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/stack-docs/edit/8.17/docs/en/stack/ml/nlp/ml-nlp-autoscaling.asciidoc">edit</a></div>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
<col class="col_4"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Level</th>
<th align="left" valign="top">Allocations</th>
<th align="left" valign="top">Threads</th>
<th align="left" valign="top">vCPUs</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p>Low</p></td>
<td align="left" valign="top"><p>2 if available, otherwise 1, statically</p></td>
<td align="left" valign="top"><p>1</p></td>
<td align="left" valign="top"><p>2 if available</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Medium</p></td>
<td align="left" valign="top"><p>the smaller of 32 or the limit set in the Cloud console, statically</p></td>
<td align="left" valign="top"><p>1</p></td>
<td align="left" valign="top"><p>32 if available</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>High</p></td>
<td align="left" valign="top"><p>Maximum available set in the  Cloud console <sup>*</sup>, statically</p></td>
<td align="left" valign="top"><p>1</p></td>
<td align="left" valign="top"><p>Maximum available set in the Cloud console, statically</p></td>
</tr>
</tbody>
</table>
</div>
<p><sup>*</sup> The Cloud console doesn&#8217;t directly set an allocations limit; it only sets a vCPU limit.
This vCPU limit indirectly determines the number of allocations, calculated as the vCPU limit divided by the number of threads.</p>
<div class="position-relative"><h4><a id="_deployments_in_cloud_optimized_for_search"></a>Deployments in Cloud optimized for search</h4><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/stack-docs/edit/8.17/docs/en/stack/ml/nlp/ml-nlp-autoscaling.asciidoc">edit</a></div>
<p>In case of search-optimized deployments, we maximize the number of threads.
The maximum number of threads that can be claimed depends on the hardware your architecture has.</p>
<div class="position-relative"><h5><a id="_adaptive_resources_enabled_2"></a>Adaptive resources enabled</h5><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/stack-docs/edit/8.17/docs/en/stack/ml/nlp/ml-nlp-autoscaling.asciidoc">edit</a></div>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
<col class="col_4"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Level</th>
<th align="left" valign="top">Allocations</th>
<th align="left" valign="top">Threads</th>
<th align="left" valign="top">vCPUs</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p>Low</p></td>
<td align="left" valign="top"><p>1</p></td>
<td align="left" valign="top"><p>2</p></td>
<td align="left" valign="top"><p>2</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Medium</p></td>
<td align="left" valign="top"><p>1 to 2 (if threads=16) dynamically</p></td>
<td align="left" valign="top"><p>maximum that the hardware allows (for example, 16)</p></td>
<td align="left" valign="top"><p>1 to 32 dynamically</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>High</p></td>
<td align="left" valign="top"><p>1 to limit set in the Cloud console <sup>*</sup>, dynamically</p></td>
<td align="left" valign="top"><p>maximum that the hardware allows (for example, 16)</p></td>
<td align="left" valign="top"><p>1 to limit set in the Cloud console, dynamically</p></td>
</tr>
</tbody>
</table>
</div>
<p><sup>*</sup> The Cloud console doesn&#8217;t directly set an allocations limit; it only sets a vCPU limit.
This vCPU limit indirectly determines the number of allocations, calculated as the vCPU limit divided by the number of threads.</p>
<div class="position-relative"><h5><a id="_adaptive_resources_disabled_2"></a>Adaptive resources disabled</h5><a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/stack-docs/edit/8.17/docs/en/stack/ml/nlp/ml-nlp-autoscaling.asciidoc">edit</a></div>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
<col class="col_4"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Level</th>
<th align="left" valign="top">Allocations</th>
<th align="left" valign="top">Threads</th>
<th align="left" valign="top">vCPUs</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p>Low</p></td>
<td align="left" valign="top"><p>1 if available, statically</p></td>
<td align="left" valign="top"><p>2</p></td>
<td align="left" valign="top"><p>2 if available</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Medium</p></td>
<td align="left" valign="top"><p>2 (if threads=16) statically</p></td>
<td align="left" valign="top"><p>maximum that the hardware allows (for example, 16)</p></td>
<td align="left" valign="top"><p>32 if available</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>High</p></td>
<td align="left" valign="top"><p>Maximum available set in the Cloud console <sup>*</sup>, statically</p></td>
<td align="left" valign="top"><p>maximum that the hardware allows (for example, 16)</p></td>
<td align="left" valign="top"><p>Maximum available set in the Cloud console, statically</p></td>
</tr>
</tbody>
</table>
</div>
<p><sup>*</sup> The Cloud console doesn&#8217;t directly set an allocations limit; it only sets a vCPU limit.
This vCPU limit indirectly determines the number of allocations, calculated as the vCPU limit divided by the number of threads.</p>
</div>
</div>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  messageStyle: "none",
  tex2jax: {
    inlineMath: [["\\(", "\\)"]],
    displayMath: [["\\[", "\\]"]],
    ignoreClass: "nostem|nolatexmath"
  },
  asciimath2jax: {
    delimiters: [["\\$", "\\$"]],
    ignoreClass: "nostem|noasciimath"
  },
  TeX: { equationNumbers: { autoNumber: "none" } }
})
MathJax.Hub.Register.StartupHook("AsciiMath Jax Ready", function () {
  MathJax.InputJax.AsciiMath.postfilterHooks.Add(function (data, node) {
    if ((node = data.script.parentNode) && (node = node.parentNode) && node.classList.contains('stemblock')) {
      data.math.root.display = "block"
    }
    return data
  })
})
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>
</div><div class="navfooter">
<span class="prev">
<a href="ml-nlp-test-inference.html">« Try it out</a>
</span>
<span class="next">
<a href="ml-nlp-inference.html">Add NLP inference to ingest pipelines »</a>
</span>
</div>
</body>
</html>
