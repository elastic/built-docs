<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="keywords" content="ML, Elastic Stack, natural language processing, inference">
<title>ELSER – Elastic Learned Sparse EncodeR | Machine Learning in the Elastic Stack [8.8] | Elastic</title>
<meta class="elastic" name="content" content="ELSER – Elastic Learned Sparse EncodeR | Machine Learning in the Elastic Stack [8.8]">

<link rel="home" href="index.html" title="Machine Learning in the Elastic Stack [8.8]"/>
<link rel="up" href="ml-nlp.html" title="Natural language processing"/>
<link rel="prev" href="ml-nlp-apis.html" title="API quick reference"/>
<link rel="next" href="ml-nlp-model-ref.html" title="Compatible third party NLP models"/>
<meta class="elastic" name="product_version" content="8.8"/>
<meta class="elastic" name="product_name" content="Machine Learning"/>
<meta class="elastic" name="website_area" content="documentation"/>
<meta name="DC.type" content="Learn/Docs/Elastic Stack/Machine Learning/8.8"/>
<meta name="DC.subject" content="Machine Learning"/>
<meta name="DC.identifier" content="8.8"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body><div class="page_header">
<strong>IMPORTANT</strong>: No additional bug fixes or documentation updates
will be released for this version. For the latest information, see the
<a href="../current/index.html">current release documentation</a>.
</div>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="index.html">Machine Learning in the Elastic Stack [8.8]</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="ml-nlp.html">Natural language processing</a></span>
</div>
<div class="navheader">
<span class="prev">
<a href="ml-nlp-apis.html">« API quick reference</a>
</span>
<span class="next">
<a href="ml-nlp-model-ref.html">Compatible third party NLP models »</a>
</span>
</div>
<div class="chapter">
<div class="titlepage"><div><div>
<h2 class="title"><a id="ml-nlp-elser"></a>ELSER – Elastic Learned Sparse EncodeR<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/stack-docs/edit/8.8/docs/en/stack/ml/nlp/ml-nlp-elser.asciidoc">edit</a></h2>
</div></div></div>

<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<p>This functionality is in technical preview and may be changed or removed in a future release. Elastic will apply best effort to fix any issues, but features in technical preview are not subject to the support SLA of official GA features.</p>
</div>
</div>
<p>Elastic Learned Sparse EncodeR - or ELSER - is a retrieval model trained by
Elastic that enables you to perform
<a href="/guide/en/elasticsearch/reference/8.8/semantic-search-elser.html" class="ulink" target="_top">semantic search</a> to retrieve more relevant
search results. This search type provides you search results based on contextual
meaning and user intent, rather than exact keyword matches.</p>
<p>ELSER is an out-of-domain model which means it does not require fine-tuning on
your own data, making it adaptable for various use cases out of the box.</p>
<h3><a id="elser-tokens"></a>Tokens - not synonyms<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/stack-docs/edit/8.8/docs/en/stack/ml/nlp/ml-nlp-elser.asciidoc">edit</a></h3>
<p>ELSER expands the indexed and searched passages into collections of terms that
are learned to co-occur frequently within a diverse set of training data. The
terms that the text is expanded into by the model <em>are not</em> synonyms for the
search terms; they are learned associations capturing relevance. These expanded
terms are weighted as some of them are more significant than others. Then the
Elasticsearch <a href="/guide/en/elasticsearch/reference/8.8/rank-features.html" class="ulink" target="_top">rank features</a> field type is used to store the
terms and weights at index time, and to search against later.</p>
<p>This approach provides a more understandable search experience compared to
vector embeddings. However, attempting to directly interpret the tokens and
weights can be misleading, as the expansion essentially results in a vector in a
very high-dimensional space. Consequently, certain tokens, especially those with
low weight, contain information that is intertwined with other low-weight tokens
in the representation. In this regard, they function similarly to a dense vector
representation, making it challenging to separate their individual
contributions. This complexity can potentially lead to misinterpretations if not
carefully considered during analysis.</p>
<h3><a id="elser-req"></a>Requirements<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/stack-docs/edit/8.8/docs/en/stack/ml/nlp/ml-nlp-elser.asciidoc">edit</a></h3>
<p>To use ELSER, you must have the <a href="/subscriptions" class="ulink" target="_top">appropriate subscription</a> level
for semantic search or the trial period activated.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>The minimum dedicated ML node size for deploying and using the ELSER model
is 4 GB in Elasticsearch Service if
<a href="/guide/en/cloud/current/ec-autoscaling.html" class="ulink" target="_top">deployment autoscaling</a> is turned off. Turning on
autoscaling is recommended because it allows your deployment to dynamically
adjust resources based on demand. Better performance can be achieved by using
more allocations or more threads per allocation, which requires bigger ML nodes.
Autoscaling provides bigger nodes when required. If autoscaling is turned off,
you must provide suitably sized nodes yourself.</p>
</div>
</div>
<h3><a id="elser-benchamrks"></a>Benchmarks<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/stack-docs/edit/8.8/docs/en/stack/ml/nlp/ml-nlp-elser.asciidoc">edit</a></h3>
<p>The following sections provide information about how ELSER performs on different
hardwares and compares the model performance to Elasticsearch BM25 and other strong
baselines such as Splade or OpenAI.</p>
<h4><a id="elser-hw-benchamrks"></a>Hardware benchmarks<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/stack-docs/edit/8.8/docs/en/stack/ml/nlp/ml-nlp-elser.asciidoc">edit</a></h4>
<p>Two data sets were utilized to evaluate the performance of ELSER in different
hardware configurations: <code class="literal">msmarco-long-light</code> and <code class="literal">arguana</code>.</p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
<col class="col_4"/>
</colgroup>
<tbody>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>Data set</strong></span></p></td>
<td align="center" valign="top"><p><span class="strong strong"><strong>Data set size</strong></span></p></td>
<td align="center" valign="top"><p><span class="strong strong"><strong>Average count of tokens / query</strong></span></p></td>
<td align="center" valign="top"><p><span class="strong strong"><strong>Average count of tokens / document</strong></span></p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">msmarco-long-light</code></p></td>
<td align="center" valign="top"><p>37367 documents</p></td>
<td align="center" valign="top"><p>9</p></td>
<td align="center" valign="top"><p>1640</p></td>
</tr>
<tr>
<td align="left" valign="top"><p><code class="literal">arguana</code></p></td>
<td align="center" valign="top"><p>8674 documents</p></td>
<td align="center" valign="top"><p>238</p></td>
<td align="center" valign="top"><p>202</p></td>
</tr>
</tbody>
</table>
</div>
<p>The <code class="literal">msmarco-long-light</code> data set contains long documents with an average of
over 512 tokens, which provides insights into the performance implications
of indexing and inference time for long documents. This is a subset of the
"msmarco" dataset specifically designed for document retrieval (it shouldn&#8217;t be
confused with the "msmarco" dataset used for passage retrieval, which primarily
consists of shorter spans of text).</p>
<p>The <code class="literal">arguana</code> data set is a <a href="https://github.com/beir-cellar/beir" class="ulink" target="_top">BEIR</a> data set.
It consists of long queries with an average of 200 tokens per query. It can
represent an upper limit for query slowness.</p>
<p>The table below present benchmarking results for ELSER using various hardware
configurations.</p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
<col class="col_4"/>
<col class="col_5"/>
<col class="col_6"/>
<col class="col_7"/>
<col class="col_8"/>
</colgroup>
<tbody>
<tr>
<td align="left" valign="top"><p></p></td>
<td align="center" colspan="3" valign="top"><p><code class="literal">msmarco-long-light</code></p></td>
<td align="center" colspan="3" valign="top"><p><code class="literal">arguana</code></p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p></p></td>
<td align="center" valign="middle"><p>inference</p></td>
<td align="center" valign="middle"><p>indexing</p></td>
<td align="center" valign="middle"><p>query latency</p></td>
<td align="center" valign="middle"><p>inference</p></td>
<td align="center" valign="middle"><p>indexing</p></td>
<td align="center" valign="middle"><p>query latency</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>ML node 4GB - 2 vCPUs (1 allocation * 1 thread)</strong></span></p></td>
<td align="center" valign="middle"><p>581   ms/call</p></td>
<td align="center" valign="middle"><p>1.7   doc/sec</p></td>
<td align="center" valign="middle"><p>713   ms/query</p></td>
<td align="center" valign="middle"><p>1200   ms/call</p></td>
<td align="center" valign="middle"><p>0.8   doc/sec</p></td>
<td align="center" valign="middle"><p>169   ms/query</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>ML node 16GB - 8 vCPUs (7 allocation * 1 thread)</strong></span></p></td>
<td align="center" valign="middle"><p>568   ms/call</p></td>
<td align="center" valign="middle"><p>12    doc/sec</p></td>
<td align="center" valign="middle"><p>689   ms/query</p></td>
<td align="center" valign="middle"><p>1280   ms/call</p></td>
<td align="center" valign="middle"><p>5.4   doc/sec</p></td>
<td align="center" valign="middle"><p>159   ms/query</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>ML node 16GB - 8 vCPUs (1 allocation * 8 thread)</strong></span></p></td>
<td align="center" valign="middle"><p>102   ms/call</p></td>
<td align="center" valign="middle"><p>9.7   doc/sec</p></td>
<td align="center" valign="middle"><p>164   ms/query</p></td>
<td align="center" valign="middle"><p>220    ms/call</p></td>
<td align="center" valign="middle"><p>4.5   doc/sec</p></td>
<td align="center" valign="middle"><p>40    ms/query</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
<tr>
<td align="left" valign="top"><p><span class="strong strong"><strong>ML node 32 GB - 16 vCPUs (15 allocation * 1 thread)</strong></span></p></td>
<td align="center" valign="middle"><p>565   ms/call</p></td>
<td align="center" valign="middle"><p>25.2  doc/sec</p></td>
<td align="center" valign="middle"><p>608   ms/query</p></td>
<td align="center" valign="middle"><p>1260   ms/call</p></td>
<td align="center" valign="middle"><p>11.4  doc/sec</p></td>
<td align="center" valign="middle"><p>138   ms/query</p></td>
<td align="left" valign="top"><p></p></td>
</tr>
</tbody>
</table>
</div>
<h4><a id="elser-qualitative-benchmarks"></a>Qualitative benchmarks<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/stack-docs/edit/8.8/docs/en/stack/ml/nlp/ml-nlp-elser.asciidoc">edit</a></h4>
<p>The metric that is used to evaluate ELSER&#8217;s ranking ability is the Normalized
Discounted Cumulative Gain (NDCG) which can handle multiple relevant documents
and fine-grained document ratings. The metric is applied to a fixed-sized list
of retrieved documents which, in this case, is the top 10 documents (NDCG@10).</p>
<p>The table below shows the performance of ELSER compared to Elasticsearch BM25 with an
English analyzer broken down by the 12 data sets used for the evaluation. ELSER
has 10 wins, 1 draw, 1 loss and an average improvement in NDCG@10 of 17%.</p>
<div class="imageblock text-center">
<div class="content">
<img src="images/ml-nlp-elser-ndcg10-beir.png" alt="ELSER benchmarks">
</div>
</div>
<p><em>NDCG@10 for BEIR data sets for BM25 and ELSER  - higher values are better)</em></p>
<p>The following table compares the average performance of ELSER to some other
strong baselines. The OpenAI results are separated out because they use a
different subset of the BEIR suite.</p>
<div class="imageblock text-center">
<div class="content">
<img src="images/ml-nlp-elser-average-ndcg.png" alt="ELSER average performance compared to other baselines">
</div>
</div>
<p><em>Average NDCG@10 for BEIR data sets vs. various high quality baselines (higher</em>
<em>is better). OpenAI chose a different subset, ELSER results on this set</em>
<em>reported separately.</em></p>
<p>To read more about the evaluation details, refer to
<a href="/blog/may-2023-launch-information-retrieval-elasticsearch-ai-model" class="ulink" target="_top">this blog post</a>.</p>
<h3><a id="download-deploy-elser"></a>Download and deploy ELSER<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/stack-docs/edit/8.8/docs/en/stack/ml/nlp/ml-nlp-elser.asciidoc">edit</a></h3>
<p>You can download and deploy ELSER either from <span class="strong strong"><strong>Machine Learning</strong></span> &gt; <span class="strong strong"><strong>Trained Models</strong></span>,
from <span class="strong strong"><strong>Enterprise Search</strong></span> &gt; <span class="strong strong"><strong>Indices</strong></span>, or by using the Dev Console.</p>
<h4><a id="trained-model"></a>Using the Trained Models page<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/stack-docs/edit/8.8/docs/en/stack/ml/nlp/ml-nlp-elser.asciidoc">edit</a></h4>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
In Kibana, navigate to <span class="strong strong"><strong>Machine Learning</strong></span> &gt; <span class="strong strong"><strong>Trained Models</strong></span>. ELSER can be found
in the list of trained models.
</li>
<li class="listitem">
<p>Click the <span class="strong strong"><strong>Download model</strong></span> button under <span class="strong strong"><strong>Actions</strong></span>. You can check the
download status on the <span class="strong strong"><strong>Notifications</strong></span> page.</p>
<div class="imageblock text-center screenshot">
<div class="content">
<img src="images/ml-nlp-elser-download.png" alt="Downloading ELSER">
</div>
</div>
</li>
<li class="listitem">
After the download is finished, start the deployment by clicking the
<span class="strong strong"><strong>Start deployment</strong></span> button.
</li>
<li class="listitem">
<p>Provide a deployment ID, select the priority, and set the number of
allocations and threads per allocation values.</p>
<div class="imageblock text-center screenshot">
<div class="content">
<img src="images/ml-nlp-deploy-elser.png" alt="Deploying ELSER">
</div>
</div>
</li>
<li class="listitem">
Click Start.
</li>
</ol>
</div>
<h4><a id="enterprise-search"></a>Using the Indices page in Enterprise Search<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/stack-docs/edit/8.8/docs/en/stack/ml/nlp/ml-nlp-elser.asciidoc">edit</a></h4>
<p>You can also <a href="/guide/en/enterprise-search/8.8/elser-text-expansion.html#deploy-elser" class="ulink" target="_top">download and deploy ELSER to an inference pipeline</a> directly from the
Enterprise Search app.</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
In Kibana, navigate to <span class="strong strong"><strong>Enterprise Search</strong></span> &gt; <span class="strong strong"><strong>Indices</strong></span>.
</li>
<li class="listitem">
Select the index from the list that has an inference pipeline in which you want
to use ELSER.
</li>
<li class="listitem">
Navigate to the <span class="strong strong"><strong>Pipelines</strong></span> tab.
</li>
<li class="listitem">
Under <span class="strong strong"><strong>Machine Learning Inference Pipelines</strong></span>, click the <span class="strong strong"><strong>Deploy</strong></span> button to
begin downloading the ELSER model. This may take a few minutes depending on your
network. Once it&#8217;s downloaded, click the <span class="strong strong"><strong>Start single-threaded</strong></span> button to
start the model with basic configuration or select the <span class="strong strong"><strong>Fine-tune performance</strong></span>
option to navigate to the <span class="strong strong"><strong>Trained Models</strong></span> page where you can configure the
model deployment.
</li>
</ol>
</div>
<h4><a id="dev-console"></a>Using the Dev Console<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/stack-docs/edit/8.8/docs/en/stack/ml/nlp/ml-nlp-elser.asciidoc">edit</a></h4>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
In Kibana, navigate to the <span class="strong strong"><strong>Dev Console</strong></span>.
</li>
<li class="listitem">
<p>Create the ELSER model configuration by running the following API call:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT _ml/trained_models/.elser_model_1
{
  "input": {
	"field_names": ["text_field"]
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/62.console"></div>
<p>The API call automatically initiates the model download if the model is not
downloaded yet.</p>
</li>
<li class="listitem">
<p>Deploy the model by using the
<a href="/guide/en/elasticsearch/reference/8.8/start-trained-model-deployment.html" class="ulink" target="_top">start trained model deployment API</a>
with a delpoyment ID:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">POST _ml/trained_models/.elser_model_1/deployment/_start?deployment_id=for_search</pre>
</div>
<div class="console_widget" data-snippet="snippets/63.console"></div>
<p>You can deploy the model multiple times with different deployment IDs.</p>
</li>
</ol>
</div>
<p>After the deployment is complete, ELSER is ready to use either in an ingest
pipeline or in a <code class="literal">text_expansion</code> query to perform semantic search.</p>
<h3><a id="air-gapped-install"></a>Deploy ELSER in an air-gapped environment<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/stack-docs/edit/8.8/docs/en/stack/ml/nlp/ml-nlp-elser.asciidoc">edit</a></h3>
<p>If you want to deploy ELSER in a restricted or closed network, you have two
options:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
create your own HTTP/HTTPS endpoint with the model artifacts on it,
</li>
<li class="listitem">
put the model artifacts into a directory inside the config directory on all
<a href="/guide/en/elasticsearch/reference/8.8/modules-node.html#master-node" class="ulink" target="_top">master-eligible nodes</a>.
</li>
</ul>
</div>
<p>You need the following files in your system:</p>
<pre class="screen">https://ml-models.elastic.co/elser_model_1.metadata.json
https://ml-models.elastic.co/elser_model_1.pt
https://ml-models.elastic.co/elser_model_1.vocab.json</pre>
<h4><a id="_using_an_http_server"></a>Using an HTTP server<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/stack-docs/edit/8.8/docs/en/stack/ml/nlp/ml-nlp-elser.asciidoc">edit</a></h4>
<p>INFO: If you use an existing HTTP server, note that the model downloader only
supports passwordless HTTP servers.</p>
<p>You can use any HTTP service to deploy ELSER. This example uses the official
Nginx Docker image to set a new HTTP download service up.</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Download the model artifact files from <a href="https://ml-models.elastic.co/" class="ulink" target="_top">https://ml-models.elastic.co/</a>.
</li>
<li class="listitem">
Put the files into a subdirectory of your choice.
</li>
<li class="listitem">
<p>Run the following commands:</p>
<div class="pre_wrapper lang-shell">
<pre class="programlisting prettyprint lang-shell">export ELASTIC_ML_MODELS="/path/to/models"
docker run --rm -d -p 8080:80 --name ml-models -v ${ELASTIC_ML_MODELS}:/usr/share/nginx/html nginx</pre>
</div>
<p>Don&#8217;t forget to change <code class="literal">/path/to/models</code> to the path of the subdirectory where
the model artifact files are located.</p>
<p>These commands start a local Docker image with an Nginx server with the
subdirectory containing the model files. As the Docker image has to be
downloaded and built, the first start might take a longer period of time.
Subsequent runs start quicker.</p>
</li>
<li class="listitem">
<p>Verify that Nginx runs properly by visiting the following URL in your
browser:</p>
<pre class="screen">http://{IP_ADDRESS_OR_HOSTNAME}:8080/elser_model_1.metadata.json</pre>
<p>If Nginx runs properly, you see the content of the metdata file of the model.</p>
</li>
<li class="listitem">
<p>Point your Elasticsearch deployment to the model artifacts on the HTTP server
by adding the following line to the <code class="literal">config/elasticsearch.yml</code> file:</p>
<pre class="screen">xpack.ml.model_repository: http://{IP_ADDRESS_OR_HOSTNAME}:8080</pre>
<p>If you use your own HTTP or HTTPS server, change the address accordingly. It is
important to specificy the protocol ("http://" or "https://"). Ensure that all
master-eligible nodes can reach the server you specify.</p>
</li>
<li class="listitem">
Repeat step 5 on all master-eligible nodes.
</li>
<li class="listitem">
<a href="/guide/en/elasticsearch/reference/8.8/restart-cluster.html#restart-cluster-rolling" class="ulink" target="_top">Restart</a> the
master-eligible nodes one by one.
</li>
</ol>
</div>
<p>The HTTP server is only required for downloading the model. After the download
has finished, you can stop and delete the service. You can stop the Docker image
used in this example by running the following command:</p>
<div class="pre_wrapper lang-shell">
<pre class="programlisting prettyprint lang-shell">docker stop ml-models</pre>
</div>
<h4><a id="_using_file_based_access"></a>Using file-based access<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/stack-docs/edit/8.8/docs/en/stack/ml/nlp/ml-nlp-elser.asciidoc">edit</a></h4>
<p>For a file-based access, follow these steps:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Download the model artifact files from <a href="https://ml-models.elastic.co/" class="ulink" target="_top">https://ml-models.elastic.co/</a>.
</li>
<li class="listitem">
Put the files into a <code class="literal">models</code> subdirectory inside the <code class="literal">config</code> directory of
your Elasticsearch deployment.
</li>
<li class="listitem">
<p>Point your Elasticsearch deployment to the model directory by adding the
following line to the <code class="literal">config/elasticsearch.yml</code> file:</p>
<pre class="screen">xpack.ml.model_repository: file://${path.home}/config/models/`</pre>
</li>
<li class="listitem">
Repeat step 2 and step 3 on all master-eligible nodes.
</li>
<li class="listitem">
<a href="/guide/en/elasticsearch/reference/8.8/restart-cluster.html#restart-cluster-rolling" class="ulink" target="_top">Restart</a> the
master-eligible nodes one by one.
</li>
</ol>
</div>
<h3><a id="further-readings"></a>Further reading<a class="edit_me edit_me_private" rel="nofollow" title="Editing on GitHub is available to Elastic" href="https://github.com/elastic/stack-docs/edit/8.8/docs/en/stack/ml/nlp/ml-nlp-elser.asciidoc">edit</a></h3>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a href="/guide/en/elasticsearch/reference/8.8/semantic-search-elser.html" class="ulink" target="_top">Perform semantic search with ELSER</a>
</li>
<li class="listitem">
<a href="/blog/may-2023-launch-information-retrieval-elasticsearch-ai-model" class="ulink" target="_top">Improving information retrieval in the Elastic Stack: Introducing Elastic Learned Sparse Encoder, our new retrieval model</a>
</li>
</ul>
</div>
</div>
<div class="navfooter">
<span class="prev">
<a href="ml-nlp-apis.html">« API quick reference</a>
</span>
<span class="next">
<a href="ml-nlp-model-ref.html">Compatible third party NLP models »</a>
</span>
</div>
</div>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  messageStyle: "none",
  tex2jax: {
    inlineMath: [["\\(", "\\)"]],
    displayMath: [["\\[", "\\]"]],
    ignoreClass: "nostem|nolatexmath"
  },
  asciimath2jax: {
    delimiters: [["\\$", "\\$"]],
    ignoreClass: "nostem|noasciimath"
  },
  TeX: { equationNumbers: { autoNumber: "none" } }
})
MathJax.Hub.Register.StartupHook("AsciiMath Jax Ready", function () {
  MathJax.InputJax.AsciiMath.postfilterHooks.Add(function (data, node) {
    if ((node = data.script.parentNode) && (node = node.parentNode) && node.classList.contains('stemblock')) {
      data.math.root.display = "block"
    }
    return data
  })
})
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>
</body>
</html>
