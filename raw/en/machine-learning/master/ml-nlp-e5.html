<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="keywords" content="ML, Elastic Stack, natural language processing, inference">
<title>E5 – EmbEddings from bidirEctional Encoder rEpresentations | Machine Learning in the Elastic Stack [master] | Elastic</title>
<meta class="elastic" name="content" content="E5 – EmbEddings from bidirEctional Encoder rEpresentations | Machine Learning in the Elastic Stack [master]">

<link rel="home" href="index.html" title="Machine Learning in the Elastic Stack [master]"/>
<link rel="up" href="ml-nlp-built-in-models.html" title="Built-in NLP models"/>
<link rel="prev" href="ml-nlp-elser.html" title="ELSER – Elastic Learned Sparse EncodeR"/>
<link rel="next" href="ml-nlp-lang-ident.html" title="Language identification"/>
<meta class="elastic" name="product_version" content="master"/>
<meta class="elastic" name="product_name" content="Machine Learning"/>
<meta class="elastic" name="website_area" content="documentation"/>
<meta name="DC.type" content="Learn/Docs/Elastic Stack/Machine Learning/master"/>
<meta name="DC.subject" content="Machine Learning"/>
<meta name="DC.identifier" content="master"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body>
<div class="page_header">
This documentation contains work-in-progress information for future Elastic Stack and Cloud releases. Use the version selector to view supported release docs. It also contains some Elastic Cloud serverless information. Check out our <a href="https://www.elastic.co/docs/current/serverless">serverless docs</a> for more details.
</div>
<div class="navheader">
<span class="prev">
<a href="ml-nlp-elser.html">« ELSER – Elastic Learned Sparse EncodeR</a>
</span>
<span class="next">
<a href="ml-nlp-lang-ident.html">Language identification »</a>
</span>
</div>
<div class="book" lang="en">
<div class="titlepage">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="index.html">Machine Learning in the Elastic Stack [master]</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="ml-nlp.html">Natural language processing</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="ml-nlp-built-in-models.html">Built-in NLP models</a></span>
</div>
<div>
<div><h1 class="title"><a id="id-1"></a>E5 – EmbEddings from bidirEctional Encoder rEpresentations</h1><a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/main/docs/en/stack/ml/nlp/ml-nlp-e5.asciidoc">edit</a></div>
</div>
<!--EXTRA-->
</div>
<div id="content">
<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="ml-nlp-e5"></a>E5 – EmbEddings from bidirEctional Encoder rEpresentations<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/main/docs/en/stack/ml/nlp/ml-nlp-e5.asciidoc">edit</a></h2>
</div></div></div>

<p>EmbEddings from bidirEctional Encoder rEpresentations - or E5 -  is a natural language processing
model that enables you to perform multi-lingual semantic search by using dense
vector representations. This model is recommended for non-English language
documents and queries. If you want to perform semantic search on English
language documents, use the <a class="xref" href="ml-nlp-elser.html" title="ELSER – Elastic Learned Sparse EncodeR">ELSER</a> model.</p>
<p><a href="/guide/en/elasticsearch/reference/master/semantic-search.html" class="ulink" target="_top">Semantic search</a> provides you search results based on
contextual meaning and user intent, rather than exact keyword matches.</p>
<p>E5 has two versions: one cross-platform version which runs on any hardware
and one version which is optimized for Intel® silicon. The
<span class="strong strong"><strong>Model Management</strong></span> &gt; <span class="strong strong"><strong>Trained Models</strong></span> page shows you which version of E5 is
recommended to deploy based on your cluster&#8217;s hardware.</p>
<p>Refer to the model cards of the
<a href="https://huggingface.co/elastic/multilingual-e5-small" class="ulink" target="_top">multilingual-e5-small</a> and
the
<a href="https://huggingface.co/elastic/multilingual-e5-small-optimized" class="ulink" target="_top">multilingual-e5-small-optimized</a>
models on HuggingFace for further information including licensing.</p>
<h4><a id="e5-req"></a>Requirements<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/main/docs/en/stack/ml/nlp/ml-nlp-e5.asciidoc">edit</a></h4>
<p>To use E5, you must have the <a href="/subscriptions" class="ulink" target="_top">appropriate subscription</a> level
for semantic search or the trial period activated.</p>
<h4><a id="download-deploy-e5"></a>Download and deploy E5<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/main/docs/en/stack/ml/nlp/ml-nlp-e5.asciidoc">edit</a></h4>
<p>You can download and deploy the E5 model either from
<span class="strong strong"><strong>Machine Learning</strong></span> &gt; <span class="strong strong"><strong>Trained Models</strong></span>, from <span class="strong strong"><strong>Search</strong></span> &gt; <span class="strong strong"><strong>Indices</strong></span>, or by using
the Dev Console.</p>
<h5><a id="trained-model-e5"></a>Using the Trained Models page<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/main/docs/en/stack/ml/nlp/ml-nlp-e5.asciidoc">edit</a></h5>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
In Kibana, navigate to <span class="strong strong"><strong>Machine Learning</strong></span> &gt; <span class="strong strong"><strong>Trained Models</strong></span>. E5 can be found in
the list of trained models. There are two versions available: one portable
version which runs on any hardware and one version which is optimized for Intel®
silicon. You can see which model is recommended to use based on your hardware
configuration.
</li>
<li class="listitem">
<p>Click the <span class="strong strong"><strong>Add trained model</strong></span> button. Select the E5 model version you want
to use in the opening modal window. The model that is recommended for you based
on your hardware configuration is highlighted. Click <span class="strong strong"><strong>Download</strong></span>. You can check
the download status on the <span class="strong strong"><strong>Notifications</strong></span> page.</p>
<div class="imageblock text-center screenshot">
<div class="content">
<img src="images/ml-nlp-e5-download.png" alt="Downloading E5">
</div>
</div>
<p>Alternatively, click the <span class="strong strong"><strong>Download model</strong></span> button under <span class="strong strong"><strong>Actions</strong></span> in the
trained model list.</p>
</li>
<li class="listitem">
After the download is finished, start the deployment by clicking the
<span class="strong strong"><strong>Start deployment</strong></span> button.
</li>
<li class="listitem">
<p>Provide a deployment ID, select the priority, and set the number of
allocations and threads per allocation values.</p>
<div class="imageblock text-center screenshot">
<div class="content">
<img src="images/ml-nlp-deployment-id-e5.png" alt="Deploying ELSER">
</div>
</div>
</li>
<li class="listitem">
Click Start.
</li>
</ol>
</div>
<h5><a id="elasticsearch-e5"></a>Using the search indices UI<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/main/docs/en/stack/ml/nlp/ml-nlp-e5.asciidoc">edit</a></h5>
<p>Alternatively, you can download and deploy the E5 model to an inference pipeline
using the search indices UI.</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
In Kibana, navigate to <span class="strong strong"><strong>Search</strong></span> &gt; <span class="strong strong"><strong>Indices</strong></span>.
</li>
<li class="listitem">
Select the index from the list that has an inference pipeline in which you want
to use E5.
</li>
<li class="listitem">
Navigate to the <span class="strong strong"><strong>Pipelines</strong></span> tab.
</li>
<li class="listitem">
<p>Under <span class="strong strong"><strong>Machine Learning Inference Pipelines</strong></span>, click the <span class="strong strong"><strong>Deploy</strong></span> button in the
<span class="strong strong"><strong>Improve your results with E5</strong></span> section to begin downloading the E5 model. This
may take a few minutes depending on your network.</p>
<div class="imageblock text-center screenshot">
<div class="content">
<img src="images/ml-nlp-deploy-e5-es.png" alt="Deploying E5 in Elasticsearch">
</div>
</div>
</li>
<li class="listitem">
<p>Once the model is downloaded, click the <span class="strong strong"><strong>Start single-threaded</strong></span> button to
start the model with basic configuration or select the <span class="strong strong"><strong>Fine-tune performance</strong></span>
option to navigate to the <span class="strong strong"><strong>Trained Models</strong></span> page where you can configure the
model deployment.</p>
<div class="imageblock text-center screenshot">
<div class="content">
<img src="images/ml-nlp-start-e5-es.png" alt="Start E5 in Elasticsearch">
</div>
</div>
</li>
</ol>
</div>
<p>When your E5 model is deployed and started, it is ready to be used in a
pipeline.</p>
<h5><a id="dev-console-e5"></a>Using the Dev Console<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/main/docs/en/stack/ml/nlp/ml-nlp-e5.asciidoc">edit</a></h5>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
In Kibana, navigate to the <span class="strong strong"><strong>Dev Console</strong></span>.
</li>
<li class="listitem">
<p>Create the E5 model configuration by running the following API call:</p>
<div class="pre_wrapper lang-console">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console">PUT _ml/trained_models/.multilingual-e5-small
{
  "input": {
	"field_names": ["text_field"]
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/65.console"></div>
<p>The API call automatically initiates the model download if the model is not
downloaded yet.</p>
</li>
<li class="listitem">
<p>Deploy the model by using the
<a href="/guide/en/elasticsearch/reference/master/start-trained-model-deployment.html" class="ulink" target="_top">start trained model deployment API</a>
with a delpoyment ID:</p>
<div class="pre_wrapper lang-console">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console">POST _ml/trained_models/.multilingual-e5-small/deployment/_start?deployment_id=for_search</pre>
</div>
<div class="console_widget" data-snippet="snippets/66.console"></div>
</li>
</ol>
</div>
<h4><a id="air-gapped-install-e5"></a>Deploy the E5 model in an air-gapped environment<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/main/docs/en/stack/ml/nlp/ml-nlp-e5.asciidoc">edit</a></h4>
<p>If you want to install E5 in an air-gapped environment, you have the following
options:
* put the model artifacts into a directory inside the config directory on all
master-eligible nodes (for <code class="literal">multilingual-e5-small</code> and
<code class="literal">multilingual-e5-small-linux-x86-64</code>)
* install the model by using HuggingFace (for <code class="literal">multilingual-e5-small</code> model
only).</p>
<h5><a id="e5-model-artifacts"></a>Model artifact files<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/main/docs/en/stack/ml/nlp/ml-nlp-e5.asciidoc">edit</a></h5>
<p>For the <code class="literal">multilingual-e5-small</code> model, you need the following files in your
system:</p>
<pre class="screen">https://ml-models.elastic.co/multilingual-e5-small.metadata.json
https://ml-models.elastic.co/multilingual-e5-small.pt
https://ml-models.elastic.co/multilingual-e5-small.vocab.json</pre>
<p>For the optimized version, you need the following files in your system:</p>
<pre class="screen">https://ml-models.elastic.co/multilingual-e5-small_linux-x86_64.metadata.json
https://ml-models.elastic.co/multilingual-e5-small_linux-x86_64.pt
https://ml-models.elastic.co/multilingual-e5-small_linux-x86_64.vocab.json</pre>
<h5><a id="_using_file_based_access_2"></a>Using file-based access<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/main/docs/en/stack/ml/nlp/ml-nlp-e5.asciidoc">edit</a></h5>
<p>For a file-based access, follow these steps:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Download the <a class="xref" href="ml-nlp-e5.html#e5-model-artifacts" title="Model artifact files">model artifact files</a>.
</li>
<li class="listitem">
Put the files into a <code class="literal">models</code> subdirectory inside the <code class="literal">config</code> directory of
your Elasticsearch deployment.
</li>
<li class="listitem">
<p>Point your Elasticsearch deployment to the model directory by adding the following line
to the <code class="literal">config/elasticsearch.yml</code> file:</p>
<pre class="screen">xpack.ml.model_repository: file://${path.home}/config/models/`</pre>
</li>
<li class="listitem">
Repeat step 2 and step 3 on all master-eligible nodes.
</li>
<li class="listitem">
<a href="/guide/en/elasticsearch/reference/master/restart-cluster.html#restart-cluster-rolling" class="ulink" target="_top">Restart</a> the
master-eligible nodes one by one.
</li>
<li class="listitem">
Navigate to the <span class="strong strong"><strong>Trained Models</strong></span> page in Kibana, E5 can be found in the
list of trained models.
</li>
<li class="listitem">
Click the <span class="strong strong"><strong>Add trained model</strong></span> button, select the E5 model version you
downloaded in step 1 and want to deploy and click <span class="strong strong"><strong>Download</strong></span>. The selected
model will be downloaded from the model directory where you put in step 2.
</li>
<li class="listitem">
After the download is finished, start the deployment by clicking the
<span class="strong strong"><strong>Start deployment</strong></span> button.
</li>
<li class="listitem">
Provide a deployment ID, select the priority, and set the number of
allocations and threads per allocation values.
</li>
<li class="listitem">
Click <span class="strong strong"><strong>Start</strong></span>.
</li>
</ol>
</div>
<h5><a id="_using_the_huggingface_repository"></a>Using the HuggingFace repository<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/main/docs/en/stack/ml/nlp/ml-nlp-e5.asciidoc">edit</a></h5>
<p>You can install the <code class="literal">multilingual-e5-small</code> model in a restricted or closed
network by pointing the <code class="literal">eland_import_hub_model</code> script to the model&#8217;s local
files.</p>
<p>For an offline install, the model first needs to be cloned locally, Git and
<a href="https://git-lfs.com/" class="ulink" target="_top">Git Large File Storage</a> are required to be installed in
your system.</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
<p>Clone the E5 model from Hugging Face by using the model URL.</p>
<div class="pre_wrapper lang-bash">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-bash">git clone https://huggingface.co/elastic/multilingual-e5-small</pre>
</div>
<p>The command results in a local copy of the model in the <code class="literal">multilingual-e5-small</code>
directory.</p>
</li>
<li class="listitem">
<p>Use the <code class="literal">eland_import_hub_model</code> script with the <code class="literal">--hub-model-id</code> set to the
directory of the cloned model to install it:</p>
<div class="pre_wrapper lang-bash">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-bash">eland_import_hub_model \
      --url 'XXXX' \
      --hub-model-id /PATH/TO/MODEL \
      --task-type text_embedding \
      --es-username elastic --es-password XXX \
      --es-model-id multilingual-e5-small</pre>
</div>
<p>If you use the Docker image to run <code class="literal">eland_import_hub_model</code> you must bind mount
the model directory, so the container can read the files.</p>
<div class="pre_wrapper lang-bash">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-bash">docker run --mount type=bind,source=/PATH/TO/MODELS,destination=/models,readonly -it --rm docker.elastic.co/eland/eland \
    eland_import_hub_model \
      --url 'XXXX' \
      --hub-model-id /models/multilingual-e5-small \
      --task-type text_embedding \
      --es-username elastic --es-password XXX \
      --es-model-id multilingual-e5-small</pre>
</div>
<p>Once it&#8217;s uploaded to Elasticsearch, the model will have the ID specified by
<code class="literal">--es-model-id</code>. If it is not set, the model ID is derived from
<code class="literal">--hub-model-id</code>; spaces and path delimiters are converted to double
underscores <code class="literal">__</code>.</p>
</li>
</ol>
</div>
<h4><a id="terms-of-use-e5"></a>Disclaimer<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/stack-docs/edit/main/docs/en/stack/ml/nlp/ml-nlp-e5.asciidoc">edit</a></h4>
<p>Customers may add third party trained models for management in Elastic. These
models are not owned by Elastic. While Elastic will support the integration with
these models in the performance according to the documentation, you understand
and agree that Elastic has no control over, or liability for, the third party
models or the underlying training data they may utilize.</p>
</div>
</div>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  messageStyle: "none",
  tex2jax: {
    inlineMath: [["\\(", "\\)"]],
    displayMath: [["\\[", "\\]"]],
    ignoreClass: "nostem|nolatexmath"
  },
  asciimath2jax: {
    delimiters: [["\\$", "\\$"]],
    ignoreClass: "nostem|noasciimath"
  },
  TeX: { equationNumbers: { autoNumber: "none" } }
})
MathJax.Hub.Register.StartupHook("AsciiMath Jax Ready", function () {
  MathJax.InputJax.AsciiMath.postfilterHooks.Add(function (data, node) {
    if ((node = data.script.parentNode) && (node = node.parentNode) && node.classList.contains('stemblock')) {
      data.math.root.display = "block"
    }
    return data
  })
})
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>
</div><div class="navfooter">
<span class="prev">
<a href="ml-nlp-elser.html">« ELSER – Elastic Learned Sparse EncodeR</a>
</span>
<span class="next">
<a href="ml-nlp-lang-ident.html">Language identification »</a>
</span>
</div>
</body>
</html>
