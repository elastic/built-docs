<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Parse and organize logs | Elastic Observability [master] | Elastic</title>
<meta class="elastic" name="content" content="Parse and organize logs | Elastic Observability [master]">

<link rel="home" href="index.html" title="Elastic Observability [master]"/>
<link rel="up" href="logs-checklist.html" title="Log monitoring"/>
<link rel="prev" href="logs-stream.html" title="Stream any log file"/>
<link rel="next" href="logs-filter-and-aggregate.html" title="Filter and aggregate logs"/>
<meta class="elastic" name="product_version" content="master"/>
<meta class="elastic" name="product_name" content="Observability"/>
<meta class="elastic" name="website_area" content="documentation"/>
<meta name="DC.type" content="Learn/Docs/Observability/Guide/master"/>
<meta name="DC.subject" content="Observability"/>
<meta name="DC.identifier" content="master"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body>
<div class="page_header">
This documentation contains work-in-progress information for future Elastic Stack and Cloud releases. Use the version selector to view supported release docs. It also contains some Elastic Cloud serverless information. Check out our <a href="https://www.elastic.co/docs/current/serverless">serverless docs</a> for more details.
</div>
<div class="navheader">
<span class="prev">
<a href="logs-stream.html">« Stream any log file</a>
</span>
<span class="next">
<a href="logs-filter-and-aggregate.html">Filter and aggregate logs »</a>
</span>
</div>
<div class="book" lang="en">
<div class="titlepage">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="index.html">Elastic Observability [master]</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="logs-checklist.html">Log monitoring</a></span>
</div>
<div>
<div><h1 class="title"><a id="id-1"></a>Parse and organize logs</h1><a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/main/docs/en/observability/logs-parse.asciidoc">edit</a></div>
</div>
<!--EXTRA-->
</div>
<div id="content">
<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="logs-parse"></a>Parse and organize logs<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/main/docs/en/observability/logs-parse.asciidoc">edit</a></h2>
</div></div></div>
<p>If your log data is unstructured or semi-structured, you can parse it and break it into meaningful fields. You can use those fields to explore and analyze your data. For example, you can find logs within a specific timestamp range or filter logs by log level to focus on potential issues.</p>
<p>After parsing, you can use the structured fields to further organize your logs by configuring a reroute processor to send specific logs to different target data streams.</p>
<p>Refer to the following sections for more on parsing and organizing your log data:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="logs-parse.html#logs-stream-parse" title="Extract structured fields">Extract structured fields</a>: Extract structured fields like timestamps, log levels, or IP addresses to make querying and filtering your data easier.
</li>
<li class="listitem">
<a class="xref" href="logs-parse.html#logs-stream-reroute" title="Reroute log data to specific data streams">Reroute log data to specific data streams</a>: Route data from the generic data stream to a target data stream for more granular control over data retention, permissions, and processing.
</li>
</ul>
</div>
<h3><a id="logs-stream-parse"></a>Extract structured fields<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/main/docs/en/observability/logs-parse.asciidoc">edit</a></h3>
<p>Make your logs more useful by extracting structured fields from your unstructured log data. Extracting structured fields makes it easier to search, analyze, and filter your log data.</p>
<p>Follow the steps below to see how the following unstructured log data is indexed by default:</p>
<div class="pre_wrapper lang-log">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-log">2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%.</pre>
</div>
<p>Start by storing the document in the <code class="literal">logs-example-default</code> data stream:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
In Kibana, go to <span class="strong strong"><strong>Management</strong></span> &#8594; <span class="strong strong"><strong>Dev Tools</strong></span>.
</li>
<li class="listitem">
<p>In the <span class="strong strong"><strong>Console</strong></span> tab, add the example log to Elasticsearch using the following command:</p>
<div class="pre_wrapper lang-console">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console">POST logs-example-default/_doc
{
  "message": "2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%."
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/19.console"></div>
</li>
<li class="listitem">
<p>Then, you can retrieve the document with the following search:</p>
<div class="pre_wrapper lang-console">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console">GET /logs-example-default/_search</pre>
</div>
<div class="console_widget" data-snippet="snippets/20.console"></div>
</li>
</ol>
</div>
<p>The results should look like this:</p>
<div class="pre_wrapper lang-JSON">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-JSON">{
  ...
  "hits": {
    ...
    "hits": [
      {
        "_index": ".ds-logs-example-default-2023.08.09-000001",
        ...
        "_source": {
          "message": "2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%.",
          "@timestamp": "2023-08-09T17:19:27.73312243Z"
        }
      }
    ]
  }
}</pre>
</div>
<p>Elasticsearch indexes the <code class="literal">message</code> field by default and adds a <code class="literal">@timestamp</code> field. Since there was no timestamp set, it&#8217;s set to <code class="literal">now</code>. At this point, you can search for phrases in the <code class="literal">message</code> field like <code class="literal">WARN</code> or <code class="literal">Disk usage exceeds</code>. For example, use the following command to search for the phrase <code class="literal">WARN</code> in the log&#8217;s <code class="literal">message</code> field:</p>
<div class="pre_wrapper lang-console">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console">GET logs-example-default/_search
{
  "query": {
    "match": {
      "message": {
        "query": "WARN"
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/21.console"></div>
<p>While you can search for phrases in the <code class="literal">message</code> field, you can&#8217;t use this field to filter log data. Your message, however, contains all of the following potential fields you can extract and use to filter and aggregate your log data:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<span class="strong strong"><strong>@timestamp</strong></span> (<code class="literal">2023-08-08T13:45:12.123Z</code>): Extracting this field lets you sort logs by date and time. This is helpful when you want to view your logs in the order that they occurred or identify when issues happened.
</li>
<li class="listitem">
<span class="strong strong"><strong>log.level</strong></span> (<code class="literal">WARN</code>): Extracting this field lets you filter logs by severity. This is helpful if you want to focus on high-severity WARN or ERROR-level logs, and reduce noise by filtering out low-severity INFO-level logs.
</li>
<li class="listitem">
<span class="strong strong"><strong>host.ip</strong></span> (<code class="literal">192.168.1.101</code>): Extracting this field lets you filter logs by the host IP addresses. This is helpful if you want to focus on specific hosts that you’re having issues with or if you want to find disparities between hosts.
</li>
<li class="listitem">
<span class="strong strong"><strong>message</strong></span> (<code class="literal">Disk usage exceeds 90%.</code>): You can search for phrases or words in the message field.
</li>
</ul>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>These fields are part of the <a href="/guide/en/ecs/8.11/ecs-reference.html" class="ulink" target="_top">Elastic Common Schema (ECS)</a>. The ECS defines a common set of fields that you can use across Elasticsearch when storing data, including log and metric data.</p>
</div>
</div>
<h4><a id="logs-stream-extract-timestamp"></a>Extract the <code class="literal">@timestamp</code> field<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/main/docs/en/observability/logs-parse.asciidoc">edit</a></h4>
<p>When you added the log to Elasticsearch in the previous section, the <code class="literal">@timestamp</code> field showed when the log was added. The timestamp showing when the log actually occurred was in the unstructured <code class="literal">message</code> field:</p>
<div class="pre_wrapper lang-JSON">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-JSON">        ...
        "_source": {
          "message": "2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%.",<a id="CO61-1"></a><i class="conum" data-value="1"></i>
          "@timestamp": "2023-08-09T17:19:27.73312243Z"<a id="CO61-2"></a><i class="conum" data-value="2"></i>
        }
        ...</pre>
</div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO61-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>The timestamp in the <code class="literal">message</code> field shows when the log occurred.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO61-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p>The timestamp in the <code class="literal">@timestamp</code> field shows when the log was added to Elasticsearch.</p>
</td>
</tr>
</table>
</div>
<p>When looking into issues, you want to filter for logs by when the issue occurred not when the log was added to your project.
To do this, extract the timestamp from the unstructured <code class="literal">message</code> field to the structured <code class="literal">@timestamp</code> field by completing the following:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
<a class="xref" href="logs-parse.html#logs-stream-ingest-pipeline" title="Use an ingest pipeline to extract the @timestamp field">Use an ingest pipeline to extract the <code class="literal">@timestamp</code> field</a>
</li>
<li class="listitem">
<a class="xref" href="logs-parse.html#logs-stream-simulate-api" title="Test the pipeline with the simulate pipeline API">Test the pipeline with the simulate pipeline API</a>
</li>
<li class="listitem">
<a class="xref" href="logs-parse.html#logs-stream-index-template" title="Configure a data stream with an index template">Configure a data stream with an index template</a>
</li>
<li class="listitem">
<a class="xref" href="logs-parse.html#logs-stream-create-data-stream" title="Create a data stream">Create a data stream</a>
</li>
</ol>
</div>
<h5><a id="logs-stream-ingest-pipeline"></a>Use an ingest pipeline to extract the <code class="literal">@timestamp</code> field<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/main/docs/en/observability/logs-parse.asciidoc">edit</a></h5>
<p>Ingest pipelines consist of a series of processors that perform common transformations on incoming documents before they are indexed. To extract the <code class="literal">@timestamp</code> field from the example log, use an ingest pipeline with a dissect processor. The <a href="/guide/en/elasticsearch/reference/master/dissect-processor.html" class="ulink" target="_top">dissect processor</a> extracts structured fields from unstructured log messages based on a pattern you set.</p>
<p>Elasticsearch can parse string timestamps that are in <code class="literal">yyyy-MM-dd'T'HH:mm:ss.SSSZ</code> and <code class="literal">yyyy-MM-dd</code> formats into date fields. Since the log example&#8217;s timestamp is in one of these formats, you don&#8217;t need additional processors. More complex or nonstandard timestamps require a <a href="/guide/en/elasticsearch/reference/master/date-processor.html" class="ulink" target="_top">date processor</a> to parse the timestamp into a date field.</p>
<p>Use the following command to extract the timestamp from the <code class="literal">message</code> field into the <code class="literal">@timestamp</code> field:</p>
<div class="pre_wrapper lang-console">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console">PUT _ingest/pipeline/logs-example-default<a id="CO62-1"></a><i class="conum" data-value="1"></i>
{
  "description": "Extracts the timestamp",
  "processors": [
    {
      "dissect": {
        "field": "message",<a id="CO62-2"></a><i class="conum" data-value="2"></i>
        "pattern": "%{@timestamp} %{message}"<a id="CO62-3"></a><i class="conum" data-value="3"></i>
      }
    }
  ]
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/22.console"></div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO62-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>The name of the pipeline,<code class="literal">logs-example-default</code>, needs to match the name of your data stream. You&#8217;ll set up your data stream in the next section. For more information, refer to the <a href="/guide/en/fleet/master/data-streams.html#data-streams-naming-scheme" class="ulink" target="_top">data stream naming scheme</a>.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO62-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p>The field you&#8217;re extracting data from, <code class="literal">message</code> in this case.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO62-3"><i class="conum" data-value="3"></i></a></p>
</td>
<td align="left" valign="top">
<p>The pattern of the elements in your log data. The <code class="literal">%{@timestamp} %{message}</code> pattern extracts the timestamp, <code class="literal">2023-08-08T13:45:12.123Z</code>, to the <code class="literal">@timestamp</code> field, while the rest of the message, <code class="literal">WARN 192.168.1.101 Disk usage exceeds 90%.</code>, stays in the <code class="literal">message</code> field. The dissect processor looks for the space as a separator defined by the pattern.</p>
</td>
</tr>
</table>
</div>
<h5><a id="logs-stream-simulate-api"></a>Test the pipeline with the simulate pipeline API<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/main/docs/en/observability/logs-parse.asciidoc">edit</a></h5>
<p>The <a href="/guide/en/elasticsearch/reference/master/simulate-pipeline-api.html#ingest-verbose-param" class="ulink" target="_top">simulate pipeline API</a> runs the ingest pipeline without storing any documents. This lets you verify your pipeline works using multiple documents. Run the following command to test your ingest pipeline with the simulate pipeline API.</p>
<div class="pre_wrapper lang-console">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console">POST _ingest/pipeline/logs-example-default/_simulate
{
  "docs": [
    {
      "_source": {
        "message": "2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%."
      }
    }
  ]
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/23.console"></div>
<p>The results should show the <code class="literal">@timestamp</code> field extracted from the <code class="literal">message</code> field:</p>
<div class="pre_wrapper lang-console">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console">{
  "docs": [
    {
      "doc": {
        "_index": "_index",
        "_id": "_id",
        "_version": "-3",
        "_source": {
          "message": "WARN 192.168.1.101 Disk usage exceeds 90%.",
          "@timestamp": "2023-08-08T13:45:12.123Z"
        },
        ...
      }
    }
  ]
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/24.console"></div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>Make sure you&#8217;ve created the ingest pipeline using the <code class="literal">PUT</code> command in the previous section before using the simulate pipeline API.</p>
</div>
</div>
<h5><a id="logs-stream-index-template"></a>Configure a data stream with an index template<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/main/docs/en/observability/logs-parse.asciidoc">edit</a></h5>
<p>After creating your ingest pipeline, run the following command to create an index template to configure your data stream&#8217;s backing indices:</p>
<div class="pre_wrapper lang-console">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console">PUT _index_template/logs-example-default-template
{
  "index_patterns": [ "logs-example-*" ],<a id="CO63-1"></a><i class="conum" data-value="1"></i>
  "data_stream": { },<a id="CO63-2"></a><i class="conum" data-value="2"></i>
  "priority": 500,<a id="CO63-3"></a><i class="conum" data-value="3"></i>
  "template": {
    "settings": {
      "index.default_pipeline":"logs-example-default"<a id="CO63-4"></a><i class="conum" data-value="4"></i>
    }
  },
  "composed_of": [<a id="CO63-5"></a><i class="conum" data-value="5"></i>
    "logs@mappings",
    "logs@settings",
    "logs@custom",
    "ecs@mappings"
  ],
  "ignore_missing_component_templates": ["logs@custom"]
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/25.console"></div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO63-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p><code class="literal">index_pattern</code>: Needs to match your log data stream. Naming conventions for data streams are <code class="literal">&lt;type&gt;-&lt;dataset&gt;-&lt;namespace&gt;</code>. In this example, your logs data stream is named <code class="literal">logs-example-*</code>. Data that matches this pattern will go through your pipeline.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO63-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p><code class="literal">data_stream</code>: Enables data streams.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO63-3"><i class="conum" data-value="3"></i></a></p>
</td>
<td align="left" valign="top">
<p><code class="literal">priority</code>: Sets the priority of you Index Template. Index templates with higher priority take precedence over lower priority. If a data stream matches multiple index templates, Elasticsearch uses the template with the higher priority. Built-in templates have a priority of <code class="literal">200</code>, so use a priority higher than <code class="literal">200</code> for custom templates.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO63-4"><i class="conum" data-value="4"></i></a></p>
</td>
<td align="left" valign="top">
<p><code class="literal">index.default_pipeline</code>: The name of your ingest pipeline. <code class="literal">logs-example-default</code> in this case.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO63-5"><i class="conum" data-value="5"></i></a></p>
</td>
<td align="left" valign="top">
<p><code class="literal">composed_of</code>: Here you can set component templates. Component templates are building blocks for constructing index templates that specify index mappings, settings, and aliases. Elastic has several built-in templates to help when ingesting your log data.</p>
</td>
</tr>
</table>
</div>
<p>The example index template above sets the following component templates:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">logs@mappings</code>: general mappings for log data streams that include disabling automatic date detection from <code class="literal">string</code> fields and specifying mappings for <a href="/guide/en/ecs/8.11/ecs-data_stream.html" class="ulink" target="_top"><code class="literal">data_stream</code> ECS fields</a>.
</li>
<li class="listitem">
<p><code class="literal">logs@settings</code>: general settings for log data streams including the following:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
The default lifecycle policy that rolls over when the primary shard reaches 50 GB or after 30 days.
</li>
<li class="listitem">
The default pipeline uses the ingest timestamp if there is no specified <code class="literal">@timestamp</code> and places a hook for the <code class="literal">logs@custom</code> pipeline. If a <code class="literal">logs@custom</code> pipeline is installed, it&#8217;s applied to logs ingested into this data stream.
</li>
<li class="listitem">
Sets the <a href="/guide/en/elasticsearch/reference/master/ignore-malformed.html" class="ulink" target="_top"><code class="literal">ignore_malformed</code></a> flag to <code class="literal">true</code>. When ingesting a large batch of log data, a single malformed field like an IP address can cause the entire batch to fail. When set to true, malformed fields with a mapping type that supports this flag are still processed.
</li>
</ul>
</div>
</li>
<li class="listitem">
<code class="literal">logs@custom</code>: a predefined component template that is not installed by default. Use this name to install a custom component template to override or extend any of the default mappings or settings.
</li>
<li class="listitem">
<code class="literal">ecs@mappings</code>: dynamic templates that automatically ensure your data stream mappings comply with the <a href="/guide/en/ecs/8.11/ecs-reference.html" class="ulink" target="_top">Elastic Common Schema (ECS)</a>.
</li>
</ul>
</div>
<h5><a id="logs-stream-create-data-stream"></a>Create a data stream<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/main/docs/en/observability/logs-parse.asciidoc">edit</a></h5>
<p>Create your data stream using the <a href="/guide/en/fleet/master/data-streams.html#data-streams-naming-scheme" class="ulink" target="_top">data stream naming scheme</a>. Name your data stream to match the name of your ingest pipeline, <code class="literal">logs-example-default</code> in this case. Post the example log to your data stream with this command:</p>
<div class="pre_wrapper lang-console">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console">POST logs-example-default/_doc
{
  "message": "2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%."
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/26.console"></div>
<p>View your documents using this command:</p>
<div class="pre_wrapper lang-console">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console">GET /logs-example-default/_search</pre>
</div>
<div class="console_widget" data-snippet="snippets/27.console"></div>
<p>You should see the pipeline has extracted the <code class="literal">@timestamp</code> field:</p>
<div class="pre_wrapper lang-JSON">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-JSON">{
...
{
  ...
  "hits": {
    ...
    "hits": [
      {
        "_index": ".ds-logs-example-default-2023.08.09-000001",
        "_id": "RsWy3IkB8yCtA5VGOKLf",
        "_score": 1,
        "_source": {
          "message": "WARN 192.168.1.101 Disk usage exceeds 90%.",
          "@timestamp": "2023-08-08T13:45:12.123Z"<a id="CO64-1"></a><i class="conum" data-value="1"></i>
        }
      }
    ]
  }
}</pre>
</div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO64-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>The extracted <code class="literal">@timestamp</code> field.</p>
</td>
</tr>
</table>
</div>
<p>You can now use the <code class="literal">@timestamp</code> field to sort your logs by the date and time they happened.</p>
<h5><a id="logs-stream-timestamp-troubleshooting"></a>Troubleshoot the <code class="literal">@timestamp</code> field<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/main/docs/en/observability/logs-parse.asciidoc">edit</a></h5>
<p>Check the following common issues and solutions with timestamps:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<span class="strong strong"><strong>Timestamp failure</strong></span>: If your data has inconsistent date formats, set <code class="literal">ignore_failure</code> to <code class="literal">true</code> for your date processor. This processes logs with correctly formatted dates and ignores those with issues.
</li>
<li class="listitem">
<span class="strong strong"><strong>Incorrect timezone</strong></span>: Set your timezone using the <code class="literal">timezone</code> option on the <a href="/guide/en/elasticsearch/reference/master/date-processor.html" class="ulink" target="_top">date processor</a>.
</li>
<li class="listitem">
<span class="strong strong"><strong>Incorrect timestamp format</strong></span>: Your timestamp can be a Java time pattern or one of the following formats: ISO8601, UNIX, UNIX_MS, or TAI64N. For more information on timestamp formats, refer to the <a href="/guide/en/elasticsearch/reference/master/mapping-date-format.html" class="ulink" target="_top">mapping date format</a>.
</li>
</ul>
</div>
<h4><a id="logs-stream-extract-log-level"></a>Extract the <code class="literal">log.level</code> field<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/main/docs/en/observability/logs-parse.asciidoc">edit</a></h4>
<p>Extracting the <code class="literal">log.level</code> field lets you filter by severity and focus on critical issues. This section shows you how to extract the <code class="literal">log.level</code> field from this example log:</p>
<div class="pre_wrapper lang-log">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-log">2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%.</pre>
</div>
<p>To extract and use the <code class="literal">log.level</code> field:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
<a class="xref" href="logs-parse.html#logs-stream-log-level-pipeline" title="Add log.level to your ingest pipeline">Add the <code class="literal">log.level</code> field to the dissect processor pattern in your ingest pipeline.</a>
</li>
<li class="listitem">
<a class="xref" href="logs-parse.html#logs-stream-log-level-simulate" title="Test the pipeline with the simulate API">Test the pipeline with the simulate API.</a>
</li>
<li class="listitem">
<a class="xref" href="logs-parse.html#logs-stream-log-level-query" title="Query logs based on log.level">Query your logs based on the <code class="literal">log.level</code> field.</a>
</li>
</ol>
</div>
<h5><a id="logs-stream-log-level-pipeline"></a>Add <code class="literal">log.level</code> to your ingest pipeline<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/main/docs/en/observability/logs-parse.asciidoc">edit</a></h5>
<p>Add the <code class="literal">%{log.level}</code> option to the dissect processor pattern in the ingest pipeline you created in the <a class="xref" href="logs-parse.html#logs-stream-ingest-pipeline" title="Use an ingest pipeline to extract the @timestamp field">Extract the <code class="literal">@timestamp</code> field</a> section with this command:</p>
<div class="pre_wrapper lang-console">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console">PUT _ingest/pipeline/logs-example-default
{
  "description": "Extracts the timestamp and log level",
  "processors": [
    {
      "dissect": {
        "field": "message",
        "pattern": "%{@timestamp} %{log.level} %{message}"
      }
    }
  ]
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/28.console"></div>
<p>Now your pipeline will extract these fields:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
The <code class="literal">@timestamp</code> field: <code class="literal">2023-08-08T13:45:12.123Z</code>
</li>
<li class="listitem">
The <code class="literal">log.level</code> field: <code class="literal">WARN</code>
</li>
<li class="listitem">
The <code class="literal">message</code> field: <code class="literal">192.168.1.101 Disk usage exceeds 90%.</code>
</li>
</ul>
</div>
<p>In addition to setting an ingest pipeline, you need to set an index template. You can use the index template created in the <a class="xref" href="logs-parse.html#logs-stream-index-template" title="Configure a data stream with an index template">Extract the <code class="literal">@timestamp</code> field</a> section.</p>
<h5><a id="logs-stream-log-level-simulate"></a>Test the pipeline with the simulate API<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/main/docs/en/observability/logs-parse.asciidoc">edit</a></h5>
<p>Test that your ingest pipeline works as expected with the <a href="/guide/en/elasticsearch/reference/master/simulate-pipeline-api.html#ingest-verbose-param" class="ulink" target="_top">simulate pipeline API</a>:</p>
<div class="pre_wrapper lang-console">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console">POST _ingest/pipeline/logs-example-default/_simulate
{
  "docs": [
    {
      "_source": {
        "message": "2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%."
      }
    }
  ]
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/29.console"></div>
<p>The results should show the <code class="literal">@timestamp</code> and the <code class="literal">log.level</code> fields extracted from the <code class="literal">message</code> field:</p>
<div class="pre_wrapper lang-JSON">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-JSON">{
  "docs": [
    {
      "doc": {
        "_index": "_index",
        "_id": "_id",
        "_version": "-3",
        "_source": {
          "message": "192.168.1.101 Disk usage exceeds 90%.",
          "log": {
            "level": "WARN"
          },
          "@timestamp": "2023-8-08T13:45:12.123Z",
        },
        ...
      }
    }
  ]
}</pre>
</div>
<h5><a id="logs-stream-log-level-query"></a>Query logs based on <code class="literal">log.level</code><a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/main/docs/en/observability/logs-parse.asciidoc">edit</a></h5>
<p>Once you&#8217;ve extracted the <code class="literal">log.level</code> field, you can query for high-severity logs like <code class="literal">WARN</code> and <code class="literal">ERROR</code>, which may need immediate attention, and filter out less critical <code class="literal">INFO</code> and <code class="literal">DEBUG</code> logs.</p>
<p>Let&#8217;s say you have the following logs with varying severities:</p>
<div class="pre_wrapper lang-log">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-log">2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%.
2023-08-08T13:45:14.003Z ERROR 192.168.1.103 Database connection failed.
2023-08-08T13:45:15.004Z DEBUG 192.168.1.104 Debugging connection issue.
2023-08-08T13:45:16.005Z INFO 192.168.1.102 User changed profile picture.</pre>
</div>
<p>Add them to your data stream using this command:</p>
<div class="pre_wrapper lang-console">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console">POST logs-example-default/_bulk
{ "create": {} }
{ "message": "2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%." }
{ "create": {} }
{ "message": "2023-08-08T13:45:14.003Z ERROR 192.168.1.103 Database connection failed." }
{ "create": {} }
{ "message": "2023-08-08T13:45:15.004Z DEBUG 192.168.1.104 Debugging connection issue." }
{ "create": {} }
{ "message": "2023-08-08T13:45:16.005Z INFO 192.168.1.102 User changed profile picture." }</pre>
</div>
<div class="console_widget" data-snippet="snippets/30.console"></div>
<p>Then, query for documents with a log level of <code class="literal">WARN</code> or <code class="literal">ERROR</code> with this command:</p>
<div class="pre_wrapper lang-console">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console">GET logs-example-default/_search
{
  "query": {
    "terms": {
      "log.level": ["WARN", "ERROR"]
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/31.console"></div>
<p>The results should show only the high-severity logs:</p>
<div class="pre_wrapper lang-JSON">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-JSON">{
...
  },
  "hits": {
  ...
    "hits": [
      {
        "_index": ".ds-logs-example-default-2023.08.14-000001",
        "_id": "3TcZ-4kB3FafvEVY4yKx",
        "_score": 1,
        "_source": {
          "message": "192.168.1.101 Disk usage exceeds 90%.",
          "log": {
            "level": "WARN"
          },
          "@timestamp": "2023-08-08T13:45:12.123Z"
        }
      },
      {
        "_index": ".ds-logs-example-default-2023.08.14-000001",
        "_id": "3jcZ-4kB3FafvEVY4yKx",
        "_score": 1,
        "_source": {
          "message": "192.168.1.103 Database connection failed.",
          "log": {
            "level": "ERROR"
          },
          "@timestamp": "2023-08-08T13:45:14.003Z"
        }
      }
    ]
  }
}</pre>
</div>
<h4><a id="logs-stream-extract-host-ip"></a>Extract the <code class="literal">host.ip</code> field<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/main/docs/en/observability/logs-parse.asciidoc">edit</a></h4>
<p>Extracting the <code class="literal">host.ip</code> field lets you filter logs by host IP addresses allowing you to focus on specific hosts that you&#8217;re having issues with or find disparities between hosts.</p>
<p>The <code class="literal">host.ip</code> field is part of the <a href="/guide/en/ecs/8.11/ecs-reference.html" class="ulink" target="_top">Elastic Common Schema (ECS)</a>. Through the ECS, the <code class="literal">host.ip</code> field is mapped as an <a href="/guide/en/elasticsearch/reference/master/ip.html" class="ulink" target="_top"><code class="literal">ip</code> field type</a>. <code class="literal">ip</code> field types allow range queries so you can find logs with IP addresses in a specific range. You can also query <code class="literal">ip</code> field types using Classless Inter-Domain Routing (CIDR) notation to find logs from a particular network or subnet.</p>
<p>This section shows you how to extract the <code class="literal">host.ip</code> field from the following example logs and query based on the extracted fields:</p>
<div class="pre_wrapper lang-log">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-log">2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%.
2023-08-08T13:45:14.003Z ERROR 192.168.1.103 Database connection failed.
2023-08-08T13:45:15.004Z DEBUG 192.168.1.104 Debugging connection issue.
2023-08-08T13:45:16.005Z INFO 192.168.1.102 User changed profile picture.</pre>
</div>
<p>To extract and use the <code class="literal">host.ip</code> field:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
<a class="xref" href="logs-parse.html#logs-stream-host-ip-pipeline" title="Add host.ip to your ingest pipeline">Add the <code class="literal">host.ip</code> field to your dissect processor in your ingest pipeline.</a>
</li>
<li class="listitem">
<a class="xref" href="logs-parse.html#logs-stream-host-ip-simulate" title="Test the pipeline with the simulate API">Test the pipeline with the simulate API.</a>
</li>
<li class="listitem">
<a class="xref" href="logs-parse.html#logs-stream-host-ip-query" title="Query logs based on host.ip">Query your logs based on the <code class="literal">host.ip</code> field.</a>
</li>
</ol>
</div>
<h5><a id="logs-stream-host-ip-pipeline"></a>Add <code class="literal">host.ip</code> to your ingest pipeline<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/main/docs/en/observability/logs-parse.asciidoc">edit</a></h5>
<p>Add the <code class="literal">%{host.ip}</code> option to the dissect processor pattern in the ingest pipeline you created in the <a class="xref" href="logs-parse.html#logs-stream-ingest-pipeline" title="Use an ingest pipeline to extract the @timestamp field">Extract the <code class="literal">@timestamp</code> field</a> section:</p>
<div class="pre_wrapper lang-console">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console">PUT _ingest/pipeline/logs-example-default
{
  "description": "Extracts the timestamp log level and host ip",
  "processors": [
    {
      "dissect": {
        "field": "message",
        "pattern": "%{@timestamp} %{log.level} %{host.ip} %{message}"
      }
    }
  ]
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/32.console"></div>
<p>Your pipeline will extract these fields:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
The <code class="literal">@timestamp</code> field: <code class="literal">2023-08-08T13:45:12.123Z</code>
</li>
<li class="listitem">
The <code class="literal">log.level</code> field: <code class="literal">WARN</code>
</li>
<li class="listitem">
The <code class="literal">host.ip</code> field: <code class="literal">192.168.1.101</code>
</li>
<li class="listitem">
The <code class="literal">message</code> field: <code class="literal">Disk usage exceeds 90%.</code>
</li>
</ul>
</div>
<p>In addition to setting an ingest pipeline, you need to set an index template. You can use the index template created in the <a class="xref" href="logs-parse.html#logs-stream-index-template" title="Configure a data stream with an index template">Extract the <code class="literal">@timestamp</code> field</a> section.</p>
<h5><a id="logs-stream-host-ip-simulate"></a>Test the pipeline with the simulate API<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/main/docs/en/observability/logs-parse.asciidoc">edit</a></h5>
<p>Test that your ingest pipeline works as expected with the <a href="/guide/en/elasticsearch/reference/master/simulate-pipeline-api.html#ingest-verbose-param" class="ulink" target="_top">simulate pipeline API</a>:</p>
<div class="pre_wrapper lang-console">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console">POST _ingest/pipeline/logs-example-default/_simulate
{
  "docs": [
    {
      "_source": {
        "message": "2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%."
      }
    }
  ]
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/33.console"></div>
<p>The results should show the <code class="literal">host.ip</code>, <code class="literal">@timestamp</code>, and <code class="literal">log.level</code> fields extracted from the <code class="literal">message</code> field:</p>
<div class="pre_wrapper lang-JSON">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-JSON">{
  "docs": [
    {
      "doc": {
        ...
        "_source": {
          "host": {
            "ip": "192.168.1.101"
          },
          "@timestamp": "2023-08-08T13:45:12.123Z",
          "message": "Disk usage exceeds 90%.",
          "log": {
            "level": "WARN"
          }
        },
        ...
      }
    }
  ]
}</pre>
</div>
<h5><a id="logs-stream-host-ip-query"></a>Query logs based on <code class="literal">host.ip</code><a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/main/docs/en/observability/logs-parse.asciidoc">edit</a></h5>
<p>You can query your logs based on the <code class="literal">host.ip</code> field in different ways, including using CIDR notation and range queries.</p>
<p>Before querying your logs, add them to your data stream using this command:</p>
<div class="pre_wrapper lang-console">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console">POST logs-example-default/_bulk
{ "create": {} }
{ "message": "2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%." }
{ "create": {} }
{ "message": "2023-08-08T13:45:14.003Z ERROR 192.168.1.103 Database connection failed." }
{ "create": {} }
{ "message": "2023-08-08T13:45:15.004Z DEBUG 192.168.1.104 Debugging connection issue." }
{ "create": {} }
{ "message": "2023-08-08T13:45:16.005Z INFO 192.168.1.102 User changed profile picture." }</pre>
</div>
<div class="console_widget" data-snippet="snippets/34.console"></div>
<h6><a id="logs-stream-ip-cidr"></a>CIDR notation<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/main/docs/en/observability/logs-parse.asciidoc">edit</a></h6>
<p>You can use <a href="https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing#CIDR_notation" class="ulink" target="_top">CIDR notation</a> to query your log data using a block of IP addresses that fall within a certain network segment. CIDR notations uses the format of <code class="literal">[IP address]/[prefix length]</code>. The following command queries IP addresses in the <code class="literal">192.168.1.0/24</code> subnet meaning IP addresses from <code class="literal">192.168.1.0</code> to <code class="literal">192.168.1.255</code>.</p>
<div class="pre_wrapper lang-console">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console">GET logs-example-default/_search
{
  "query": {
    "term": {
      "host.ip": "192.168.1.0/24"
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/35.console"></div>
<p>Because all of the example logs are in this range, you&#8217;ll get the following results:</p>
<div class="pre_wrapper lang-JSON">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-JSON">{
  ...
  },
  "hits": {
    ...
      {
        "_index": ".ds-logs-example-default-2023.08.16-000001",
        "_id": "ak4oAIoBl7fe5ItIixuB",
        "_score": 1,
        "_source": {
          "host": {
            "ip": "192.168.1.101"
          },
          "@timestamp": "2023-08-08T13:45:12.123Z",
          "message": "Disk usage exceeds 90%.",
          "log": {
            "level": "WARN"
          }
        }
      },
      {
        "_index": ".ds-logs-example-default-2023.08.16-000001",
        "_id": "a04oAIoBl7fe5ItIixuC",
        "_score": 1,
        "_source": {
          "host": {
            "ip": "192.168.1.103"
          },
          "@timestamp": "2023-08-08T13:45:14.003Z",
          "message": "Database connection failed.",
          "log": {
            "level": "ERROR"
          }
        }
      },
      {
        "_index": ".ds-logs-example-default-2023.08.16-000001",
        "_id": "bE4oAIoBl7fe5ItIixuC",
        "_score": 1,
        "_source": {
          "host": {
            "ip": "192.168.1.104"
          },
          "@timestamp": "2023-08-08T13:45:15.004Z",
          "message": "Debugging connection issue.",
          "log": {
            "level": "DEBUG"
          }
        }
      },
      {
        "_index": ".ds-logs-example-default-2023.08.16-000001",
        "_id": "bU4oAIoBl7fe5ItIixuC",
        "_score": 1,
        "_source": {
          "host": {
            "ip": "192.168.1.102"
          },
          "@timestamp": "2023-08-08T13:45:16.005Z",
          "message": "User changed profile picture.",
          "log": {
            "level": "INFO"
          }
        }
      }
    ]
  }
}</pre>
</div>
<h6><a id="logs-stream-range-query"></a>Range queries<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/main/docs/en/observability/logs-parse.asciidoc">edit</a></h6>
<p>Use <a href="/guide/en/elasticsearch/reference/master/query-dsl-range-query.html" class="ulink" target="_top">range queries</a> to query logs in a specific range.</p>
<p>The following command searches for IP addresses greater than or equal to <code class="literal">192.168.1.100</code> and less than or equal to <code class="literal">192.168.1.102</code>.</p>
<div class="pre_wrapper lang-console">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console">GET logs-example-default/_search
{
  "query": {
    "range": {
      "host.ip": {
        "gte": "192.168.1.100",<a id="CO65-1"></a><i class="conum" data-value="1"></i>
        "lte": "192.168.1.102"<a id="CO65-2"></a><i class="conum" data-value="2"></i>
      }
    }
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/36.console"></div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO65-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>Greater than or equal to <code class="literal">192.168.1.100</code>.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO65-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p>Less than or equal to <code class="literal">192.168.1.102</code>.</p>
</td>
</tr>
</table>
</div>
<p>You&#8217;ll get the following results only showing logs in the range you&#8217;ve set:</p>
<div class="pre_wrapper lang-JSON">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-JSON">{
  ...
  },
  "hits": {
    ...
      {
        "_index": ".ds-logs-example-default-2023.08.16-000001",
        "_id": "ak4oAIoBl7fe5ItIixuB",
        "_score": 1,
        "_source": {
          "host": {
            "ip": "192.168.1.101"
          },
          "@timestamp": "2023-08-08T13:45:12.123Z",
          "message": "Disk usage exceeds 90%.",
          "log": {
            "level": "WARN"
          }
        }
      },
      {
        "_index": ".ds-logs-example-default-2023.08.16-000001",
        "_id": "bU4oAIoBl7fe5ItIixuC",
        "_score": 1,
        "_source": {
          "host": {
            "ip": "192.168.1.102"
          },
          "@timestamp": "2023-08-08T13:45:16.005Z",
          "message": "User changed profile picture.",
          "log": {
            "level": "INFO"
          }
        }
      }
    ]
  }
}</pre>
</div>
<h3><a id="logs-stream-reroute"></a>Reroute log data to specific data streams<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/main/docs/en/observability/logs-parse.asciidoc">edit</a></h3>
<p>By default, an ingest pipeline sends your log data to a single data stream. To simplify log data management, use a <a href="/guide/en/elasticsearch/reference/master/reroute-processor.html" class="ulink" target="_top">reroute processor</a> to route data from the generic data stream to a target data stream. For example, you might want to send high-severity logs to a specific data stream to help with categorization.</p>
<p>This section shows you how to use a reroute processor to send the high-severity logs (<code class="literal">WARN</code> or <code class="literal">ERROR</code>) from the following example logs to a specific data stream and keep the regular logs (<code class="literal">DEBUG</code> and <code class="literal">INFO</code>) in the default data stream:</p>
<div class="pre_wrapper lang-log">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-log">2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%.
2023-08-08T13:45:14.003Z ERROR 192.168.1.103 Database connection failed.
2023-08-08T13:45:15.004Z DEBUG 192.168.1.104 Debugging connection issue.
2023-08-08T13:45:16.005Z INFO 192.168.1.102 User changed profile picture.</pre>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>When routing data to different data streams, we recommend picking a field with a limited number of distinct values to prevent an excessive increase in the number of data streams. For more details, refer to the <a href="/guide/en/elasticsearch/reference/master/size-your-shards.html" class="ulink" target="_top">Size your shards</a> documentation.</p>
</div>
</div>
<p>To use a reroute processor:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
<a class="xref" href="logs-parse.html#logs-stream-reroute-pipeline" title="Add a reroute processor to the ingest pipeline">Add a reroute processor to your ingest pipeline.</a>
</li>
<li class="listitem">
<a class="xref" href="logs-parse.html#logs-stream-reroute-add-logs" title="Add logs to a data stream">Add the example logs to your data stream.</a>
</li>
<li class="listitem">
<a class="xref" href="logs-parse.html#logs-stream-reroute-verify" title="Verify the reroute processor worked">Query your logs and verify the high-severity logs were routed to the new data stream.</a>
</li>
</ol>
</div>
<h4><a id="logs-stream-reroute-pipeline"></a>Add a reroute processor to the ingest pipeline<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/main/docs/en/observability/logs-parse.asciidoc">edit</a></h4>
<p>Add a reroute processor to your ingest pipeline with the following command:</p>
<div class="pre_wrapper lang-console">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console">PUT _ingest/pipeline/logs-example-default
{
  "description": "Extracts fields and reroutes WARN",
  "processors": [
    {
      "dissect": {
        "field": "message",
        "pattern": "%{@timestamp} %{log.level} %{host.ip} %{message}"
      },
      "reroute": {
        "tag": "high_severity_logs",<a id="CO66-1"></a><i class="conum" data-value="1"></i>
        "if" : "ctx.log?.level == 'WARN' || ctx.log?.level == 'ERROR'",<a id="CO66-2"></a><i class="conum" data-value="2"></i>
        "dataset": "critical"<a id="CO66-3"></a><i class="conum" data-value="3"></i>
      }
    }
  ]
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/37.console"></div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO66-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p><code class="literal">tag</code>: Identifier for the processor that you can use for debugging and metrics. In the example, the tag is set to <code class="literal">high_severity_logs</code>.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO66-2"><i class="conum" data-value="2"></i></a></p>
</td>
<td align="left" valign="top">
<p><code class="literal">if</code>: Conditionally runs the processor. In the example, <code class="literal">"ctx.log?.level == 'WARN' || ctx.log?.level == 'ERROR'",</code> means the processor runs when the <code class="literal">log.level</code> field is <code class="literal">WARN</code> or <code class="literal">ERROR</code>.</p>
</td>
</tr>
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO66-3"><i class="conum" data-value="3"></i></a></p>
</td>
<td align="left" valign="top">
<p><code class="literal">dataset</code>: the data stream dataset to route your document to if the previous condition is <code class="literal">true</code>. In the example, logs with a <code class="literal">log.level</code> of <code class="literal">WARN</code> or <code class="literal">ERROR</code> are routed to the <code class="literal">logs-critical-default</code> data stream.</p>
</td>
</tr>
</table>
</div>
<p>In addition to setting an ingest pipeline, you need to set an index template. You can use the index template created in the <a class="xref" href="logs-parse.html#logs-stream-index-template" title="Configure a data stream with an index template">Extract the <code class="literal">@timestamp</code> field</a> section.</p>
<h4><a id="logs-stream-reroute-add-logs"></a>Add logs to a data stream<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/main/docs/en/observability/logs-parse.asciidoc">edit</a></h4>
<p>Add the example logs to your data stream with this command:</p>
<div class="pre_wrapper lang-console">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console">POST logs-example-default/_bulk
{ "create": {} }
{ "message": "2023-08-08T13:45:12.123Z WARN 192.168.1.101 Disk usage exceeds 90%." }
{ "create": {} }
{ "message": "2023-08-08T13:45:14.003Z ERROR 192.168.1.103 Database connection failed." }
{ "create": {} }
{ "message": "2023-08-08T13:45:15.004Z DEBUG 192.168.1.104 Debugging connection issue." }
{ "create": {} }
{ "message": "2023-08-08T13:45:16.005Z INFO 192.168.1.102 User changed profile picture." }</pre>
</div>
<div class="console_widget" data-snippet="snippets/38.console"></div>
<h4><a id="logs-stream-reroute-verify"></a>Verify the reroute processor worked<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/observability-docs/edit/main/docs/en/observability/logs-parse.asciidoc">edit</a></h4>
<p>The reroute processor should route any logs with a <code class="literal">log.level</code> of <code class="literal">WARN</code> or <code class="literal">ERROR</code> to the <code class="literal">logs-critical-default</code> data stream. Query the the data stream using the following command to verify the log data was routed as intended:</p>
<div class="pre_wrapper lang-console">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-console">GET logs-critical-default/_search</pre>
</div>
<div class="console_widget" data-snippet="snippets/39.console"></div>
<p>Your should see similar results to the following showing that the high-severity logs are now in the <code class="literal">critical</code> dataset:</p>
<div class="pre_wrapper lang-JSON">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-JSON">{
  ...
  "hits": {
    ...
    "hits": [
        ...
        "_source": {
          "host": {
            "ip": "192.168.1.101"
          },
          "@timestamp": "2023-08-08T13:45:12.123Z",
          "message": "Disk usage exceeds 90%.",
          "log": {
            "level": "WARN"
          },
          "data_stream": {
            "namespace": "default",
            "type": "logs",
            "dataset": "critical"
          },
          {
        ...
        "_source": {
          "host": {
            "ip": "192.168.1.103"
           },
          "@timestamp": "2023-08-08T13:45:14.003Z",
          "message": "Database connection failed.",
          "log": {
            "level": "ERROR"
          },
          "data_stream": {
            "namespace": "default",
            "type": "logs",
            "dataset": "critical"
          }
        }
      }
    ]
  }
}</pre>
</div>
</div>
</div>
</div><div class="navfooter">
<span class="prev">
<a href="logs-stream.html">« Stream any log file</a>
</span>
<span class="next">
<a href="logs-filter-and-aggregate.html">Filter and aggregate logs »</a>
</span>
</div>
</body>
</html>
