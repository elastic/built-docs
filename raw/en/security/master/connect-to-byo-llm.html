<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Connect to your own local LLM | Elastic Security Solution [master] | Elastic</title>
<meta class="elastic" name="content" content="Connect to your own local LLM | Elastic Security Solution [master]">

<link rel="home" href="index.html" title="Elastic Security Solution [master]"/>
<link rel="up" href="llm-connector-guides.html" title="Set up connectors for large language models (LLM)"/>
<link rel="prev" href="assistant-connect-to-openai.html" title="Connect to OpenAI"/>
<link rel="next" href="assistant-use-cases.html" title="Use cases"/>
<meta class="elastic" name="product_version" content="master"/>
<meta class="elastic" name="product_name" content="Security"/>
<meta class="elastic" name="website_area" content="documentation"/>
<meta name="DC.type" content="Learn/Docs/Security/Guide/master"/>
<meta name="DC.subject" content="Security"/>
<meta name="DC.identifier" content="master"/>
<meta name="robots" content="noindex,nofollow"/>
</head>
<body>
<div class="page_header">
This documentation contains work-in-progress information for future Elastic Stack and Cloud releases. Use the version selector to view supported release docs. It also contains some Elastic Cloud serverless information. Check out our <a href="https://www.elastic.co/docs/current/serverless">serverless docs</a> for more details.
</div>
<div class="navheader">
<span class="prev">
<a href="assistant-connect-to-openai.html">« Connect to OpenAI</a>
</span>
<span class="next">
<a href="assistant-use-cases.html">Use cases »</a>
</span>
</div>
<div class="book" lang="en">
<div class="titlepage">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="/guide/">Elastic Docs</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="index.html">Elastic Security Solution [master]</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="ai-for-security.html">AI for security</a></span>
<span class="chevron-right">›</span><span class="breadcrumb-link"><a href="llm-connector-guides.html">Set up connectors for large language models (LLM)</a></span>
</div>
<div>
<div><h1 class="title"><a id="id-1"></a>Connect to your own local LLM</h1><a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/security-docs/edit/main/docs/AI-for-security/connect-to-byo.asciidoc">edit</a></div>
</div>
<!--EXTRA-->
</div>
<div id="content">
<div class="section">
<div class="titlepage"><div><div>
<h2 class="title"><a id="connect-to-byo-llm"></a>Connect to your own local LLM<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/security-docs/edit/main/docs/AI-for-security/connect-to-byo.asciidoc">edit</a></h2>
</div></div></div>
<p>This page provides instructions for setting up a connector to a large language model (LLM) of your choice using LM Studio. This allows you to use your chosen model within Elastic Security. You&#8217;ll first need to set up a reverse proxy to communicate with Elastic Security, then set up LM Studio on a server, and finally configure the connector in your Elastic deployment. <a href="/blog/ai-assistant-locally-hosted-models" class="ulink" target="_top">Learn more about the benefits of using a local LLM</a>.</p>
<p>This example uses a single server hosted in GCP to run the following components:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
LM Studio with the <a href="https://mistral.ai/technology/#models" class="ulink" target="_top">Mixtral-8x7b</a> model
</li>
<li class="listitem">
A reverse proxy using Nginx to authenticate to Elastic Cloud
</li>
</ul>
</div>
<div class="imageblock">
<div class="content">
<img src="images/lms-studio-arch-diagram.png" alt="Architecture diagram for this guide">
</div>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>For testing, you can use alternatives to Nginx such as <a href="https://learn.microsoft.com/en-us/azure/developer/dev-tunnels/overview" class="ulink" target="_top">Azure Dev Tunnels</a> or <a href="https://ngrok.com/" class="ulink" target="_top">Ngrok</a>, but using Nginx makes it easy to collect additional telemetry and monitor its status by using Elastic&#8217;s native Nginx integration. While this example uses cloud infrastructure, it could also be replicated locally without an internet connection.</p>
</div>
</div>
<h4><a id="_configure_your_reverse_proxy"></a>Configure your reverse proxy<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/security-docs/edit/main/docs/AI-for-security/connect-to-byo.asciidoc">edit</a></h4>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>If your Elastic instance is on the same host as LM Studio, you can skip this step.</p>
</div>
</div>
<p>You need to set up a reverse proxy to enable communication between LM Studio and Elastic. For more complete instructions, refer to a guide such as <a href="https://www.digitalocean.com/community/tutorials/how-to-configure-nginx-as-a-reverse-proxy-on-ubuntu-22-04" class="ulink" target="_top">this one</a>.</p>
<p>The following is an example Nginx configuration file:</p>
<div class="pre_wrapper lang-txt">
<div class="console_code_copy" title="Copy to clipboard"></div>
<pre class="programlisting prettyprint lang-txt">server {
    listen                          80;
    listen                          [::]:80;
    server_name                     &lt;yourdomainname.com&gt;;
    server_tokens off;
    add_header x-xss-protection "1; mode=block" always;
    add_header x-frame-options "SAMEORIGIN" always;
    add_header X-Content-Type-Options "nosniff" always;
    return 301                      https://$server_name$request_uri;
}

server {

    listen                          443 ssl http2;
    listen                          [::]:443 ssl http2;
    server_name                     &lt;yourdomainname.com&gt;;
    server_tokens off;
    ssl_certificate                 /etc/letsencrypt/live/&lt;yourdomainname.com&gt;/fullchain.pem;
    ssl_certificate_key             /etc/letsencrypt/live/&lt;yourdomainname.com&gt;/privkey.pem;
    ssl_session_timeout 1d;
    ssl_session_cache shared:SSL:50m;
    ssl_session_tickets on;
    ssl_ciphers 'ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256';
    ssl_protocols TLSv1.3 TLSv1.2;
    ssl_prefer_server_ciphers on;
    add_header Strict-Transport-Security "max-age=31536000; includeSubDomains; preload" always;
    add_header x-xss-protection "1; mode=block" always;
    add_header x-frame-options "SAMEORIGIN" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header Referrer-Policy "strict-origin-when-cross-origin" always;
    ssl_stapling on;
    ssl_stapling_verify on;
    ssl_trusted_certificate /etc/letsencrypt/live/&lt;yourdomainname.com&gt;/fullchain.pem;
    resolver 1.1.1.1;
    location / {

    if ($http_authorization != "Bearer &lt;secret token&gt;") {
    return 401;
}

    proxy_pass http://localhost:1234/;
   }

}</pre>
</div>
<div class="important admon">
<div class="icon"></div>
<div class="admon_content">
<p>If using the example configuration file above, you must replace several values: Replace <code class="literal">&lt;secret token&gt;</code> with your actual token, and keep it safe since you&#8217;ll need it to set up the Elastic Security connector. Replace <code class="literal">&lt;yourdomainname.com&gt;</code> with your actual domain name. Update the <code class="literal">proxy_pass</code> value at the bottom of the configuration if you decide to change the port number in LM Studio to something other than 1234.</p>
</div>
</div>
<h5><a id="_optional_set_up_performance_monitoring_for_your_reverse_proxy"></a>(Optional) Set up performance monitoring for your reverse proxy<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/security-docs/edit/main/docs/AI-for-security/connect-to-byo.asciidoc">edit</a></h5>
<p>You can use Elastic&#8217;s <a href="https://docs.elastic.co/en/integrations/nginx" class="ulink" target="_top">Nginx integration</a> to monitor performance and populate monitoring dashboards in the Elastic Security app.</p>
<h4><a id="_configure_lm_studio_and_download_a_model"></a>Configure LM Studio and download a model<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/security-docs/edit/main/docs/AI-for-security/connect-to-byo.asciidoc">edit</a></h4>
<p>First, install <a href="https://lmstudio.ai/" class="ulink" target="_top">LM Studio</a>. LM Studio supports the OpenAI SDK, which makes it compatible with Elastic&#8217;s OpenAI connector, allowing you to connect to any model available in the LM Studio marketplace.</p>
<p>One current limitation of LM Studio is that when it is installed on a server, you must launch the application using its GUI before doing so using the CLI. For example, by using Chrome RDP with an <a href="https://cloud.google.com/architecture/chrome-desktop-remote-on-compute-engine" class="ulink" target="_top">X Window System</a>. After you&#8217;ve opened the application the first time using the GUI, you can start it by using <code class="literal">sudo lms server start</code> in the CLI.</p>
<p>Once you&#8217;ve launched LM Studio:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Go to LM Studio&#8217;s Search window.
</li>
<li class="listitem">
Search for an LLM (for example, <code class="literal">Mixtral-8x7B-instruct</code>). Your chosen model must include <code class="literal">instruct</code> in its name in order to work with Elastic.
</li>
<li class="listitem">
<p>Filter your search for "Compatibility Guess" to optimize results for your hardware. Results will be color coded:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
Green means "Full GPU offload possible", which yields the best results.
</li>
<li class="listitem">
Blue means "Partial GPU offload possible", which may work.
</li>
<li class="listitem">
Red for "Likely too large for this machine", which typically will not work.
</li>
</ul>
</div>
</li>
<li class="listitem">
Download one or more models.
</li>
</ol>
</div>
<div class="important admon">
<div class="icon"></div>
<div class="admon_content">
<p>For security reasons, before downloading a model, verify that it is from a trusted source. It can be helpful to review community feedback on the model (for example using a site like Hugging Face).</p>
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="images/lms-model-select.png" alt="The LM Studio model selection interface">
</div>
</div>
<p>In this example we used <a href="https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF" class="ulink" target="_top"><code class="literal">TheBloke/Mixtral-8x7B-Instruct-v0.1.Q3_K_M.gguf</code></a>. It has 46.7B total parameters, a 32,000 token context window, and uses GGUF <a href="https://huggingface.co/docs/transformers/main/en/quantization/overview" class="ulink" target="_top">quanitization</a>. For more information about model names and format information, refer to the following table.</p>
<div class="informaltable">
<table border="1" cellpadding="4px">
<colgroup>
<col class="col_1"/>
<col class="col_2"/>
<col class="col_3"/>
<col class="col_4"/>
</colgroup>
<thead>
<tr>
<th align="left" valign="top">Model Name</th>
<th align="left" valign="top">Parameter Size</th>
<th align="left" valign="top">Tokens/Context Window</th>
<th align="left" valign="top">Quantization Format</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top"><p>Name of model, sometimes with a version number.</p></td>
<td align="left" valign="top"><p>LLMs are often compared by their number of parameters — higher numbers mean more powerful models.</p></td>
<td align="left" valign="top"><p>Tokens are small chunks of input information. Tokens do not necessarily correspond to characters. You can use <a href="https://platform.openai.com/tokenizer" class="ulink" target="_top">Tokenizer</a> to see how many tokens a given prompt might contain.</p></td>
<td align="left" valign="top"><p>Quantization reduces overall parameters and helps the model to run faster, but reduces accuracy.</p></td>
</tr>
<tr>
<td align="left" valign="top"><p>Examples: Llama, Mistral, Phi-3, Falcon.</p></td>
<td align="left" valign="top"><p>The number of parameters is a measure of the size and the complexity of the model. The more parameters a model has, the more data it can process, learn from, generate, and predict.</p></td>
<td align="left" valign="top"><p>The context window defines how much information the model can process at once. If the number of input tokens exceeds this limit, input gets truncated.</p></td>
<td align="left" valign="top"><p>Specific formats for quantization vary, most models now support GPU rather than CPU offloading.</p></td>
</tr>
</tbody>
</table>
</div>
<h4><a id="_load_a_model_in_lm_studio"></a>Load a model in LM Studio<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/security-docs/edit/main/docs/AI-for-security/connect-to-byo.asciidoc">edit</a></h4>
<p>After downloading a model, load it in LM Studio using the GUI or LM Studio&#8217;s <a href="https://lmstudio.ai/blog/lms" class="ulink" target="_top">CLI tool</a>.</p>
<h5><a id="_option_1_load_a_model_using_the_cli_recommended"></a>Option 1: load a model using the CLI (Recommended)<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/security-docs/edit/main/docs/AI-for-security/connect-to-byo.asciidoc">edit</a></h5>
<p>It is a best practice to download models from the marketplace using the GUI, and then load or unload them using the CLI. The GUI allows you to search for models, whereas the CLI only allows you to import specific paths, but the CLI provides a good interface for loading and unloading.</p>
<p>Use the following commands in your CLI:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Verify LM Studio is installed: <code class="literal">lms</code>
</li>
<li class="listitem">
Check LM Studio&#8217;s status: <code class="literal">lms status</code>
</li>
<li class="listitem">
List all downloaded models: <code class="literal">lms ls</code>
</li>
<li class="listitem">
Load a model: <code class="literal">lms load</code>
</li>
</ol>
</div>
<div class="imageblock">
<div class="content">
<img src="images/lms-cli-welcome.png" alt="The CLI interface during execution of initial LM Studio commands">
</div>
</div>
<p>After the model loads, you should see a <code class="literal">Model loaded successfully</code> message in the CLI.</p>
<div class="imageblock">
<div class="content">
<img src="images/lms-studio-model-loaded-msg.png" alt="The CLI message that appears after a model loads">
</div>
</div>
<p>To verify which model is loaded, use the <code class="literal">lms ps</code> command.</p>
<div class="imageblock">
<div class="content">
<img src="images/lms-ps-command.png" alt="The CLI message that appears after running lms ps">
</div>
</div>
<p>If your model uses NVIDIA drivers, you can check the GPU performance with the <code class="literal">sudo nvidia-smi</code> command.</p>
<h5><a id="_option_2_load_a_model_using_the_gui"></a>Option 2: load a model using the GUI<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/security-docs/edit/main/docs/AI-for-security/connect-to-byo.asciidoc">edit</a></h5>
<p>Refer to the following video to see how to load a model using LM Studio&#8217;s GUI. You can change the <span class="strong strong"><strong>port</strong></span> setting, which is referenced in the Nginx configuration file. Note that the <span class="strong strong"><strong>GPU offload</strong></span> was set to <span class="strong strong"><strong>Max</strong></span>.</p>
<div class="exampleblock">
<div class="content">
<script type="text/javascript" async src="https://play.vidyard.com/embed/v4.js"></script>
<img
  style="width: 100%; margin: auto; display: block;"
  class="vidyard-player-embed"
  src="https://play.vidyard.com/FMx2wxGQhquWPVhGQgjkyM.jpg"
  data-uuid="FMx2wxGQhquWPVhGQgjkyM"
  data-v="4"
  data-type="inline"
/>
</br>
</div>
</div>
<h4><a id="_optional_collect_logs_using_elastics_custom_logs_integration"></a>(Optional) Collect logs using Elastic&#8217;s Custom Logs integration<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/security-docs/edit/main/docs/AI-for-security/connect-to-byo.asciidoc">edit</a></h4>
<p>You can monitor the performance of the host running LM Studio using Elastic&#8217;s <a href="https://docs.elastic.co/en/integrations/log" class="ulink" target="_top">Custom Logs integration</a>. This can also help with troubleshooting. Note that the default path for LM Studio logs is <code class="literal">/tmp/lmstudio-server-log.txt</code>, as in the following screenshot:</p>
<div class="imageblock">
<div class="content">
<img src="images/lms-custom-logs-config.png" alt="The configuration window for the custom logs integration">
</div>
</div>
<h4><a id="_configure_the_connector_in_your_elastic_deployment"></a>Configure the connector in your Elastic deployment<a class="edit_me" rel="nofollow" title="Edit this page on GitHub" href="https://github.com/elastic/security-docs/edit/main/docs/AI-for-security/connect-to-byo.asciidoc">edit</a></h4>
<p>Finally, configure the connector:</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
Log in to your Elastic deployment.
</li>
<li class="listitem">
Navigate to <span class="strong strong"><strong>Stack Management → Connectors → Create Connector → OpenAI</strong></span>. The OpenAI connector enables this use case because LM Studio uses the OpenAI SDK.
</li>
<li class="listitem">
Name your connector to help keep track of the model version you are using.
</li>
<li class="listitem">
Under <span class="strong strong"><strong>URL</strong></span>, enter the domain name specified in your Nginx configuration file, followed by <code class="literal">/v1/chat/completions</code>.
</li>
<li class="listitem">
Under <span class="strong strong"><strong>Default model</strong></span>, enter <code class="literal">local-model</code>.
</li>
<li class="listitem">
Under <span class="strong strong"><strong>API key</strong></span>, enter the secret token specified in your Nginx configuration file.
</li>
<li class="listitem">
Click <span class="strong strong"><strong>Save</strong></span>.
</li>
</ol>
</div>
<div class="imageblock">
<div class="content">
<img src="images/lms-edit-connector.png" alt="The Edit connector page in the Elastic Security app" width="with appropriate values populated">
</div>
</div>
<p>Setup is now complete. You can use the model you&#8217;ve loaded in LM Studio to power Elastic&#8217;s generative AI features. You can test a variety of models as you interact with AI Assistant to see what works best without having to update your connector.</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>While local models work well for <a class="xref" href="security-assistant.html" title="AI Assistant">AI Assistant</a>, we recommend you use one of <a class="xref" href="llm-performance-matrix.html" title="Large language model performance matrix">these models</a> for interacting with <a class="xref" href="attack-discovery.html" title="Attack discovery">Attack discovery</a>. As local models become more performant over time, this is likely to change.</p>
</div>
</div>
</div>
</div>
</div><div class="navfooter">
<span class="prev">
<a href="assistant-connect-to-openai.html">« Connect to OpenAI</a>
</span>
<span class="next">
<a href="assistant-use-cases.html">Use cases »</a>
</span>
</div>
</body>
</html>
